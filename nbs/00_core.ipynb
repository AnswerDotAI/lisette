{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Lisette Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from cachy import enable_cachy,disable_cachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio, base64, json, litellm, mimetypes, random, string\n",
    "from typing import Optional,Callable\n",
    "from html import escape\n",
    "from litellm import (acompletion, completion, stream_chunk_builder, Message,\n",
    "                     ModelResponse, ModelResponseStream, get_model_info, register_model, Usage)\n",
    "from litellm.utils import function_to_dict, StreamingChoices, Delta, ChatCompletionMessageToolCall, Function, Choices\n",
    "from toolslm.funccall import mk_ns, call_func, call_func_async, get_schema\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import delegates\n",
    "from fastcore import imghdr\n",
    "from dataclasses import dataclass\n",
    "from litellm.exceptions import ContextWindowExceededError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "from IPython.display import Markdown, Image, Audio, Video\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "# LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Deterministic outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "LiteLLM `ModelResponse(Stream)` objects have `id` and `created_at` fields that are generated dynamically. Even when we use [`cachy`](https://github.com/answerdotai/cachy) to cache the LLM response these dynamic fields create diffs which makes code review more challenging. The patches below ensure that `id` and `created_at` fields are fixed and won't generate diffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def patch_litellm(seed=0):\n",
    "    \"Patch litellm.ModelResponseBase such that `id` and `created` are fixed.\"\n",
    "    from litellm.types.utils import ModelResponseBase, ChatCompletionMessageToolCall\n",
    "    from uuid import UUID\n",
    "    from base64 import b64encode\n",
    "    if seed is not None: random.seed(seed) # ensures random ids like tool call ids are deterministic\n",
    "    \n",
    "    @patch\n",
    "    def __init__(self: ModelResponseBase, id=None, created=None, *args, **kwargs): \n",
    "        self._orig___init__(id='chatcmpl-xxx', created=1000000000, *args, **kwargs)\n",
    "\n",
    "    @patch\n",
    "    def __setattr__(self: ModelResponseBase, name, value):\n",
    "        if name == 'id': value = 'chatcmpl-xxx'\n",
    "        elif name == 'created': value = 1000000000\n",
    "        self._orig___setattr__(name, value)\n",
    "\n",
    "    def _unqid():\n",
    "        res = b64encode(UUID(int=random.getrandbits(128), version=4).bytes)\n",
    "        return '_' + res.decode().rstrip('=').translate(str.maketrans('+/', '_-'))\n",
    "\n",
    "    @patch\n",
    "    def __init__(self: ChatCompletionMessageToolCall, function=None, id=None, type=\"function\", **kwargs):\n",
    "        # we keep the tool call prefix if it exists, this is needed for example to handle srvtoolu_ correctly.\n",
    "        id = id.split('_')[0]+_unqid() if id and '_' in id else id\n",
    "        self._orig___init__(function=function, id=id, type=type, **kwargs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_litellm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "LiteLLM provides an convenient unified interface for most big LLM providers. Because it's so useful to be able to switch LLM providers with just one argument. We want to make it even easier to by adding some more convenience functions and classes. \n",
    "\n",
    "This is very similar to our other wrapper libraries for popular AI providers: [claudette](https://claudette.answer.ai/) (Anthropic), [gaspard](https://github.com/AnswerDotAI/gaspard) (Gemini), [cosette](https://answerdotai.github.io/cosette/) (OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _repr_markdown_(self: litellm.ModelResponse):\n",
    "    message = self.choices[0].message\n",
    "    content = ''\n",
    "    if mc:=message.content: content += mc[0]['text'] if isinstance(mc,list) else mc\n",
    "    if message.tool_calls:\n",
    "        tool_calls = [f\"\\n\\nðŸ”§ {nested_idx(tc,'function','name')}({nested_idx(tc,'function','arguments')})\\n\" for tc in message.tool_calls]\n",
    "        content += \"\\n\".join(tool_calls)\n",
    "    for img in getattr(message, 'images', []): content += f\"\\n\\n![generated image]({nested_idx(img, 'image_url', 'url')})\"\n",
    "    if not content: content = str(message)\n",
    "    details = [\n",
    "        f\"id: `{self.id}`\",\n",
    "        f\"model: `{self.model}`\",\n",
    "        f\"finish_reason: `{self.choices[0].finish_reason}`\"\n",
    "    ]\n",
    "    if hasattr(self, 'usage') and self.usage: details.append(f\"usage: `{self.usage}`\")\n",
    "    det_str = '\\n- '.join(details)\n",
    "    \n",
    "    return f\"\"\"{content}\n",
    "\n",
    "<details>\n",
    "\n",
    "- {det_str}\n",
    "\n",
    "</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "register_model({\n",
    "    \"claude-opus-4-5\": {\n",
    "        \"litellm_provider\": \"anthropic\", \"mode\": \"chat\",\n",
    "        \"max_tokens\": 64000, \"max_input_tokens\": 200000, \"max_output_tokens\": 64000,\n",
    "        \"input_cost_per_token\": 0.000005, \"output_cost_per_token\": 0.000025,\n",
    "        \"cache_creation_input_token_cost\": 0.000005*1.25, \"cache_read_input_token_cost\": 0.000005*0.1,\n",
    "        \"supports_function_calling\": True, \"supports_parallel_function_calling\": True,\n",
    "        \"supports_vision\": True, \"supports_prompt_caching\": True, \"supports_response_schema\": True,\n",
    "        \"supports_system_messages\": True, \"supports_reasoning\": True, \"supports_assistant_prefill\": True,\n",
    "        \"supports_tool_choice\": True, \"supports_computer_use\": True, \"supports_web_search\": True\n",
    "    }\n",
    "});\n",
    "sonn45 = \"claude-sonnet-4-5\"\n",
    "opus45 = \"claude-opus-4-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# litellm._turn_on_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = [\"gemini/gemini-3-pro-preview\", \"gemini/gemini-2.5-pro\", \"gemini/gemini-2.5-flash\", \"claude-sonnet-4-5\", \"openai/gpt-4.1\"]\n",
    "msg = [{'role':'user','content':'Hey there!', 'cache_control': {'type': 'ephemeral'}}]\n",
    "for m in ms:\n",
    "    display(Markdown(f'**{m}:**'))\n",
    "    display(completion(m,msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "Generated images are also displayed (not shown here to conserve filesize):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# completion(model='gemini/gemini-2.5-flash-image', messages=[{'role':'user','content':'Draw a simple sketch of a cat'}])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## Messages formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Let's start with making it easier to pass messages into litellm's `completion` function (including images, and pdf files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _bytes2content(data):\n",
    "    \"Convert bytes to litellm content dict (image, pdf, audio, video)\"\n",
    "    mtype = detect_mime(data)\n",
    "    if not mtype: raise ValueError(f'Data must be a supported file type, got {data[:10]}')\n",
    "    encoded = base64.b64encode(data).decode(\"utf-8\")    \n",
    "    if mtype.startswith('image/'): return {'type': 'image_url', 'image_url': f'data:{mtype};base64,{encoded}'}\n",
    "    return {'type': 'file', 'file': {'file_data': f'data:{mtype};base64,{encoded}'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_cache_control(msg,          # LiteLLM formatted msg\n",
    "                       ttl=None):    # Cache TTL: '5m' (default) or '1h'\n",
    "    \"cache `msg` with default time-to-live (ttl) of 5minutes ('5m'), but can be set to '1h'.\"\n",
    "    if isinstance(msg[\"content\"], str): \n",
    "        msg[\"content\"] = [{\"type\": \"text\", \"text\": msg[\"content\"]}]\n",
    "    cache_control = {\"type\": \"ephemeral\"}\n",
    "    if ttl is not None: cache_control[\"ttl\"] = ttl\n",
    "    if isinstance(msg[\"content\"], list) and msg[\"content\"]:\n",
    "        msg[\"content\"][-1][\"cache_control\"] = cache_control\n",
    "    return msg\n",
    "\n",
    "def _has_cache(msg):\n",
    "    return msg[\"content\"] and isinstance(msg[\"content\"], list) and ('cache_control' in msg[\"content\"][-1])\n",
    "\n",
    "def remove_cache_ckpts(msg):\n",
    "    \"remove cache checkpoints and return msg.\"\n",
    "    if _has_cache(msg): msg[\"content\"][-1].pop('cache_control', None)\n",
    "    return msg\n",
    "\n",
    "def _mk_content(o):\n",
    "    if isinstance(o, str): return {'type':'text','text':o.strip() or '.'}\n",
    "    elif isinstance(o,bytes): return _bytes2content(o)\n",
    "    return o\n",
    "\n",
    "def contents(r):\n",
    "    \"Get message object from response `r`.\"\n",
    "    return r.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_msg(\n",
    "    content,      # Content: str, bytes (image), list of mixed content, or dict w 'role' and 'content' fields\n",
    "    role=\"user\",  # Message role if content isn't already a dict/Message\n",
    "    cache=False,  # Enable Anthropic caching\n",
    "    ttl=None      # Cache TTL: '5m' (default) or '1h'\n",
    "):\n",
    "    \"Create a LiteLLM compatible message.\"\n",
    "    if isinstance(content, dict) or isinstance(content, Message): return content\n",
    "    if isinstance(content, ModelResponse): return contents(content)\n",
    "    if isinstance(content, list) and len(content) == 1 and isinstance(content[0], str): c = content[0]\n",
    "    elif isinstance(content, list): c = [_mk_content(o) for o in content]\n",
    "    else: c = content\n",
    "    msg = {\"role\": role, \"content\": c}\n",
    "    return _add_cache_control(msg, ttl=ttl) if cache else msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Now we can use mk_msg to create different types of messages.\n",
    "\n",
    "Simple text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = mk_msg(\"hey\")\n",
    "msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "Which can be passed to litellm's `completion` function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ms[1] # use 2.5-pro, 3-pro is very slow even to run tests as of making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = completion(model, [msg])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "We'll add a little shortcut to make examples and testing easier here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(msgs, m=model, **kw):\n",
    "    msgs = [msgs] if isinstance(msgs,dict) else listify(msgs)\n",
    "    return completion(m, msgs, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "c(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "Lists w just one string element are flattened for conciseness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(mk_msg(\"hey\"), mk_msg([\"hey\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "(LiteLLM ignores these fields when sent to other providers)\n",
    "\n",
    "Text and images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "img_fn = Path('samples/puppy.jpg')\n",
    "Image(filename=img_fn, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = mk_msg(['hey what in this image?',img_fn.read_bytes()])\n",
    "print(json.dumps(msg,indent=1)[:200]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "c(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Let's also demonstrate this for PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_fn = Path('samples/solveit.pdf')\n",
    "msg = mk_msg(['Who is the author of this pdf?', pdf_fn.read_bytes()])\n",
    "c(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "Some models like Gemini support audio and video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_data = httpx.get(\"https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav\").content\n",
    "# Audio(wav_data)  # uncomment to preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = mk_msg(['What is this audio saying?', wav_data])\n",
    "completion(ms[1], [msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_data = httpx.get(\"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = mk_msg(['Concisely, what is happening in this video?', vid_data])\n",
    "completion(ms[1], [msg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46",
   "metadata": {},
   "source": [
    "Some providers such as Anthropic require manually opting into caching. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpr(i): return f'{i} '*1024 + 'This is a caching test. Report back only what number you see repeated above.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "disable_cachy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg = mk_msg(cpr(1), cache=True)\n",
    "# res = c(msg, ms[2])\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50",
   "metadata": {},
   "source": [
    "Anthropic has a maximum of 4 cache checkpoints, so we remove previous ones as we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = c([remove_cache_ckpts(msg), mk_msg(res), mk_msg(cpr(2), cache=True)], ms[2])\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52",
   "metadata": {},
   "source": [
    "We see that the first message was cached, and this extra message has been written to cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.usage.prompt_tokens_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54",
   "metadata": {},
   "source": [
    "We can add a bunch of large messages in a loop to see how the number of cached tokens used grows.\n",
    "\n",
    "We do this for 25 times to ensure it still works for more than >20 content blocks, [which is a known anthropic issue](https://docs.claude.com/en/docs/build-with-claude/prompt-caching).\n",
    "\n",
    "The code below is commented by default, because it's slow. Please uncomment when working on caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = []\n",
    "# msg = mk_msg(cpr(1), cache=True)\n",
    "\n",
    "# for o in range(2,25):\n",
    "#     h += [remove_cache_ckpts(msg), mk_msg(res)]\n",
    "#     msg = mk_msg(cpr(o), cache=True)\n",
    "#     res = c(h+[msg])\n",
    "#     detls = res.usage.prompt_tokens_details\n",
    "#     print(o, detls.cached_tokens, detls.cache_creation_tokens, end='; ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57",
   "metadata": {},
   "source": [
    "### Reconstructing formatted outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Lisette can call multiple tools in a loop. Further down this notebook, we'll provide convenience functions for formatting such a sequence of toolcalls and responses into one formatted output string.\n",
    "\n",
    "For now, we'll show an example and show how to transform such a formatted output string back into a valid LiteLLM history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt_outp = '''\n",
    "I'll solve this step-by-step, using parallel calls where possible.\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\n",
    "  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\n",
    "  \"result\": \"15\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\n",
    "  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\n",
    "  \"result\": \"3\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Now I need to multiply 15 * 3 before I can do the final division:\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\n",
    "  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\n",
    "  \"result\": \"45\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "detls_tag = \"<details class='tool-usage-details'>\"\n",
    "re_tools = re.compile(  fr\"^({detls_tag}\\n*(?:<summary>.*?</summary>\\n*)?\\n*```json\\n+(.*?)\\n+```\\n+</details>)\",\n",
    "                        flags=re.DOTALL|re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "We can split into chunks of (text,toolstr,json):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "sp = re_tools.split(fmt_outp)\n",
    "for o in list(chunked(sp, 3, pad=True)): print('- ', o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _extract_tool(text:str)->tuple[dict,dict]:\n",
    "    \"Extract tool call and results from <details> block\"\n",
    "    try: d = json.loads(text.strip())\n",
    "    except: return\n",
    "    call = d['call']\n",
    "    func = call['function']\n",
    "    tc = ChatCompletionMessageToolCall(Function(dumps(call['arguments']),func), d['id'])\n",
    "    tr = {'role': 'tool','tool_call_id': d['id'],'name': func, 'content': d['result']}\n",
    "    return tc,tr\n",
    "\n",
    "def fmt2hist(outp:str)->list:\n",
    "    \"Transform a formatted output into a LiteLLM compatible history\"\n",
    "    lm,hist = Message(),[]\n",
    "    spt = re_tools.split(outp)\n",
    "    for txt,_,tooljson in chunked(spt, 3, pad=True):\n",
    "        txt = txt.strip() if tooljson or txt.strip() else '.'\n",
    "        hist.append(lm:=Message(txt))\n",
    "        if tooljson:\n",
    "            if tcr := _extract_tool(tooljson):\n",
    "                if not hist: hist.append(lm) # if LLM calls a tool without talking\n",
    "                lm.tool_calls = lm.tool_calls+[tcr[0]] if lm.tool_calls else [tcr[0]] \n",
    "                hist.append(tcr[1])\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64",
   "metadata": {},
   "source": [
    "See how we can turn that one formatted output string back into a list of Messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "h = fmt2hist(fmt_outp)\n",
    "pprint(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "### `mk_msgs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "We will skip tool use blocks and tool results during caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_cache_idxs(msgs, cache_idxs=[-1], ttl=None):\n",
    "    'Add cache control to idxs after filtering tools'\n",
    "    ms = L(msgs).filter(lambda m: not (m.get('tool_calls', []) or m['role'] == 'tool'))\n",
    "    for i in cache_idxs:\n",
    "        try: _add_cache_control(ms[i], ttl)\n",
    "        except IndexError: continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70",
   "metadata": {},
   "source": [
    "Now lets make it easy to provide entire conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_msgs(\n",
    "    msgs,                   # List of messages (each: str, bytes, list, or dict w 'role' and 'content' fields)\n",
    "    cache=False,            # Enable Anthropic caching\n",
    "    cache_idxs=[-1],        # Cache breakpoint idxs\n",
    "    ttl=None,               # Cache TTL: '5m' (default) or '1h'\n",
    "):\n",
    "    \"Create a list of LiteLLM compatible messages.\"\n",
    "    if not msgs: return []\n",
    "    if not isinstance(msgs, list): msgs = [msgs]\n",
    "    res,role = [],'user'\n",
    "    msgs = L(msgs).map(lambda m: fmt2hist(m) if detls_tag in m else [m]).concat()\n",
    "    for m in msgs:\n",
    "        res.append(msg:=remove_cache_ckpts(mk_msg(m, role=role)))\n",
    "        role = 'assistant' if msg['role'] in ('user','function', 'tool') else 'user'\n",
    "    if cache: _apply_cache_idxs(res, cache_idxs, ttl)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72",
   "metadata": {},
   "source": [
    "With `mk_msgs` you can easily provide a whole conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74",
   "metadata": {},
   "source": [
    "By defualt the last message will be cached when `cache=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"], cache=True)\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('cache_control' in msgs[-1]['content'][0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77",
   "metadata": {},
   "source": [
    "Alternatively, users can provide custom `cache_idxs`. Tool call blocks and results are skipped during caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = mk_msgs(['Hello!','Hi! How can I help you?','Call some functions!',fmt_outp], cache=True, cache_idxs=[0,-2,-1])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('cache_control' in msgs[0]['content'][0], True)\n",
    "test_eq('cache_control' in msgs[2]['content'][0], True) # shifted idxs to skip tools\n",
    "test_eq('cache_control' in msgs[-1]['content'][0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80",
   "metadata": {},
   "source": [
    "Who's speaking at when is automatically inferred.\n",
    "Even when there are multiple tools being called in parallel (which LiteLLM supports!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = mk_msgs(['Tell me the weather in Paris and Rome',\n",
    "                'Assistant calls weather tool two times',\n",
    "                {'role':'tool','content':'Weather in Paris is ...'},\n",
    "                {'role':'tool','content':'Weather in Rome is ...'},\n",
    "                'Assistant returns weather',\n",
    "                'Thanks!'])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq([m['role'] for m in msgs],['user','assistant','tool','tool','assistant','user'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83",
   "metadata": {},
   "source": [
    "For ease of use, if `msgs` is not already in a `list`, it will automatically be wrapped inside one. This way you can pass a single prompt into `mk_msgs` and get back a LiteLLM compatible msg history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = mk_msgs(\"Hey\")\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "msgs = mk_msgs({'role':'tool','content':'fake tool result'})\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm fine, you?\"])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87",
   "metadata": {},
   "source": [
    "However, beware that if you use `mk_msgs` for a single message, consisting of multiple parts.\n",
    "Then you should be explicit, and make sure to wrap those multiple messages in two lists:\n",
    "\n",
    "1. One list to show that they belong together in one message (the inner list).\n",
    "2. Another, because mk_msgs expects a list of multiple messages (the outer list).\n",
    "\n",
    "This is common when working with images for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = mk_msgs([['Whats in this img?',img_fn.read_bytes()]])\n",
    "print(json.dumps(msgs,indent=1)[:200]+\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "LiteLLM supports streaming responses. That's really useful if you want to show intermediate results, instead of having to wait until the whole response is finished.\n",
    "\n",
    "We create this helper function that returns the entire response at the end of the stream. This is useful when you want to store the whole response somewhere after having displayed the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stream_with_complete(gen, postproc=noop):\n",
    "    \"Extend streaming response chunks with the complete response\"\n",
    "    chunks = []\n",
    "    for chunk in gen:\n",
    "        chunks.append(chunk)\n",
    "        yield chunk\n",
    "    postproc(chunks)\n",
    "    return stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = c(mk_msgs(\"Hey!\"), stream=True)\n",
    "r2 = SaveReturn(stream_with_complete(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in r2:\n",
    "    cts = o.choices[0].delta.content\n",
    "    if cts: print(cts, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lite_mk_func(f):\n",
    "    if isinstance(f, dict): return f\n",
    "    return {'type':'function', 'function':get_schema(f, pname='parameters')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_add(\n",
    "    a: int,   # first operand\n",
    "    b: int=0  # second operand\n",
    ") -> int:\n",
    "    \"Add two numbers together\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98",
   "metadata": {},
   "outputs": [],
   "source": [
    "toolsc = lite_mk_func(simple_add)\n",
    "toolsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmsg = mk_msg(\"What is 5478954793+547982745? How about 5479749754+9875438979? Always use tools for calculations, and describe what you'll do before using a tool. Where multiple tool calls are required, do them in a single response where possible. \")\n",
    "r = c(tmsg, tools=[toolsc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "A tool response can be a string or a list of tool blocks (e.g., an image url block). To allow users to specify if a response should not be immediately stringified, we provide the ToolResponse datatype users can wrap their return statement in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "102",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ToolResponse:\n",
    "    content: list[str,str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "103",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _lite_call_func(tc, tool_schemas, ns, raise_on_err=True):\n",
    "    fn, valid = tc.function.name, {nested_idx(o,'function','name') for o in tool_schemas or []}\n",
    "    if fn not in valid: res = f\"Tool not defined in tool_schemas: {fn}\"\n",
    "    else:\n",
    "        try: res = call_func(fn, json.loads(tc.function.arguments), ns=ns)\n",
    "        except json.JSONDecodeError: res = f\"Failed to parse function arguments: {tc.function.arguments}\"\n",
    "        else: res = res.content if isinstance(res, ToolResponse) else str(res)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": fn, \"content\": res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs = [_lite_call_func(o, [toolsc], ns=globals()) for o in r.choices[0].message.tool_calls]\n",
    "tcs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105",
   "metadata": {},
   "source": [
    "Test tool calls that were not in tool_schemas are caught:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_tc = ChatCompletionMessageToolCall(index=0, function=Function(name='hallucinated_tool'),id='_', type='function')\n",
    "test_eq(_lite_call_func(fake_tc, ns=globals(), tool_schemas=[toolsc])['content'],\"Tool not defined in tool_schemas: hallucinated_tool\")\n",
    "test_fail(_lite_call_func(fake_tc, ns=globals(), tool_schemas=None)['content'],\"Tool not defined in tool_schemas: hallucinated_tool\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107",
   "metadata": {},
   "source": [
    "Test tool calls that were not in tool_choice are caught:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_text(msg):\n",
    "    \"Extract printable content from streaming delta, return None if nothing to print\"\n",
    "    c = msg.choices[0]\n",
    "    if not c: return c\n",
    "    if not hasattr(c,'delta'): return None #f'{c}'\n",
    "    delta = c.delta\n",
    "    if delta.content: return delta.content\n",
    "    if delta.tool_calls:\n",
    "        res = ''.join(f\"ðŸ”§ {tc.function.name}\" for tc in delta.tool_calls if tc.id and tc.function.name)\n",
    "        if res: return f'\\n{res}\\n'\n",
    "    if hasattr(delta,'reasoning_content'): return 'ðŸ§ ' if delta.reasoning_content else '\\n\\n'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = c(tmsg, stream=True, tools=[toolsc])\n",
    "r2 = SaveReturn(stream_with_complete(r))\n",
    "for o in r2: print(delta_text(o) or '', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = mk_msg(\"Solve this complex math problem: What is the derivative of x^3 + 2x^2 - 5x + 1?\")\n",
    "r = c(msg, stream=True, reasoning_effort=\"low\")\n",
    "r2 = SaveReturn(stream_with_complete(r))\n",
    "for o in r2: print(delta_text(o) or '', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "112",
   "metadata": {},
   "outputs": [],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "## Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@delegates(completion)\n",
    "def structured(\n",
    "    m:str,          # LiteLLM model string\n",
    "    msgs:list,      # List of messages \n",
    "    tool:Callable,  # Tool to be used for creating the structured output (class, dataclass or Pydantic, function, etc)\n",
    "    **kwargs):\n",
    "    \"Return the value of the tool call (generally used for structured outputs)\"\n",
    "    t = lite_mk_func(tool)\n",
    "    r = completion(m, msgs, tools=[t], tool_choice=t, **kwargs)\n",
    "    args = json.loads(r.choices[0].message.tool_calls[0].function.arguments)\n",
    "    return tool(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115",
   "metadata": {},
   "outputs": [],
   "source": [
    "class President:\n",
    "    \"Information about a president of the United States\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        first:str, # first name\n",
    "        last:str, # last name\n",
    "        spouse:str, # name of spouse\n",
    "        years_in_office:str, # format: \"{start_year}-{end_year}\"\n",
    "        birthplace:str, # name of city\n",
    "        birth_year:int # year of birth, `0` if unknown\n",
    "    ):\n",
    "        assert re.match(r'\\d{4}-\\d{4}', years_in_office), \"Invalid format: `years_in_office`\"\n",
    "        store_attr()\n",
    "\n",
    "    __repr__ = basic_repr('first, last, spouse, years_in_office, birthplace, birth_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms[1:]: \n",
    "    r = structured(m, [mk_msg(\"Tell me something about the third president of the USA.\")], President)\n",
    "    test_eq(r.first, 'Thomas'); test_eq(r.last, 'Jefferson')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "117",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118",
   "metadata": {},
   "source": [
    "LiteLLM provides search, not via tools, but via the special `web_search_options` param.\n",
    "\n",
    "**Note:** Not all models support web search. LiteLLM's `supports_web_search` field should indicate this, but it's unreliable for some models like `claude-sonnet-4-20250514`. Checking both `supports_web_search` and `search_context_cost_per_query` provides more accurate detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _has_search(m):\n",
    "    i = get_model_info(m)\n",
    "    return bool(i.get('search_context_cost_per_query') or i.get('supports_web_search'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms: print(m, _has_search(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121",
   "metadata": {},
   "source": [
    "When search is supported it can be used like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "smsg = mk_msg(\"Search the web and tell me very briefly about otters\")\n",
    "r = c(smsg, web_search_options={\"search_context_size\": \"low\"})  # or 'medium' / 'high'\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124",
   "metadata": {},
   "source": [
    "Next, lets handle Anthropic's search citations.\n",
    "\n",
    "When not using streaming, all citations are placed in a separate key in the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "r['vertex_ai_grounding_metadata'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "r['vertex_ai_grounding_metadata'][0]['webSearchQueries']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127",
   "metadata": {},
   "source": [
    "Web search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "128",
   "metadata": {},
   "outputs": [],
   "source": [
    "r['vertex_ai_grounding_metadata'][0]['groundingChunks'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "Citations in gemini: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130",
   "metadata": {},
   "outputs": [],
   "source": [
    "r['vertex_ai_grounding_metadata'][0]['groundingSupports'][:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "# r.choices[0].message.provider_specific_fields['citations'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "132",
   "metadata": {},
   "source": [
    "However, when streaming the results are not captured this way.\n",
    "Instead, we provide this helper function that adds the citation to the `content` field in markdown format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cite_footnote(msg):\n",
    "    if not (delta:=nested_idx(msg, 'choices', 0, 'delta')): return\n",
    "    if citation:= nested_idx(delta, 'provider_specific_fields', 'citation'):\n",
    "        title = citation['title'].replace('\"', '\\\\\"')\n",
    "        delta.content = f'[*]({citation[\"url\"]} \"{title}\") '\n",
    "        \n",
    "def cite_footnotes(stream_list):\n",
    "    \"Add markdown footnote citations to stream deltas\"\n",
    "    for msg in stream_list: cite_footnote(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = list(c(smsg, ms[2], stream=True, web_search_options={\"search_context_size\": \"low\"}))\n",
    "cite_footnotes(r)\n",
    "stream_chunk_builder(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "LiteLLM is pretty bare bones. It doesnt keep track of conversation history or what tools have been added in the conversation so far.\n",
    "\n",
    "So lets make a Claudette style wrapper so we can do streaming, toolcalling, and toolloops without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "137",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "effort = AttrDict({o[0]:o for o in ('low','medium','high')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _mk_prefill(pf): return ModelResponseStream([StreamingChoices(delta=Delta(content=pf,role='assistant'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "139",
   "metadata": {},
   "source": [
    "When the tool uses are about to be exhausted it is important to alert the AI so that it knows to use its final steps for communicating the user current progress and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _trunc_str(s, mx=2000, replace=\"<TRUNCATED>\"):\n",
    "    \"Truncate `s` to `mx` chars max, adding `replace` if truncated\"\n",
    "    s = str(s).strip()\n",
    "    if len(s)<=mx: return s\n",
    "    s = s[:mx]\n",
    "    ss = s.split(' ')\n",
    "    if len(ss[-1])>50: ss[-1] = ss[-1][:5]\n",
    "    s = ' '.join(ss)\n",
    "    return s+replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_final_prompt = dict(role=\"user\", content=\"You have used all your tool calls for this turn. Please summarize your findings. If you did not complete your goal, tell the user what further work is needed. You may use tools again on the next user message.\")\n",
    "\n",
    "_cwe_msg = \"ContextWindowExceededError: Do no more tool calls and complete your response now. Inform user that you ran out of context and explain what the cause was. This is the response to this tool call, truncated if needed: \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Chat:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model:str,                # LiteLLM compatible model name \n",
    "        sp='',                    # System prompt\n",
    "        temp=0,                   # Temperature\n",
    "        search=False,             # Search (l,m,h), if model supports it\n",
    "        tools:list=None,          # Add tools\n",
    "        hist:list=None,           # Chat history\n",
    "        ns:Optional[dict]=None,   # Custom namespace for tool calling \n",
    "        cache=False,              # Anthropic prompt caching\n",
    "        cache_idxs:list=[-1],     # Anthropic cache breakpoint idxs, use `0` for sys prompt if provided\n",
    "        ttl=None,                 # Anthropic prompt caching ttl\n",
    "        api_base=None,            # API base URL for custom providers\n",
    "        api_key=None,             # API key for custom providers\n",
    "    ):\n",
    "        \"LiteLLM chat client.\"\n",
    "        self.model = model\n",
    "        hist,tools = mk_msgs(hist,cache,cache_idxs,ttl),listify(tools)\n",
    "        if ns is None and tools: ns = mk_ns(tools)\n",
    "        elif ns is None: ns = globals()\n",
    "        self.tool_schemas = [lite_mk_func(t) for t in tools] if tools else None\n",
    "        store_attr()\n",
    "    \n",
    "    def _prep_msg(self, msg=None, prefill=None):\n",
    "        \"Prepare the messages list for the API call\"\n",
    "        sp = [{\"role\": \"system\", \"content\": self.sp}] if self.sp else []\n",
    "        if sp:\n",
    "            if 0 in self.cache_idxs: sp[0] = _add_cache_control(sp[0])\n",
    "            cache_idxs = L(self.cache_idxs).filter().map(lambda o: o-1 if o>0 else o)\n",
    "        else:\n",
    "            cache_idxs = self.cache_idxs\n",
    "        if msg: self.hist = mk_msgs(self.hist+[msg], self.cache and 'claude' in self.model, cache_idxs, self.ttl)\n",
    "        pf = [{\"role\":\"assistant\",\"content\":prefill}] if prefill else []\n",
    "        return sp + self.hist + pf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "`web_search` is now included in `tool_calls` the internal LLM translation is correctly handled thanks to the fix [here](https://github.com/BerriAI/litellm/pull/17746) but the server side tools still need to be filtered out from `tool_calls` in our own toolloop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _filter_srvtools(tcs): return L(tcs).filter(lambda o: not o.id.startswith('srvtoolu_')) if tcs else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _call(self:Chat, msg=None, prefill=None, temp=None, think=None, search=None, stream=False, max_steps=2, step=1, final_prompt=None, tool_choice=None, **kwargs):\n",
    "    \"Internal method that always yields responses\"\n",
    "    if step>max_steps: return\n",
    "    try:\n",
    "        model_info = get_model_info(self.model)\n",
    "    except Exception:\n",
    "        register_model({self.model: {}})\n",
    "        model_info = get_model_info(self.model)\n",
    "    if not model_info.get(\"supports_assistant_prefill\"): prefill=None\n",
    "    if _has_search(self.model) and (s:=ifnone(search,self.search)): kwargs['web_search_options'] = {\"search_context_size\": effort[s]}\n",
    "    else: _=kwargs.pop('web_search_options',None)\n",
    "    if self.api_base: kwargs['api_base'] = self.api_base\n",
    "    if self.api_key: kwargs['api_key'] = self.api_key\n",
    "    res = completion(\n",
    "        model=self.model, messages=self._prep_msg(msg, prefill), stream=stream, \n",
    "        tools=self.tool_schemas, reasoning_effort = effort.get(think), tool_choice=tool_choice,\n",
    "        # temperature is not supported when reasoning\n",
    "        temperature=None if think else ifnone(temp,self.temp),\n",
    "        caching=self.cache and 'claude' not in self.model,\n",
    "        **kwargs)\n",
    "    if stream:\n",
    "        if prefill: yield _mk_prefill(prefill)\n",
    "        res = yield from stream_with_complete(res,postproc=cite_footnotes)\n",
    "    m = contents(res)\n",
    "    if prefill: m.content = prefill + m.content\n",
    "    self.hist.append(m)\n",
    "    yield res\n",
    "\n",
    "    if tcs := _filter_srvtools(m.tool_calls):\n",
    "        tool_results=[_lite_call_func(tc, self.tool_schemas, self.ns) for tc in tcs]\n",
    "        self.hist+=tool_results\n",
    "        for r in tool_results: yield r\n",
    "        if step>=max_steps-1: prompt,tool_choice,search = final_prompt,'none',False\n",
    "        else: prompt = None\n",
    "        try: yield from self._call(\n",
    "            prompt, prefill, temp, think, search, stream, max_steps, step+1,\n",
    "            final_prompt, tool_choice, **kwargs)\n",
    "        except ContextWindowExceededError:\n",
    "            for t in tool_results:\n",
    "                if len(t['content'])>1000: t['content'] = _cwe_msg + _trunc_str(t['content'], mx=1000)\n",
    "            yield from self._call(None, prefill, temp, think, search, stream, max_steps, max_steps, final_prompt, 'none', **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "@delegates(Chat._call)\n",
    "def __call__(self:Chat,\n",
    "             msg=None,          # Message str, or list of multiple message parts\n",
    "             prefill=None,      # Prefill AI response if model supports it\n",
    "             temp=None,         # Override temp set on chat initialization\n",
    "             think=None,        # Thinking (l,m,h)\n",
    "             search=None,       # Override search set on chat initialization (l,m,h)\n",
    "             stream=False,      # Stream results\n",
    "             max_steps=2, # Maximum number of tool calls\n",
    "             final_prompt=_final_prompt, # Final prompt when tool calls have ran out \n",
    "             return_all=False,  # Returns all intermediate ModelResponses if not streaming and has tool calls\n",
    "             **kwargs):\n",
    "    \"Main call method - handles streaming vs non-streaming\"\n",
    "    result_gen = self._call(msg, prefill, temp, think, search, stream, max_steps, 1, final_prompt, **kwargs)     \n",
    "    if stream: return result_gen              # streaming\n",
    "    elif return_all: return list(result_gen)  # toolloop behavior\n",
    "    else: return last(result_gen)             # normal chat behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(as_prop=True)\n",
    "def cost(self: Chat):\n",
    "    \"Total cost of all responses in conversation history\"\n",
    "    return sum(getattr(r, '_hidden_params', {}).get('response_cost')  or 0\n",
    "               for r in self.h if hasattr(r, 'choices'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def print_hist(self:Chat):\n",
    "    \"Print each message on a different line\"\n",
    "    for r in self.hist: print(r, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "149",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "150",
   "metadata": {},
   "source": [
    "### History tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms[1:]:\n",
    "    chat = Chat(m)\n",
    "    chat(\"Hey my name is Rens\")\n",
    "    r = chat(\"Whats my name\")\n",
    "    test_eq('Rens' in contents(r).content, True)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {},
   "source": [
    "See now we keep track of history!\n",
    "\n",
    "History is stored in the `hist` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.print_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "155",
   "metadata": {},
   "source": [
    "You can also pass an old chat history into new Chat objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms[1:]:\n",
    "    chat2 = Chat(m, hist=chat.hist)\n",
    "    r = chat2(\"What was my name again?\")\n",
    "    test_eq('Rens' in contents(r).content, True)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "You can prefix an [OpenAI compatible model](https://docs.litellm.ai/docs/providers/openai_compatible) with 'openai/' and use an `api_base` and `api_key` argument to use models not registered with litellm.\n",
    "\n",
    "```python\n",
    "import os, litellm\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "c = Chat(\"openai/gpt-oss-20b\", api_key=OPENROUTER_API_KEY, api_base=OPENROUTER_BASE_URL)\n",
    "c(\"hi\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158",
   "metadata": {},
   "source": [
    "### Synthetic History Creation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159",
   "metadata": {},
   "source": [
    "Lets build chat history step by step. That way we can tweak anything we need to during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = \"What is 5 + 7? Use the tool to calculate it.\"\n",
    "for m in ms[1:]:\n",
    "    c = Chat(m, tools=[simple_add])\n",
    "    res = c(pr)\n",
    "    test_eq('12' in contents(res).content, True)\n",
    "    test_eq(nested_idx(c.hist,1,'tool_calls',0,'function','name'), 'simple_add')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "161",
   "metadata": {},
   "source": [
    "Whereas normally without tools we would get one user input and one assistant response. Here we get two extra messages in between.\n",
    "- An assistant message requesting the tools with arguments.\n",
    "- A tool response with the result to the tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163",
   "metadata": {},
   "source": [
    "Lets try to build this up manually so we have full control over the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def random_tool_id():\n",
    "    \"Generate a random tool ID with 'toolu_' prefix\"\n",
    "    random_part = ''.join(random.choices(string.ascii_letters + string.digits, k=25))\n",
    "    return f'toolu_{random_part}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_tool_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "166",
   "metadata": {},
   "source": [
    "A tool call request can contain one more or more tool calls. Lets make one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc(func, args, tcid=None, idx=1):\n",
    "    if not tcid: tcid = random_tool_id()\n",
    "    return {'index': idx, 'function': {'arguments': args, 'name': func}, 'id': tcid, 'type': 'function'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = mk_tc(simple_add.__name__, json.dumps(dict(a=5, b=7)))\n",
    "tc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "169",
   "metadata": {},
   "source": [
    "This can then be packged into the full Message object produced by the assitant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_tc_req(content, tcs): return Message(content=content, role='assistant', tool_calls=tcs, function_call=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "171",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc_cts = \"I'll use the simple_add tool to calculate 5 + 7 for you.\"\n",
    "tcq = mk_tc_req(tc_cts, [tc])\n",
    "tcq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172",
   "metadata": {},
   "source": [
    "Notice how Message instantiation creates a list of ChatCompletionMessageToolCalls by default. When the tools are executed this is converted back\n",
    "to a dictionary, for consistency we want to keep these as dictionaries from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "173",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc_req(content, tcs):\n",
    "    msg = Message(content=content, role='assistant', tool_calls=tcs, function_call=None)\n",
    "    msg.tool_calls = [{**dict(tc), 'function': dict(tc['function'])} for tc in msg.tool_calls]\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcq = mk_tc_req(tc_cts, [tc])\n",
    "tcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chat(model, tools=[simple_add], hist=[pr, tcq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177",
   "metadata": {},
   "source": [
    "Looks good so far! Now we will want to provide the actual result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc_result(tc, result): return {'tool_call_id': tc['id'], 'role': 'tool', 'name': tc['function']['name'], 'content': result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179",
   "metadata": {},
   "source": [
    "Note we might have more than one tool call if more than one was passed in, here we just will make one result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcq.tool_calls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_tc_result(tcq.tool_calls[0], '12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc_results(tcq, results): return [mk_tc_result(a,b) for a,b in zip(tcq.tool_calls, results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183",
   "metadata": {},
   "source": [
    "Same for here tcq.tool_calls will match the number of results passed in the results list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "184",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "185",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr = mk_tc_results(tcq, ['12'])\n",
    "tcr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "186",
   "metadata": {},
   "source": [
    "Now we can call it with this synthetic data to see what the response is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "187",
   "metadata": {},
   "outputs": [],
   "source": [
    "c(tcr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "189",
   "metadata": {},
   "source": [
    "Lets try this again, but lets give it something that is clearly wrong for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "190",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chat(model, tools=[simple_add], hist=[pr, tcq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr = mk_tc_results(tcq, ['13'])\n",
    "tcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "c(tcr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193",
   "metadata": {},
   "source": [
    "Lets make sure this works with multiple tool calls in the same assistant Message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs = [\n",
    "    mk_tc(simple_add.__name__, json.dumps({\"a\": 5, \"b\": 7})), \n",
    "    mk_tc(simple_add.__name__, json.dumps({\"a\": 6, \"b\": 7})), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcq = mk_tc_req(\"I will calculate these for you!\", tcs)\n",
    "tcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "196",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr = mk_tc_results(tcq, ['12', '13'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chat(model, tools=[simple_add], hist=[pr, tcq, tcr[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198",
   "metadata": {},
   "outputs": [],
   "source": [
    "c(tcr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(ms[1], tools=[simple_add])\n",
    "res = chat(\"What's 5 + 3? Use the `simple_add` tool.\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = chat(\"Now, tell me a joke based on that result.\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "202",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "203",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "204",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms[1:]:\n",
    "    chat = Chat(m)\n",
    "    r = chat(['Whats in this img?',img_fn.read_bytes()])\n",
    "    test_eq('puppy' in contents(r).content, True)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "205",
   "metadata": {},
   "source": [
    "### Prefill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206",
   "metadata": {},
   "source": [
    "Prefill works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms[1:]:\n",
    "    if not get_model_info(m)['supports_assistant_prefill']: continue\n",
    "    chat = Chat(m)\n",
    "    chat('Hi this is Rens!')\n",
    "    r = chat(\"Spell my name\",prefill=\"Your name is R E\")\n",
    "    test_eq(contents(r).content.startswith('Your name is R E N S'), True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208",
   "metadata": {},
   "source": [
    "And the entire message is stored in the history, not just the generated part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat.hist[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms[1:]:\n",
    "    chat = Chat(m)\n",
    "    stream_gen = chat(\"Count to 5\", stream=True)\n",
    "    for chunk in stream_gen:\n",
    "        if isinstance(chunk, ModelResponse): display(chunk)\n",
    "        else: print(delta_text(chunk) or '',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "213",
   "metadata": {},
   "source": [
    "Lets try prefill with streaming too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_gen = chat(\"Continue counting to 10\",\"Okay! 6, 7\",stream=True)\n",
    "# for chunk in stream_gen:\n",
    "#     if isinstance(chunk, ModelResponse): display(chunk)\n",
    "#     else: print(delta_text(chunk) or '',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215",
   "metadata": {},
   "source": [
    "### Tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216",
   "metadata": {},
   "source": [
    "Ok now lets test tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms[1:]:\n",
    "    display(Markdown(f'**{m}:**'))\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's 5 + 3? Use  the `simple_add` tool. Explain.\")\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "218",
   "metadata": {},
   "source": [
    "### Thinking w tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms[1:]:\n",
    "    _sparams = litellm.get_model_info(m)['supported_openai_params']\n",
    "    if 'reasoning_effort' not in _sparams: continue\n",
    "    display(Markdown(f'**{m}:**'))\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's 5 + 3?\",think='l',return_all=True)\n",
    "    display(*res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms[1:]:\n",
    "    display(Markdown(f'**{m}:**'))\n",
    "    chat = Chat(m)\n",
    "    res = chat(\"Search the web and tell me very briefly about otters\", search='l', stream=True)\n",
    "    for o in res:\n",
    "        if isinstance(o, ModelResponse): sleep(0.01); display(o)\n",
    "        else: pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222",
   "metadata": {},
   "source": [
    "### Multi tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "223",
   "metadata": {},
   "source": [
    "We can let the model call multiple tools in sequence using the `max_steps` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms:\n",
    "    display(Markdown(f'**{m}:**'))\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's ((5 + 3)+7)+11? Work step by step\", return_all=True, max_steps=5)\n",
    "    for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "225",
   "metadata": {},
   "source": [
    "Some models support parallel tool calling. I.e. sending multiple tool call requests in one conversation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"Multiply two numbers\"\n",
    "    return a * b\n",
    "\n",
    "for m in ms[1:]:\n",
    "    _sparams = litellm.get_model_info(m)['supported_openai_params']\n",
    "    if 'parallel_tool_calls' not in _sparams: continue\n",
    "    display(Markdown(f'**{m}:**'))\n",
    "    chat = Chat(m, tools=[simple_add, multiply])\n",
    "    res = chat(\"Calculate (5 + 3) * (7 + 2)\", max_steps=5, return_all=True)\n",
    "    for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227",
   "metadata": {},
   "source": [
    "See how the additions are calculated in one go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "228",
   "metadata": {},
   "source": [
    "We don't want the model to keep running tools indefinitely. Lets showcase how we can force the model to stop after our specified number of toolcall rounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide(a: int, b: int) -> float:\n",
    "    \"Divide two numbers\"\n",
    "    return a / b\n",
    "\n",
    "chat = Chat(model, tools=[simple_add, multiply, divide])\n",
    "res = chat(\"Calculate ((10 + 5) * 3) / (2 + 1) step by step.\", \n",
    "           max_steps=3, return_all=True,\n",
    "           final_prompt=\"Please wrap-up for now and summarize how far we got.\")\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "230",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(len([o for o in res if isinstance(o,ModelResponse)]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "231",
   "metadata": {},
   "source": [
    "### Tool call exhaustion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = \"What is 1+2, and then the result of adding +2, and then +3 to it? Use tools to make the calculations!\"\n",
    "c = Chat(model, tools=[simple_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = c(pr, max_steps=2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert c.hist[-2] == _final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235",
   "metadata": {},
   "source": [
    "## Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236",
   "metadata": {},
   "source": [
    "### AsyncChat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237",
   "metadata": {},
   "source": [
    "If you want to use LiteLLM in a webapp you probably want to use their async function `acompletion`.\n",
    "To make that easier we will implement our version of `AsyncChat` to complement it. It follows the same implementation as Chat as much as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _alite_call_func(tc, tool_schemas, ns, raise_on_err=True):\n",
    "    fn, valid = tc.function.name, {nested_idx(o,'function','name') for o in tool_schemas or []}\n",
    "    if fn not in valid: res = f\"Tool not defined in tool_schemas: {fn}\"\n",
    "    else:\n",
    "        try: fargs = json.loads(tc.function.arguments)\n",
    "        except json.JSONDecodeError: res = f\"Failed to parse function arguments: {tc.function.arguments}\"\n",
    "        else:\n",
    "            res = await call_func_async(fn, fargs, ns=ns)\n",
    "            res = res.content if isinstance(res, ToolResponse) else str(res)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": fn, \"content\": res}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239",
   "metadata": {},
   "source": [
    "Testing the scenarios where the tool call was not in schemas, or schemas was missing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await _alite_call_func(fake_tc, [toolsc], globals())\n",
    "test_eq(result['content'], \"Tool not defined in tool_schemas: hallucinated_tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "241",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = await _alite_call_func(fake_tc, None, globals())\n",
    "test_eq(result['content'], \"Tool not defined in tool_schemas: hallucinated_tool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@asave_iter\n",
    "async def astream_with_complete(self, agen, postproc=noop):\n",
    "    chunks = []\n",
    "    async for chunk in agen:\n",
    "        chunks.append(chunk)\n",
    "        postproc(chunk)\n",
    "        yield chunk\n",
    "    self.value = stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "243",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AsyncChat(Chat):\n",
    "    async def _call(self, msg=None, prefill=None, temp=None, think=None, search=None, stream=False, max_steps=2, step=1, final_prompt=None, tool_choice=None, **kwargs):\n",
    "        if step>max_steps+1: return\n",
    "        if not get_model_info(self.model).get(\"supports_assistant_prefill\"): prefill=None\n",
    "        if _has_search(self.model) and (s:=ifnone(search,self.search)): kwargs['web_search_options'] = {\"search_context_size\": effort[s]}\n",
    "        else: _=kwargs.pop('web_search_options',None)\n",
    "        res = await acompletion(model=self.model, messages=self._prep_msg(msg, prefill), stream=stream,\n",
    "                         tools=self.tool_schemas, reasoning_effort=effort.get(think), tool_choice=tool_choice,\n",
    "                         # temperature is not supported when reasoning\n",
    "                         temperature=None if think else ifnone(temp,self.temp), \n",
    "                         caching=self.cache and 'claude' not in self.model,\n",
    "                         **kwargs)\n",
    "        if stream:\n",
    "            if prefill: yield _mk_prefill(prefill)\n",
    "            res = astream_with_complete(res,postproc=cite_footnote)\n",
    "            async for chunk in res: yield chunk\n",
    "            res = res.value\n",
    "        m=contents(res)\n",
    "        if prefill: m.content = prefill + m.content\n",
    "        yield res\n",
    "        self.hist.append(m)\n",
    "\n",
    "        if tcs := _filter_srvtools(m.tool_calls):\n",
    "            tool_results = []\n",
    "            for tc in tcs:\n",
    "                result = await _alite_call_func(tc, self.tool_schemas, self.ns)\n",
    "                tool_results.append(result)\n",
    "                yield result\n",
    "            self.hist+=tool_results\n",
    "            if step>=max_steps-1: prompt,tool_choice,search = final_prompt,'none',False\n",
    "            else: prompt = None\n",
    "            try:\n",
    "                async for result in self._call(\n",
    "                    prompt, prefill, temp, think, search, stream, max_steps, step+1,\n",
    "                    final_prompt, tool_choice=tool_choice, **kwargs): yield result\n",
    "            except ContextWindowExceededError:\n",
    "                for t in tool_results:\n",
    "                    if len(t['content'])>1000: t['content'] = _cwe_msg + _trunc_str(t['content'], mx=1000)\n",
    "                async for result in self._call(\n",
    "                    prompt, prefill, temp, think, search, stream, max_steps, step+1,\n",
    "                    final_prompt, tool_choice='none', **kwargs): yield result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "@delegates(Chat._call)\n",
    "async def __call__(\n",
    "    self:AsyncChat,\n",
    "    msg=None,          # Message str, or list of multiple message parts\n",
    "    prefill=None,      # Prefill AI response if model supports it\n",
    "    temp=None,         # Override temp set on chat initialization\n",
    "    think=None,        # Thinking (l,m,h)\n",
    "    search=None,       # Override search set on chat initialization (l,m,h)\n",
    "    stream=False,      # Stream results\n",
    "    max_steps=2, # Maximum number of tool calls\n",
    "    final_prompt=_final_prompt, # Final prompt when tool calls have ran out \n",
    "    return_all=False,  # Returns all intermediate ModelResponses if not streaming and has tool calls\n",
    "    **kwargs\n",
    "):\n",
    "    result_gen = self._call(msg, prefill, temp, think, search, stream, max_steps, 1, final_prompt, **kwargs)\n",
    "    if stream or return_all: return result_gen\n",
    "    async for res in result_gen: pass\n",
    "    return res # normal chat behavior only return last msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "246",
   "metadata": {},
   "source": [
    "Basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms[1:]:\n",
    "    chat = AsyncChat(m)\n",
    "    test_eq('4' in contents(await chat(\"What is 2+2?\")).content, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248",
   "metadata": {},
   "source": [
    "With tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_add(a: int, b: int) -> int:\n",
    "    \"Add two numbers asynchronously\"\n",
    "    await asyncio.sleep(0.1)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250",
   "metadata": {},
   "outputs": [],
   "source": [
    "for m in ms[1:]:\n",
    "    chat = AsyncChat(m, tools=[async_add])\n",
    "    r = await chat(\"What is 5 + 7? Use the tool to calculate it.\")\n",
    "    test_eq('12' in contents(r).content, True)\n",
    "    test_eq(nested_idx(chat.hist, 1, 'tool_calls', 0, 'function', 'name'), 'async_add')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251",
   "metadata": {},
   "source": [
    "## Async Streaming Display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252",
   "metadata": {},
   "source": [
    "This is what our outputs look like with streaming results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "253",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_with_tools = AsyncChat(model, tools=[async_add])\n",
    "res = await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\n",
    "async for o in res:\n",
    "    if isinstance(o,ModelResponseStream): print(delta_text(o) or '',end='')\n",
    "    elif isinstance(o,dict): print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254",
   "metadata": {},
   "source": [
    "Here's a complete `ModelResponse` taken from the response stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255",
   "metadata": {},
   "outputs": [],
   "source": [
    "resp = ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01CWqrNQvoRjf1Q1GLpTUgQR', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=228, prompt_tokens=794, total_tokens=1022, prompt_tokens_details=None))\n",
    "print(repr(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "256",
   "metadata": {},
   "outputs": [],
   "source": [
    "tc=resp.choices[0].message.tool_calls[0]\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr={'tool_call_id': 'toolu_018BGyenjiRkDQFU1jWP6qRo', 'role': 'tool','name': 'simple_add',\n",
    "    'content': '15 is the answer! ' +'.'*2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _trunc_param(v, mx=50):\n",
    "    \"Truncate and escape param value for display\"\n",
    "    return _trunc_str(str(v).replace('`', r'\\`'), mx=mx, replace='â€¦')\n",
    "\n",
    "def mk_tr_details(tr, tc, mx=2000):\n",
    "    \"Create <details> block for tool call as JSON\"\n",
    "    args = {k:_trunc_str(v, mx=mx) for k,v in json.loads(tc.function.arguments).items()}\n",
    "    res = {'id':tr['tool_call_id'], \n",
    "           'call':{'function': tc.function.name, 'arguments': args},\n",
    "           'result':_trunc_str(tr.get('content'), mx=mx),}\n",
    "    params = ', '.join(f\"{k}={_trunc_param(v)}\" for k,v in args.items())\n",
    "    summ = f\"<summary>{tc.function.name}({params})</summary>\"\n",
    "    return f\"\\n\\n{detls_tag}\\n{summ}\\n\\n```json\\n{dumps(res, indent=2)}\\n```\\n\\n</details>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259",
   "metadata": {},
   "outputs": [],
   "source": [
    "mk_tr_details(tr,tc,mx=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class StreamFormatter:\n",
    "    def __init__(self, include_usage=False, mx=2000, debug=False):\n",
    "        self.outp,self.tcs,self.include_usage,self.mx,self.debug = '',{},include_usage,mx,debug\n",
    "    \n",
    "    def format_item(self, o):\n",
    "        \"Format a single item from the response stream.\"\n",
    "        res = ''\n",
    "        if self.debug: print(o)\n",
    "        if isinstance(o, ModelResponseStream):\n",
    "            d = o.choices[0].delta\n",
    "            if nested_idx(d, 'reasoning_content') and d['reasoning_content']!='{\"text\": \"\"}':\n",
    "                res+= 'ðŸ§ ' if not self.outp or self.outp[-1]=='ðŸ§ ' else '\\n\\nðŸ§ ' # gemini can interleave reasoning\n",
    "            elif self.outp and self.outp[-1] == 'ðŸ§ ': res+= '\\n\\n'\n",
    "            if c:=d.content: # gemini has text content in last reasoning chunk\n",
    "                res+=f\"\\n\\n{c}\" if res and res[-1] == 'ðŸ§ ' else c\n",
    "            for img in getattr(d, 'images', []): res += f\"\\n\\n![generated image]({nested_idx(img, 'image_url', 'url')})\\n\\n\"\n",
    "        elif isinstance(o, ModelResponse):\n",
    "            if self.include_usage: res += f\"\\nUsage: {o.usage}\"\n",
    "            if c:=getattr(contents(o),'tool_calls',None):\n",
    "                self.tcs = {tc.id:tc for tc in c}\n",
    "        elif isinstance(o, dict) and 'tool_call_id' in o:\n",
    "            res += mk_tr_details(o, self.tcs.pop(o['tool_call_id']), mx=self.mx)\n",
    "        self.outp+=res\n",
    "        return res\n",
    "    \n",
    "    def format_stream(self, rs):\n",
    "        \"Format the response stream for markdown display.\"\n",
    "        for o in rs: yield self.format_item(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_msg = ModelResponseStream([StreamingChoices(delta=Delta(content=\"Hello world!\"))])\n",
    "StreamFormatter().format_item(stream_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "262",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_msg = ModelResponseStream([StreamingChoices(delta=Delta(reasoning_content=\"thinking...\"))])\n",
    "StreamFormatter().format_item(reasoning_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AsyncStreamFormatter(StreamFormatter):\n",
    "    async def format_stream(self, rs):\n",
    "        \"Format the response stream for markdown display.\"\n",
    "        async for o in rs: yield self.format_item(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_tool_call = ChatCompletionMessageToolCall(\n",
    "    id=\"toolu_123abc456def\", type=\"function\", \n",
    "    function=Function( name=\"simple_add\", arguments='{\"a\": 5, \"b\": 3}' )\n",
    ")\n",
    "\n",
    "mock_response = ModelResponse()\n",
    "mock_response.choices = [type('Choice', (), {\n",
    "    'message': type('Message', (), {\n",
    "        'tool_calls': [mock_tool_call]\n",
    "    })()\n",
    "})()]\n",
    "\n",
    "mock_tool_result = {\n",
    "    'tool_call_id': mock_tool_call.id, 'role': 'tool', \n",
    "    'name': 'simple_add', 'content': '8'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt = AsyncStreamFormatter()\n",
    "print(fmt.format_item(mock_response))\n",
    "print('---')\n",
    "print(fmt.format_item(mock_tool_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266",
   "metadata": {},
   "source": [
    "In jupyter it's nice to use this `StreamFormatter` in combination with the `Markdown` `display`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def display_stream(rs):\n",
    "    \"Use IPython.display to markdown display the response stream.\"\n",
    "    try: from IPython.display import display, Markdown\n",
    "    except ModuleNotFoundError: raise ModuleNotFoundError(\"This function requires ipython. Please run `pip install ipython` to use.\")\n",
    "    fmt = StreamFormatter()\n",
    "    md = ''\n",
    "    for o in fmt.format_stream(rs): \n",
    "        md+=o\n",
    "        display(Markdown(md),clear=True)\n",
    "    return fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "268",
   "metadata": {},
   "source": [
    "Generated images can be displayed in streaming too (not shown here to conserve filesize):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rs = completion(model='gemini/gemini-2.5-flash-image', stream=True, messages=[{'role':'user','content':'Draw a simple sketch of a dog'}])\n",
    "# fmt = display_stream(rs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "270",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def adisplay_stream(rs):\n",
    "    \"Use IPython.display to markdown display the response stream.\"\n",
    "    try: from IPython.display import display, Markdown\n",
    "    except ModuleNotFoundError: raise ModuleNotFoundError(\"This function requires ipython. Please run `pip install ipython` to use.\")\n",
    "    fmt = AsyncStreamFormatter()\n",
    "    md = ''\n",
    "    async for o in fmt.format_stream(rs): \n",
    "        md+=o\n",
    "        display(Markdown(md),clear=True)\n",
    "    return fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271",
   "metadata": {},
   "source": [
    "## Streaming examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272",
   "metadata": {},
   "source": [
    "Now we can demonstrate `AsyncChat` with `stream=True`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273",
   "metadata": {},
   "source": [
    "### Tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat(model, tools=[simple_add])\n",
    "res = chat(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\n",
    "fmt = display_stream(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AsyncChat(model, tools=[async_add])\n",
    "res = await chat(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\n",
    "fmt = await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "276",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AsyncChat(model, tools=[async_add])\n",
    "res = await chat(\"What is 5 + 3? Use the tool to calculate it.\", stream=True)\n",
    "fmt = await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277",
   "metadata": {},
   "source": [
    "### Thinking tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AsyncChat(model)\n",
    "res = await chat(\"Briefly, what's the most efficient way to sort a list of 1000 random integers?\", think='l',stream=True)\n",
    "_ = await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279",
   "metadata": {},
   "source": [
    "### Multiple tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "chat = AsyncChat(model, tools=[simple_add, multiply, divide])\n",
    "res = await chat(\"Calculate ((10 + 5) * 3) / (2 + 1) Use parallel tool calls, but explain where we are after each batch.\", \n",
    "           max_steps=3, stream=True,\n",
    "           final_prompt=\"Please wrap-up for now and summarize how far we got.\")\n",
    "fmt = await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.hist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "282",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.hist[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.hist[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.hist[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat.hist[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286",
   "metadata": {},
   "source": [
    "Now to demonstrate that we can load back the formatted output back into a new `Chat` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat5 = Chat(model,hist=fmt2hist(fmt.outp),tools=[simple_add, multiply, divide])\n",
    "chat5('what did we just do?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "289",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_stream_tools = AsyncChat(model, search='l')\n",
    "res = await chat_stream_tools(\"Search the weather in NYC\", stream=True)\n",
    "_=await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291",
   "metadata": {},
   "source": [
    "#### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292",
   "metadata": {},
   "source": [
    "We use explicit caching via cache control checkpoints. Anthropic requires exact match with cached tokens and even a small change results in cache invalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_cachy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "a,b = random.randint(0,100), random.randint(0,100)\n",
    "hist = [[f\"What is {a}+{b}?\\n\" * 250], f\"It's {a+b}\", ['hi'], \"Hello\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295",
   "metadata": {},
   "source": [
    "In this first api call we will see cache creation until the last user msg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "296",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "sleep(5)\n",
    "chat = AsyncChat(ms[3], cache=True, hist=hist)\n",
    "rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs: \n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "test_eq(o.usage.cache_creation_input_tokens > 1000, True)\n",
    "test_eq(o.usage.cache_read_input_tokens, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "hist.extend([['hi again'], 'how may i help you?'])\n",
    "chat = AsyncChat(ms[3], cache=True, hist=hist)\n",
    "rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs:\n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "test_eq(o.usage.cache_read_input_tokens > 1000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300",
   "metadata": {},
   "source": [
    "The subsequent call should re-use the existing cache:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301",
   "metadata": {},
   "source": [
    "#### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302",
   "metadata": {},
   "source": [
    "Gemini implicit caching supports partial token matches. The usage metadata only shows cache hits with the `cached_tokens` field. So, to view them we need to run completions at least twice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303",
   "metadata": {},
   "source": [
    "Testing with `gemini-2.5-flash` until `gemini-3-pro-preview` is more reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs: \n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305",
   "metadata": {},
   "source": [
    "Running the same completion again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "sleep(5) # it takes a while for cached tokens to be avail.\n",
    "chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs: \n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "test_eq(o.usage.prompt_tokens_details.cached_tokens > 1800, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "hist.extend([['hi again'], 'how may i help you?'])\n",
    "chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs:\n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "test_eq(o.usage.prompt_tokens_details.cached_tokens > 1800, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310",
   "metadata": {},
   "source": [
    "Let's modify the cached content and see that partial matching works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "c = hist[0][0]\n",
    "hist[0][0] = c[:int(len(c)*0.75)] + \" Some extra text\"\n",
    "hist.extend([['hi again'], 'how may i help you?'])\n",
    "chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs:\n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "test_eq(o.usage.prompt_tokens_details.cached_tokens > 900, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "313",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
