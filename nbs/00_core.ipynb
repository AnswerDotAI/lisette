{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "798b1171",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Lisette Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from cachy import enable_cachy,disable_cachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82380377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio, base64, json, litellm, mimetypes, random, string\n",
    "from typing import Optional\n",
    "from html import escape\n",
    "from IPython.display import Markdown\n",
    "from litellm import (acompletion, completion, stream_chunk_builder, Message,\n",
    "                     ModelResponse, ModelResponseStream, get_model_info, register_model, Usage)\n",
    "from litellm.utils import function_to_dict, StreamingChoices, Delta, ChatCompletionMessageToolCall, Function, Choices\n",
    "from toolslm.funccall import mk_ns, call_func, call_func_async, get_schema\n",
    "from fastcore.utils import *\n",
    "from fastcore import imghdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "from IPython.display import Image\n",
    "from fastcore.xtras import SaveReturn\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f20759",
   "metadata": {},
   "source": [
    "# LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a774f",
   "metadata": {},
   "source": [
    "## Deterministic outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dfd53d",
   "metadata": {},
   "source": [
    "LiteLLM `ModelResponse(Stream)` objects have `id` and `created_at` fields that are generated dynamically. Even when we use [`cachy`](https://github.com/answerdotai/cachy) to cache the LLM response these dynamic fields create diffs which makes code review more challenging. The patches below ensure that `id` and `created_at` fields are fixed and won't generate diffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def patch_litellm(seed=0):\n",
    "    \"Patch litellm.ModelResponseBase such that `id` and `created` are fixed.\"\n",
    "    from litellm.types.utils import ModelResponseBase\n",
    "    @patch\n",
    "    def __init__(self: ModelResponseBase, id=None, created=None, *args, **kwargs): \n",
    "        self._orig___init__(id='chatcmpl-xxx', created=1000000000, *args, **kwargs)\n",
    "\n",
    "    @patch\n",
    "    def __setattr__(self: ModelResponseBase, name, value):\n",
    "        if name == 'id': value = 'chatcmpl-xxx'\n",
    "        elif name == 'created': value = 1000000000\n",
    "        self._orig___setattr__(name, value)\n",
    "\n",
    "    if seed is not None: random.seed(seed) # ensures random ids like tool call ids are deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e05085",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_litellm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfbbaf6",
   "metadata": {},
   "source": [
    "## Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c4ea2",
   "metadata": {},
   "source": [
    "LiteLLM provides an convenient unified interface for most big LLM providers. Because it's so useful to be able to switch LLM providers with just one argument. We want to make it even easier to by adding some more convenience functions and classes. \n",
    "\n",
    "This is very similar to our other wrapper libraries for popular AI providers: [claudette](https://claudette.answer.ai/) (Anthropic), [gaspard](https://github.com/AnswerDotAI/gaspard) (Gemini), [cosette](https://answerdotai.github.io/cosette/) (OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61cf441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _repr_markdown_(self: litellm.ModelResponse):\n",
    "    message = self.choices[0].message\n",
    "    content = ''\n",
    "    if mc:=message.content: content += mc[0]['text'] if isinstance(mc,list) else mc\n",
    "    if message.tool_calls:\n",
    "        tool_calls = [f\"\\n\\nðŸ”§ {nested_idx(tc,'function','name')}({nested_idx(tc,'function','arguments')})\\n\" for tc in message.tool_calls]\n",
    "        content += \"\\n\".join(tool_calls)\n",
    "    if not content: content = str(message)\n",
    "    details = [\n",
    "        f\"id: `{self.id}`\",\n",
    "        f\"model: `{self.model}`\",\n",
    "        f\"finish_reason: `{self.choices[0].finish_reason}`\"\n",
    "    ]\n",
    "    if hasattr(self, 'usage') and self.usage: details.append(f\"usage: `{self.usage}`\")\n",
    "    det_str = '\\n- '.join(details)\n",
    "    \n",
    "    return f\"\"\"{content}\n",
    "\n",
    "<details>\n",
    "\n",
    "- {det_str}\n",
    "\n",
    "</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "register_model({\n",
    "    \"claude-sonnet-4-5\": {\n",
    "        \"max_tokens\": 64000, \"max_input_tokens\": 200000, \"max_output_tokens\": 64000,\n",
    "        \"input_cost_per_token\": 3e-06, \"output_cost_per_token\": 1.5e-05, \"cache_creation_input_token_cost\": 3.75e-06, \"cache_read_input_token_cost\": 3e-07,\n",
    "        \"litellm_provider\": \"anthropic\", \"mode\": \"chat\",\n",
    "        \"supports_function_calling\": True, \"supports_parallel_function_calling\": True, \"supports_vision\": True, \"supports_prompt_caching\": True, \"supports_response_schema\": True, \"supports_system_messages\": True, \"supports_reasoning\": True, \"supports_assistant_prefill\": True,\n",
    "        \"supports_tool_choice\": True, \"supports_computer_use\": True \n",
    "    }\n",
    "});\n",
    "sonn45 = \"claude-sonnet-4-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# litellm._turn_on_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6f62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**gemini/gemini-2.5-flash:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hey there! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=153, prompt_tokens=4, total_tokens=157, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=143, rejected_prediction_tokens=None, text_tokens=10), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hey there! How can I help you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=153, prompt_tokens=4, total_tokens=157, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=143, rejected_prediction_tokens=None, text_tokens=10), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**claude-sonnet-4-5:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=12, prompt_tokens=10, total_tokens=22, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I help you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=12, prompt_tokens=10, total_tokens=22, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**openai/gpt-4.1:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I help you today? ðŸ˜Š\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=10, prompt_tokens=10, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_e24a1fec47', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I help you today? ðŸ˜Š', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=10, prompt_tokens=10, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ms = [\"gemini/gemini-2.5-flash\", \"claude-sonnet-4-5\", \"openai/gpt-4.1\"]\n",
    "msg = [{'role':'user','content':'Hey there!', 'cache_control': {'type': 'ephemeral'}}]\n",
    "for m in ms:\n",
    "    display(Markdown(f'**{m}:**'))\n",
    "    display(completion(m,msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fff5a",
   "metadata": {},
   "source": [
    "## Messages formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671bcb0",
   "metadata": {},
   "source": [
    "Let's start with making it easier to pass messages into litellm's `completion` function (including images, and pdf files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17911e6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _bytes2content(data):\n",
    "    \"Convert bytes to litellm content dict (image or pdf)\"\n",
    "    mtype = 'application/pdf' if data[:4] == b'%PDF' else mimetypes.types_map.get(f'.{imghdr.what(None, h=data)}')\n",
    "    if not mtype: raise ValueError(f'Data must be image or PDF bytes, got {data[:10]}')\n",
    "    return {'type': 'image_url', 'image_url': f'data:{mtype};base64,{base64.b64encode(data).decode(\"utf-8\")}'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef65f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_cache_control(msg,          # LiteLLM formatted msg\n",
    "                       ttl=None):    # Cache TTL: '5m' (default) or '1h'\n",
    "    \"cache `msg` with default time-to-live (ttl) of 5minutes ('5m'), but can be set to '1h'.\"\n",
    "    if isinstance(msg[\"content\"], str): \n",
    "        msg[\"content\"] = [{\"type\": \"text\", \"text\": msg[\"content\"]}]\n",
    "    cache_control = {\"type\": \"ephemeral\"}\n",
    "    if ttl is not None: cache_control[\"ttl\"] = ttl\n",
    "    if isinstance(msg[\"content\"], list) and msg[\"content\"]:\n",
    "        msg[\"content\"][-1][\"cache_control\"] = cache_control\n",
    "    return msg\n",
    "\n",
    "def _has_cache(msg):\n",
    "    return msg[\"content\"] and isinstance(msg[\"content\"], list) and ('cache_control' in msg[\"content\"][-1])\n",
    "\n",
    "def remove_cache_ckpts(msg):\n",
    "    \"remove cache checkpoints and return msg.\"\n",
    "    if _has_cache(msg): msg[\"content\"][-1].pop('cache_control', None)\n",
    "    return msg\n",
    "\n",
    "def _mk_content(o):\n",
    "    if isinstance(o, str): return {'type':'text','text':o.strip() or '.'}\n",
    "    elif isinstance(o,bytes): return _bytes2content(o)\n",
    "    return o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb67a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_msg(\n",
    "    content,      # Content: str, bytes (image), list of mixed content, or dict w 'role' and 'content' fields\n",
    "    role=\"user\",  # Message role if content isn't already a dict/Message\n",
    "    cache=False,  # Enable Anthropic caching\n",
    "    ttl=None      # Cache TTL: '5m' (default) or '1h'\n",
    "):\n",
    "    \"Create a LiteLLM compatible message.\"\n",
    "    if isinstance(content, dict) or isinstance(content, Message): return content\n",
    "    if isinstance(content, ModelResponse): return content.choices[0].message\n",
    "    if isinstance(content, list) and len(content) == 1 and isinstance(content[0], str): c = content[0]\n",
    "    elif isinstance(content, list): c = [_mk_content(o) for o in content]\n",
    "    else: c = content\n",
    "    msg = {\"role\": role, \"content\": c}\n",
    "    return _add_cache_control(msg, ttl=ttl) if cache else msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39f96f",
   "metadata": {},
   "source": [
    "Now we can use mk_msg to create different types of messages.\n",
    "\n",
    "Simple text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6217f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user', 'content': 'hey'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = mk_msg(\"hey\")\n",
    "msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922030e",
   "metadata": {},
   "source": [
    "Which can be passed to litellm's `completion` function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1295ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ms[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hey! How's it going? What's on your mind?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=16, prompt_tokens=8, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hey! How's it going? What's on your mind?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=16, prompt_tokens=8, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = completion(model, [msg])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68485b58",
   "metadata": {},
   "source": [
    "We'll add a little shortcut to make examples and testing easier here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(msgs, **kw):\n",
    "    msgs = [msgs] if isinstance(msgs,dict) else listify(msgs)\n",
    "    return completion(model, msgs, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0186ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hey! How's it going? What's on your mind?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=16, prompt_tokens=8, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hey! How's it going? What's on your mind?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=16, prompt_tokens=8, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e1298",
   "metadata": {},
   "source": [
    "Lists w just one string element are flattened for conciseness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(mk_msg(\"hey\"), mk_msg([\"hey\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e399e",
   "metadata": {},
   "source": [
    "(LiteLLM ignores these fields when sent to other providers)\n",
    "\n",
    "Text and images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e6c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "/9j/4AAQSkZJRgABAQAAAQABAAD/4gxUSUNDX1BST0ZJTEUAAQEAAAxEVUNDTQJAAABtbnRyUkdCIFhZWiAH0wAEAAQAAAAAAABhY3NwTVNGVAAAAABDQU5PWjAwOQAAAAAAAAAAAAAAAAAA9tYAAQAAAADTLUNBTk8AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAA5yVFJDAAABLAAACAxnVFJDAAABLAAACAxiVFJDAAABLAAACAxyWFlaAAAJOAAAABRnWFlaAAAJTAAAABRiWFlaAAAJYAAAABRjaGFkAAAJdAAAACxjcHJ0AAAJoAAAAEBkbW5kAAAJ4AAAAHxkbWRkAAAKXAAAAJR3dHB0AAAK8AAAABR0ZWNoAAALBAAAAAxkZXNjAAAKXAAAAJR1Y21JAAALEAAAATRjdXJ2AAAAAAAABAAAAAAEAAkADgATABgAHQAiACcALAAxADYAOwBAAEUASgBPAFQAWQBeAGMAaABtAHIAdgB7AIAAhQCKAI8AlACZAJ4AowCoAK0AsgC3ALwAwQDGAMsA0ADVANoA3wDlAOoA8AD1APsBAQEGAQwBEgEYAR4BJAErATEBNwE+AUQBSwFSAVkBXwFmAW0BdQF8AYMBigGSAZkBoQGpAbABuAHAAcgB0AHYAeEB6QHxAfoCAgILAhQCHQImAi8COAJBAkoCUwJdAmYCcAJ6AoMCjQKXAqECrAK2AsACygLVAuAC6gL1AwADCwMWAyEDLAM3A0MDTgNaA2YDcQN9A4kDlQOhA60DugPGA9MD3wPsA/kEBgQTBCAELQQ6BEcEVQRiBHAEfgSMBJoEqAS2BMQE0gThBO8E/gUNBRsFKgU5BUgFWAVnBXYFhgWVBaUFtQXFBdUF5QX1BgUGFgYmBjcGSAZYBmkGegaLBp0Grga/BtEG4wb0BwYHGAcqBzwHTwdhB3MHhgeZB6sHvgfRB+QH+AgLCB4IMghFCFkIbQiBCJUIqQi+CNII5gj7CRAJJAk5CU4JZAl5CY4JpAm5Cc8J5Qn7ChEKJwo9ClMKagqACpcKrgrFCtwK8wsKCyELOQtQC2gLgAuYC7ALyAvgC/kMEQwqDEIMWwx0DI0MpgzADNkM8g0MDSYNQA1aDXQNjg2oDcMN3Q34DhMOLg5JDmQOfw6aDrYO0Q7tDwkPJQ9BD10PeQ+WD7IPzw/sEAkQJhBDEGAQfRCbELkQ1hD0ERIRMBFOEW0RixGqEcgR5xIGEiUSRBJkEoMSoxLCEuITAhMiE0ITYxODE6QTxBPlFAYUJxRIFGkUixSsFM4U8BURFTQVVhV4FZoVvRXfFgIWJRZIFmsWjxayFtUW+RcdF0EXZReJF60X0hf2GBsYQBhlGIoYrxjUGPoZHxlFGWsZkRm3Gd0aAxoqGlAadxqeGsUa7BsTGzsbYhuKG7Eb2RwBHCkcUhx6HKMcyxz0HR0dRh1vHZkdwh3sHhYePx5pHpMevh7oHxMfPR9oH5Mfvh/pIBUgQCBsIJcgwyDvIRshSCF0IaEhzSH6IiciVCKBIq8i3CMKIzcjZSOTI8Ij8CQeJE0kfCSqJNklCCU4JWcllyXGJfYmJiZWJoYmtybnJxgnSSd5J6on3CgNKD4ocCiiKNQpBik4KWopnSnPKgIqNSpoKpsqzisBKzUraSudK9EsBSw5LG0soizXLQstQC11Last4C4WLksugS63Lu0vIy9aL5Avxy/+MDUwbDCjMNoxEjFKMYExuTHxMioyYjKbMtMzDDNFM34ztzPxNCo0ZDSeNNg1EjVMNYc1wTX8Njc2cjatNug3JDdfN5s31zgTOE84jDjIOQU5QTl+Obs5+To2OnM6sTrvOy07azupO+c8JjxlPKQ84z0iPWE9oD3gPiA+YD6gPuA/ID9hP6E/4kAjQGRApUDnQShBakGsQe5CMEJyQrRC90M6Q31DwEQDREZEikTNRRFFVUWZRd1GIkZmRqtG8Ec1R3pHv0gFSEpIkEjWSRxJYkmpSe9KNkp9SsRLC0tSS5pL4UwpTHFMuU0CTUpNkk3bTiRObU62TwBPSU+TT9xQJlBwULtRBVFQUZpR5VIwUnxSx1MSU15TqlP2VEJUjlTbVSdVdFXBVg5WW1apVvZXRFeSV+BYLlh8WMtZGlloWbdaB1pWWqVa9VtFW5Vb5Vw1XIVc1l0nXXddyV4aXmtevV8OX2BfsmAEYFdgqWD8YU9homH1Ykhim2LvY0Njl2PrZD9klGToZT1lkmXnZjxmkmbnZz1nk2fpaD9olWjsaUNpmWnwakhqn2r3a05rpmv+bFZsr20HbWBtuW4RbmtuxG8db3dv0XArcIVw33E6cZRx73JKcqVzAXNcc7h0E3RvdMx1KHWEdeF2Pnabdvh3VXezeBB4bnjMeSp5iHnnekV6pHsDe2J7wXwhfIF84H1AfaB+AX5hfsJ/I3+Ef+WARoCogQmBa4HNgi+CkYL0g1eDuYQchICE44VGhaqGDoZyhtaHOoefiASIaIjNiTOJmIn+imOKyYsvi5WL/IxijMmNMI2Xjf6OZo7NjzWPnZAFkG2Q1pE/kaeSEJJ5kuOTTJO2lCCUipT0lV6VyZYzlp6XCZd1l+CYTJi3mSOZj5n7mmia1ZtBm66cG5yJnPadZJ3SnkCerp8cn4uf+aBooNehRqG2oiWilaMFo3Wj5aRWpMalN6Wophmmi6b8p26n4KhSqMSpNqmpqhyqjqsCq3Wr6KxcrNCtRK24riyuoa8Vr4qv/7B0sOqxX7HVskuywbM3s660JLSbtRK1ibYBtni28Ldot+C4WLjRuUm5wro7urS7LbunvCG8mr0UvY++Cb6Evv6/eb/0wHDA68FnwePCX8Lbw1fD1MRRxM3FS8XIxkXGw8dBx7/IPci7yTrJuco4yrfLNsu1zDXMtc01zbXONc62zzfPuNA50LrRO9G90j/SwdND08XUSNTL1U7V0dZU1tjXW9ff2GPY59ls2fDaddr623/cBNyK3RDdlt4c3qLfKN+v4DbgveFE4cviU+La42Lj6uRz5PvlhOYN5pbnH+eo6DLovOlG6dDqWurl62/r+uyF7RDtnO4n7rPvP+/L8Fjw5PFx8f7yi/MZ86b0NPTC9VD13vZs9vv3ivgZ+Kj5N/nH+lf65/t3/Af8mP0o/bn+Sv7b/23//1hZWiAAAAAAAABvoAAAOPIAAAOPWFlaIAAAAAAAAGKWAAC3igAAGNpYWVogAAAAAAAAJKAAAA+FAAC2xHNmMzIAAAAAAAEMPwAABdz///MnAAAHkAAA/ZL///ui///9owAAA9wAAMBxdGV4dAAAAABDb3B5cmlnaHQgKGMpIDIwMDMsIENhbm9uIEluYy4gIEFsbCByaWdodHMgcmVzZXJ2ZWQuAAAAAGRlc2MAAAAAAAAAC0Nhbm9uIEluYy4AAAAAAAAAAAoAQwBhAG4AbwBuACAASQBuAGMALgAAC0Nhbm9uIEluYy4AAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAABkZXNjAAAAAAAAABNzUkdCIHYxLjMxIChDYW5vbikAAAAAAAAAABIAcwBSAEcAQgAgAHYAMQAuADMAMQAgACgAQwBhAG4AbwBuACkAABNzUkdCIHYxLjMxIChDYW5vbikAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAWFlaIAAAAAAAAPbWAAEAAAAA0y1zaWcgAAAAAENSVCB1Y21JQ1NJRwAAASgBCAAAAQgAAAEAAAAAAAABAAAAAQAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAVklUIExhYm9yYXRvcnkAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAENJTkMAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAAADzVAABAAAAARbPAAAAAAAAAAAAAAAAAAAAAwAAAAAAAAAAABQAAAAAAAEAAQAAAAAAAf/bAEMABAMDBAMDBAQDBAUEBAUGCgcGBgYGDQkKCAoPDRAQDw0PDhETGBQREhcSDg8VHBUXGRkbGxsQFB0fHRofGBobGv/bAEMBBAUFBgUGDAcHDBoRDxEaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGhoaGv/AABEIAMgBLAMBIgACEQEDEQH/xAAdAAABBAMBAQAAAAAAAAAAAAAGAwQFBwACCAEJ/8QAQhAAAgECBAQEBAMHAwQBAwUAAQIDBBEABRIhBhMxQSJRYXEHFDKBkaGxCBUjQlLB8DPR8RYkYuGCQ3KSCRg0U6L/xAAaAQADAQEBAQAAAAAAAAAAAAACAwQBBQAG/8QAMREAAgICAgEDAgQFBQEBAAAAAQIAEQMhEjEEE0FhIlEjcaHwFDKxwdFCgZHh8TNi/9oADAMBAAIRAxEAPwCtuF+JeC6SDNsygzareKiAu4kZGjuPoHS9yNididr4p74qfENuLpRSUVdVVeWU8xKySnwygjwtpYa1IuQQSRfcYr4SOheKOR9DoRKqGwbe9j54UpDTTVSxVQZKXWCxQXcLft54a3k/gjEihV+BGNlbJ2Y94WySHOc3io55DFG8bP4CNbkfyrfv+PthXiDJlyWqIiqObGygXZLFbW2I97flhtLTxQ1UlTltQwhhkC0+vZyLbHbv/vhLMcyrM2qWFdJLOyR6QWU3Fu59ffEf81EGCCApBG5GtqEpbWtyb++FquqeomvZRpGkW7+uGsWzXI2XfCmuI28LavfB0LuBZjyZ4mjjMSaSo8TEm5NulvTG9OGbTZb6+m+9/bDcVEaR20KzkX3X6fv3xYPw5pEkyvNMyraekrItaxurwGR0UeIk9lU7epsewxipyB+AT9zqexpzapC0fD1dW5LUZrDSLNSU7jVp1K+k7FwehCkgN3FwbW3xP5/8Oc3yKanWn15k8omYpTxF25UZALsPI3Htt3xa+V0wi4cC08XIQXSOmZDrcHcjTa2k6j1seuCLI6PMsypav5RJKCoI5C1OyOoB+lS4IDC43sd+nni/H4+JuCFWtxfXVdg/Yyr+HAB3OWKj5f8Af1PJVUjVUYZJKiLUVdgDYr128vTFhycHZfxrmT1OQUByOnAZOVG/MTmAdLXJB3U9dwGt0xD1fw/zKPibMstnf93yRSgCWpOtmVm+slRudwTt+eHPCua1fAMwir4m+UqQC00NjoFzbUCtydrgXBsduuIkw8GDZAaFxY+zDUY8QcD1mRSV1KZoaxcupYJ5nRSulZugt2NyL/lgZSBYAWmUBgRp0tvqOOmaeXI+IuD8zBq8rrZatGrSJZXhEukaQ8nQnSwC7nw7DyJ5eliZSyKSUDX1Hz8sZmxcFVqrluu6+LEHKoQ/TuT2U5/meUUscVBWTJSrzf4BN1OtQrgqbixsNvQHqL4e5dNV5jmaTu80swTxSu2skkWuSdybWGGL5JVZZl9DXVQpZ4KxLqsc2pkJ/wD7FG6mxv62w6yWjmirI3hvDN9cXgK+1z5nfYYk58iFY2PzhICDRhpQrkWTy/MZvTPVo8RQJKTpJPU36Bhbp64hqyVY6WSSNWSkMn/bM6/y/wBJttcd8TVCK2qhNDUPI1HGplSIMBy5CbA3PVf6u+BvMcylqqgUEhhJefSwF+XfVYEX9D1HW2MxY2V2yve9fH9qP3qWkg4wK/zH/DFRSpWPU11TyVRbRNYAFjtY37e2E+JqiD971Bgm542u2nTvbfDdYKnJ6QVFVlzrDN/28mtPDuuqyn/7WBv2JxEmllIaUROkQJ2JvYdgT9xvhysQhVj79fb/ANg0eIAkNnEoaQC/fEeV5inG+biWGtKTqUNgwB8j0whFJpRi2DXqLyG2mfLKoDNhKblqLL1xo9QSdugwizFgScMiYi9rm2HWULetX2w0OH+Si9WT5Ljx/lMwfzSXmO5HntthC76yF2uML1A7oNhvhESBH2323FsSGVDubxxCNDdmsbat74cIpN9N1Vh4Tq3OGqTeBA67A36YerOx0WW1rNYCwwlrjkqbGBjpZnKA32J3IwsaZFiR+cFJYlrE9LYSjlkcuzW3/lAtcY30c6RWIOg9rd8K3HCo8hhhsHeUyWsVBWy/c3xJ01DTz6I4njEhYF21MAl+3r74iaepMSOCU5bC7XBJ69vXDzL61OZqq1eQX1SG9t+wH5bYS11HJV7hKI5qWKalzFVEqkLHK0ZugHcE7m9xiKqKKQFV1JVSNtrAYFd/LzxI09ZV1yOtLIsskcYjsNmcX73PbCGYMaeeSB2TmI3idUA3HWx8xiZNHfcobYsdSPePTYNPA8khJBMTMQR239sNGleV3b/ti2ohjyDuQcPkrpgjRpIF+rSzDbzONI5YIkUEAORdrN1Pn6YaDUTVwNajpI6OMxpJ80d2bXZT+ONqfK2qFjpYKaRq47gmULqF+tmta3nhWlekkp+dLJNFKEvDy1DaWB737bYkqziuo4kMUmdlagwOFja+kjb02N7dMdS2F1Pn1VSNmRVPltdSVAM0UUQhY/6pDLcd+/2Iw0gqDUVUrVtS9OtrNMgLFu1j33wmk8gjsJHILWAPYb48XVbxbqDY9rjDADu4LMAKWIiAVEjLCgjDsqrv6279ziRzXhwZM1OKqVC0l9cam5QC1zfoe/4YQlmgdH0agvVreEgennho9VPVyhFMkkrOAutrkb2AwY5E66grXE33JjKuDc3zL5iopctqaqkpjaSVF2H+4tvthPJ86rch1fJTN8s0gdoyLq5B6keeCT/quu4cjqYsonJDSPZHAAjtsGF/Yf4MAkuoqJJWJdiSRbvffCsD5y7FqrVVd/NxjBFUFCb94ccPfErMslrK6qEMVRLVSiUF92TxhiinqEa1jbe3lg7rPjHxDPM9BRUyU8tZDHNVLLHZIJdZJK3uGRoyFIPXa24wC/Dykoq6SZI3nknZQainaj1x2B8LK4N0YHoTbc2sQcWFmvD4zp5uTIlJmb6Qz7KbKCOWVuPxPTttjpN5D4UVPUouaH5x2JXyAm48oKf96CjrWWiindHaDXULHNUxqvi1KSey3BJHQDDHiKiymegdc3knpIle9pA0aCS22twrWNiNjbFd55S8QcDV8/zcaU8uYQPDExUM4Q2uV8iel8GGdZ/nVbQPnlPSw09QtItDmFI6NoqYiAqBomAJlRrm/wD5LpuMFgxLjFHlaggi7Bvd/n/bUY+fkCtfpKoinVJY9UjuobZQbEjv7X/y+H1RBG8cM0c2oPcNEPqS36389seUmTs0zU9Q0lNWxy8hoOUdYbyN7WANr98aZjltZlTQx1sckLSoW8SadgSNj3Fx1GOcyNdyAKQt1qFXDtMmaUMlFT/KwzBlcTsTrBHWxHYA3097YfHK63JIvnoa5JqpY1MgdrvpdR4SQfC1j0/4AZRZjNlxL0cjJ4dDjURrB7G2Cavno80y6TM8slgy6q5apVUp25yjbw72NjbYgbEb4DHiBJrRG7v7e3/H2lSOCN+0IshzakylJRTSyzGVv4EjDSULC7E9b+La3pjQUtPV5s+Y5lO88shBVCFBA+2wP6YHChraGhShkl5hJ17BQo3vYefTfBVlmTpAEaotGFFhrJ29lG59ycT+R5RbGMa6H5To+Nj5m2Fw+yqvpqmIRSwWUWsHZSG/92GJV8sy+oheOSlQKwtp0jcehHtgOpocucCzszdjyyP7nElzkoU8FQ4QdAx/tjhZCQbud7GAdVIziv4Z0vETs9O4o6lnDGRU1a7CwB9PbAr/APt/zmojkaHNMtjRbW5sjLcfh1wcx8Scl7NJ4T3viVp+JYyLc4D0tbDcPm58QobEVm8HDlNnRgRkf7NkWYqFzHi+lpJjtpipHlA+5IGLX4b/AGGsjzQxms43rpEIuwgy+Nb+xLH9Dh5w3BNns6LEguzKVcCxBuQT9v7jHUPAXCE2XxQSvIQNjpHbzGOz4ufNlP1CcnyfHw4h9JlGj/8AT+4DNOB/1BxHzdP+pzICCfPTy/yvgEzD9gPNsrM03DvGVDWm3girKF4Sf/mrMB+GO/UgAQAja2GksOkkWuOmOmdipzABc+V3FH7OPxL4WDvWcL1NdTq1jNlzrVr+CHUPuMVfX5bWZRVy0uaUs9DUx/XBPE0br/8AFgDj7H1VExRxHcb3BHbHM/xm4SlztHpOKMoizywLU55JEqi+3LdbMnfv23xJkpBcoxr6hoGcAQFQSty3ffthzy9R1Bh0sCfLB1nXw+gpM05eVVBINx8tPKNaHuNQ+seosR388EdH8PKWoypovCk5Xa/Y9r+vUG3XED50Xdy3H42Q2KlYRUzxDmrd0XdxboMec4EFtAJFgpvi9M44fyah4cai5emELd5F2bbqb+Z6YpzMMuanqW5cfIi7ISCbHp97YSmUZLIjXwnH3GskkXiEaEra4t2J9cPqSoRI0aa2m5GkISbj2+2GkdCqxo7SOFJIZLA7X69cSFLAs/8ABvy+rqxTt064I1UFY+o6iGQPrtFbSWHiO57bG9sO66Coo2C1DKqFNeo7kL+N/wAcR0uTIoD8y0W13WPYgffD9Kaasp1pBoc72kMJBUX6YUR7iMB9o0dYZI+YFN1BGpbAknsD3wyFRBQloqhhI+okkIHtftfDk5TJH4hGXiVtLFQ2kDp+vlhSny7LzHeoZAxP88ljbA8gO5oUmVtdo4boQTv5AAXx5azKskiJyxzBcXGrsNvS2NpomlZIpCqEMQ3Tc3wznlBkeygAenXHeAufL1FZphG76Bfc2GPYYJq0yclGblrdjq7Y3jozUTFNSq2m/W32PriVyGhooczRc/ephoZFdS1LYszdgR5bG+AZgikjuEuMncgw4QaL9fxAwpSymKsQqQhS5Ut2Nuv26++Cmm4AzDMK2gFFyxR5nHLLTVMouQiMQVYDo9rbeuHj/DDNaf5yeR4Y6WOaWCmmlBvUsnWyC5CgXux2FsUMjLjLkamrjcnQgxFFDVCX595VhPiQxKC1+gvfthJ5FWkjUcmQJcKoQ3A9b4ThR1kYzMVK9QO1u/5Y2pq8RLNrQSMwKgWtf++E9dTAb0ZOcMca5pwxpipZWSheXnSQoAu5sCwI6vYGxOy3uBfFpU/xkyCrjqFq4ZaSAwQrGBDeQTHVzGVh/SdHXqPwxSLSRJyxStolt4ypP3vhCoSFHRi5IKAgAeeK8HlZMQpZoYpJvPuIDW54k+VSSRw08oliZAYwjXB1BCdKNcb6bKSLgC9sXyGkp5KesraepikqlR4pKuNA4cX8RVbqG3uO18c000EuZ1KQU6qsjtYK7BV9yT0xanDHGFdkNTl0ObzVZWlVlldKpeZMHYBSjyAqqKEA3HcnBI4IIdytkGx8H+kdhYqSalu5F8PssqpaOMNN/FqmqMzhqYlabWQTrVzvo9t97emBD4xfDhKDKIK7KJKqtahOmRjECml236bkrsSwFgCAd8WdkfEaZsMxgyHMMvnlomQVwVmlpixBOkVGlVLC3VR374kKPP5ctqWphK9PPMGGtJRddQtqXazEe2x7Y6nmv4mLHzyro0oYC+z8e1jfzK1xeqpVTOKqmOrJBaJmjsAjBbi32xKUGW1FQ2p0a/1G9gCf7DB5xvl2W8N8SSQ5fCtLTNEpCJPJL4wSG1Ft9RO59+mBw1bSAgNyl8yT+mPmPI54WOMjqFi8VG2T/tHuXzR5XpEQ507G1k/QYnYKZ6phJVVGhT11NtfyAHXA3l78ybl0zc2Ug6nIsFGJmgiiec8yQTOpAYt0Hp/6HXHLce862IUKENsuWmy+IchDJIR1ZbfliJzyYVU2ssFb+rVsPxwtWSGKApAq6EFnYnTqPrbt6YAs0zMIzFIqdbGwYktv6euI0RsjalxZca7krUVjQbGujla9rCPb8Ri0fhrw0+dIkuZVaQU7NZWWHVY+uroD/bFE5dHV5lnVNSs2uTUHcDYRj28/THZvw5kyimoIo3pI1qTHZ0dbiRfT/NsXekq0D3IfWZrI6EsTgLhlcrr4oa9EYq2kOqAAhha+wHcC/li/MvhWKNSg0g21D/y88VvwzNTPDGv8gUKl9yo7b+mLFoZwVAc72tjr+OAi6nHzsXazJiwI364bhfEQbA74wTBiL9xj1X1am7nFFycTyWNVTp2vgG4zRpqGeGljEkzLtftv54NKyQCLTqsxHXA1m0ixxGOO3TxN3/HAnYM1dGcVccZLR5RmMsUgR5w51DQNCHyB7n9PfAquaU6NpCqGHSxub++Lw+LEOWutSDMxqRYOYjpWK/RQe7Hy3vjlnMKxqbNHijYizdH3NvQ9/wAMfNeRgtjRn0vjZ/p3DqWSKZQ0/wDEC7qna/rgD4koFlnec0kxksbMguPwxMNnRESgIgIFrmQg4bGqqPrWQhP6iLr+uEYuWMx2UB5VtRFVQs6xwyAarXYbX9sbfMSEpFLBIzDoYxYdcWRVr83E9mjWYjwSgXF/XzxDCOvpgSaqIaFCguNF/uf0746mN1yDrc5OTG+M96gvFmkVPIoho5KmNTqbWLG52sLYcSZ8ZXjC0eiEbWK2J+/tgkT52UQiJ4GdDqkj0B1vfuexwuGzPW7PErwQOpSOOEXAHWxwwoD7RIYj3goM45mnnQCI72207jzPlhxTZsksZMrwxkGwVKYsAPe+CWalrDS6onjnKklLwA3U9eovcHr6YUpp6qZC1OBpBteYAMT5207DywsqK1GhjdGVBVsnPkkp/FcGwtax6YbxOI5GlYgsG/h7X3v1PoMb0TAuEtq1baffCU0dpXVbBAbCx8sdVRWp89dkmbUupqidy7FgvXuSdv1wtTsTXx8pGciRVCLcsx8u+5OPYKadKaWpjjfS7BY7kX69fx298NkpWjqDCsjRzBwLFSrBr+XUG+PLRJM9sTqulpjR0KVr5PU5Ssr8ySlqoxEUl02LCx/m7na+xth9BkUXGeVmnmqJaGCSMpL8u38TSTdkU6TYMbXI3IFvPFH8GfE3MskzPJaWeZpssjqgtXCzgmdNR3dyL7E3+2LG4m+OVPwzVtQ8Iw2rYp3p6pYtBhmUfRNGwBGrsRYjvjs48qvl9ZshCVRQgd/e5eMqcKlHcY8Pw8M8UVOXQ1lRWRBv9SWmeEkEnazgE2/qtY9sQfKEbNYi8a7m/c/4cO8xzeu4gzE1Obzz1taZSXmllaR2BJIG/YX2AsMLNkGcT0r18WW1RoF1HmiI6fD9R/ztfyOOWw5ueA1ICLOpDyPy9v6upBw8dKRsqVkEprg1lN/BywT+e+GtfSVFHUPFUxtHKrbqw3Hf77EH74cBpBBAoa0hH4Dz9AMAwqeGhNKFGhJqJNS6D4durYdTTU9TE7TNLLUki2tj0/36YcZjRxw0cMkVW05k/rNlYdyDfrfrfzx7kq5XDWqOJzUNSlG8NPIA2q23nthYaxcZxZWo+8fcM8X5xw1A9NR1WnLmqFnkp5Yg8Usi2IunUjwrtextvi3uFvjstVnGd1nE0dPAgoYjQUoVrNPGSWW9iQXBYav5TpPQYoWlkWOpVRKwphKpLutxpva5W+5t2xY+bZfSZhlE5y6OnIhd21ohDBRvc+4PmcV4/I8jFvHsf073XxHYQWB31H/xJ4nyDi/ihsx4WiqFjaCMSmawu1uw/lIuVbtdbjY2wDy6p5REr2W/iK9hjSlPIpQuoFifEb9cJCqSJySeZv0UD9cc3M7ZsrZD7zq4zxQAyVlrly2iZIBpllIVSOo9cPuHHaeeKKNdfisiD+Zj3OBqXmVb82UaV6IowXcJUzQMTFfmMpC27A9T74lygKnzHYiXf4hRnFpIXjDgQxCzHUPG3e3p+uASWCKhhqMxrCDMgIp0Jvo/8rDofLBZxRIKVWjjMemKyOzG4DW3AHc4rmtq1lPIYFkd10qLi4B3PtfE/ioWHxKfKcJDH4bZDFVVZrauVhKW1AkW38wcdEZXnlNSUxikdUbquobBv6rjp7j74qHg6SCny1UjiUSWuT2H2/5xrmVZVU78+mqVOk/Qx6/hhWTIWymHjxccQnTnA3xPWnzOGizDTypHsJAwOk/7f579I0FWskSPGwYEbEdxj5t5Hms2Yyo8EpQBrOL3KsDtjtH4WcRSvk8cFVIZGRF0k+uK/GzENwaQ+VgBXmsuZKk9jj0VvL5uo9ALYhVrBYWNziOr8yMTugbcnHQL8ZzAtyXqczLuSWNhgP4wzpqegk5J/iEeBL2JOIbPON6PK0cSzKWTqt/wGOcviL8YqivzAw0eoxXtZAT9yRiZvIC6lOPx2bftHnE0tZNK71QGrUxBeVYljHfStyTfzO574oHjl4qPM4Ji4EbHSWVgdJ8/XFhfvda6Fnaop0dhc60ZiDireN4pgrSnRMmsE6Lb/briVSGcS8gohk9SZxUrBGoaKRLeEmMEEehxMCRqmkLJIgNt1tsfQjFdZGzUyry5Q1PJuEc7XHWx7HBoQsmXvpkK3XwNqsVbt6YjyYwr6luPIWx7jLl/LAvSXCMd0PiAOEJ4KmpjAlkVvFcAoLYjcrzKSSlcOdMiNvb3xLJVc5L2VTbquGEFDFCnEio5M3oZppRUuhcgFlXYjV3/ADxvJn2ZRzNor5DqUqA52W/Ww/TD+aoHKZAWNx1ABt+eIOZEEgUFFckEEi+3kb9Dhi5GMmbGo1HT5/mtrU+ZyqYyXAY3uPK9sb5fn9bFTaDPO1mNrSCwHWw2xFyzI1lWy+I7EgEnphVNAQCRPEBvY3wXNqg8FuNOEvh/UZ/V1cM1bHl01Kf4kDqTMD2fSbXTsbG462OJ+X4Wz5bnL5dSxjMvmJeVHU6QI47gG5J2B637+WGHDnxBzk5nQ5ZFmFFQUpmHzNTNTqb+bu1ixPYb+QAxdAzWLIVoIcxmiHMk5fhXQWZuhFze58vXHdfP4qDDjyA8nYDX+L6+Zx/HxK3Jl9vvB/JsnyfhLNmkpqVa8RaFjSQhhGwO9gLg3IvfqPTEVx5ltDLllPm8VDGkyl3eo5gjMrO5MrAWLSMSfqJAVRsD1xMcSfFzhGiqa6mehqq6vSGSO1RSaBFLayrZjsL9dsV9x18UpuLCKOgp3yrKxAnMpgbI7kDbSNrBr2IsbY6OfhjGQBwVPSgdH3JPvBLoo0NiP8q+G9HUiGszVnpVqYdZUOFEZ3O3uCp9wR3wtxT8L4skjXMOE655cxjjab5XTzNMZJOot0RQndjvv6YjOFeOAyUuVZ40jUaqscDqNbBiQo1Mx2QAknY9AMHf/VnCFTV1uQtTZnXPFLKsNXTcuWJwBs5V/D6XtYW8sDjXFlHGhxI73yv3FTLRhfvKKyqr0VktTUIWVlLmQWGliO3nc4tH4VcVT1dVPkyy0NJCpvSQuHM07MT4V6rt3vbY998V5xHw9WZPFS1FcUhWvD1KjUuqxa3iVRZSL7AbHe22IWqrIdMLZaklNyxd21WJPv1xHgyN42bkB/5FK7KKudLQcNcM5rBNV5zlVPKGaNUlLxlRyyYwgYuBYEEdQCbAg7YoHjrJ4sm4praHLaLMaeBSGCV0aq9jc7aPDo8iOuHGTcfZpl2QDLFiiqKeKaQojr4THKhWWMgfUGuG67FRbBDxJwZm1TleV1uZNFl8EsCRQrKhEqgAC1u4O7XJ872OHO6+gqKCa7J739z776jD+OfpG5W7RgiRPCoAFiCDbzw2IsLyPta4BHiOJHNcsOS1BpfmIqomCKW6KRpLLq0kHuL4SoYoknY1sHOTRuWawDefXEp+m5NsEgxGgaSSri5bCLQwYH+m2+LbyLMos6o5aWk5b1TIUmhBJbSe+w327i/UXxV+YTZcrwjLoSIkjtKGa+p/MHz/AEwbfDzhueugfMFEYJIWlMMzBonUhrG2xDgFWBOsXVhtfDcIyZbVCRf/ACJVjPA0NxbPPhxmeXU09VS6ZqeIcx42dVeNbd1JvgQji0WWMBn87YuH4j8P5zXcPGukK5gYq+omu8aq1LSFysSoQAd7Eld76rgXF8VOkQjYIJAxNr+ajywHmYxgcBRQl+P69zWFSswBOuQ2A74POGEaGCon0ljDYj3tt+ZwNU9PHCTILM/0r/f/AD3wYZVMKChZqkXVAGdT/OQNhjhZ2sTo4E4mRvEFFyqaFJrtOVDG+5F9yTiv5I5JM0OgCyBQPbr/AHwf5xNLKvNqZLtJ4msd/wDgdsQWX5eZD80gBZm1FfIdsF47cFJMHyU9RgBMn4jrsgphFGn1DYhsN2h4hqaGjzGoNMkFfzGpg9UoeTQSGst79RYXG/bE5neWRT0YeVdnFiQb28jgAamkpZgJoj18LgEg+xGLfGXE68iNyHy8mfG3HlqGPC2eT5XnUYq45Iv5ZVbYkdsdefB/iiSpEEUj6WY2AJ3sB0+2OLYy8/y60+uSSKPxMbne97A+mOu/2fOHa2oSCZouQDYcyVTYjyFsS58NZQySnBlLYiHnTQreTBdbl2AAxC5xJOIaqpKtpRCdvbBeuVxx8uMDmOerW2GMzLJvm6Gtp1XRrjKhrd7dcPbGzSEMAZw7xfxW1HLU1lZUGUK1gn3/ANv1xR+d8bz5pO3LCwLfY+ntgi+KtdNTZzV0M45ZhlZGCm6kqxF/xvbFUIVFSTURtJD3CvpPvifxPGDWzS/yvK9MBVh/kmYCUamqpGfyLC34YfcQTCqyqS4R7DoU/wAOKx5hpalHy93I6lSemCCozqQUTx1IKh069be+GZPHZXBBgYvJXJjIIqPsjkhr6SWimYwzKebCx3FxsQe9j/tiehkalpmpai2iVDpcHY/7HAFlrtHNE4O4a4N7ehGC/MJWiy68hPS6nzwvOn1RuB/o/KR1GzQPUAGxuCDcH9MSUMhur7hH8uxwN0c4So8d9DrY2xOZdKYpOXKQ8T/Sw74zIvcLE3tJKoaSJOby46kdyBZgPcYiK3MoZYiQo0rbwkeJfY98Tq5TFVSMFqmpd9WpX02979cMM34KepjU0ua0yyMpJaUaAw+3fHsWMN7xWbIVvUGjVq1wQWsbg9cOFzSO3jFj5DDKs4OzyhXUhpauMk2aCpRr/a4OB+WoqaeRop0MciGzKwsRiz+GDdGQfxTL2JM0bpRzsdKz1VuY9xstug273xpm/Elfmzn56aWVVa6RhrIp8x6jDFlaOMRIAFYgykm246DGLopo5QbSGTuOg38/PDlxry5nZnLDGquKVNTJmtSavOKiWSaQqryOdbMALA372AGPZ0jjlMNJKZItdlkcabgdCfLDM6dKqthGr6iLb4d86CSnPLSzK2q7G+1+nvg2Ju4PeolGzsrLH9VrKF6Li6OAcvoc24btlFLI1dBOEq4rcxpPD9Qby6+Hp274plp2U6Y2LR27ixJ98T9JxlnOX5JFlmXZk9LSJHINMQ0XLNdmJG5Y7bnsAMNxMq8g10QRrvfz7Q8bhDc6FoOHf35TNFNHDHUwobNUwxM8SnYsI5CQNrjVY9O2Ky4c+F2UcRRzxU2dkSRGRGQpGV56E3BKn6WABVgTcXGAfiXjut4mzHLZYL5eaCnEKSRSEMT1Zi3Xc3NvXCGS8TS5PmBqMlaSBeXaXxbSDvf0vig5seNAnEvxHZOz9rjWdWayNSxKb4fZnwfNlvEVTlSfu+nQSRRfN8uZZGGzbXsF269z3wcR5PHxfSUVb8xRZlWVWuRqaStL3I8TA6RsFuL3B3NtthgVPxgoqzhmmoa7LY8wqebrqKeUMqOq/QisDfc7/bFU0PFdbkOc1dfwyf3aZQ6CNm5hRGa+m562sN8D+C2P0nFqQLF7B70dQxkGNrUy3fjbkdDT5PR1pphBmotA0sNmVmUf6bjbTYbqw28+oxRyxPydbKxRG0NJbw3sbC/n6YIzX5txGJjnc9RUzmRX0iMnchVBt2J8IB77YjHo62i1UOZmakjY89ElU2JFwGt62IvhWZ1ZrQUBQ/4+YrIOR5feROrlm7JYLuRax/8AWCrgfis8OZiJKqgWrpZotPMB0SRqpO4PQi5NwQb2FrHEdwtwhmHGOZtQ5QLSXDPLI2lFuwBJP39cOs6yWoyCuFBm0DrVGNXKF7XU/SdvPc22xgL4wHA/3gpyXYnVnDMUnEf73pKavizBYm0SCjqlSWMW2YEq1gb9R+Ixzrxhk9Nl3E1dT5fFJHHA+6TVKVDX6El0AB9rAjvviQ4Kq6Wgelkyesqcvr2IM7RzEMgB8SjYeEjbfETxVEaHPquSBzIs15dTS62a43LeVzfY9rYF/SHhrixLXE9XdA3/AL1OyjNy5N7zSGoWiBkkQuyi58h5ffC8de9W6i7FWsQL9yf8/DDKMCuURC4hcgyN3XbpiXNDFQx3iN3J2Nr6f/e/5Y4r0O+50EsjXU3r5TJGRsUTdj3Jt09gLY84KeCuplhkYcxR9N7fcHDStqUFPOFuI0j2Pc7bnBL8Nfh9V5/BrykCpIANozZ1PtfGBfwzMZvxBHVdk87ppSO8JH1FgbeuBWbIi9QUgD6na1h1OOpuG/2ceJc6p1+fqIcvgNiecSW/AYJ4v2ZY8uUClrBWVbGxlaOyRg9SB3PvheMZkF1HM+F9MZy38Ovhfm/FfFkWWZKjuSCsz6bpEvck9MfSnhDgaj4aySgy2mhT/tkUFgOpA64YfDn4e5L8OsnWHL6cCoYDnTMLvIfU4NWzSKKLVcA47GNKFt3ONlyA2qdR7S5RChDMBcY9qKWAxyDRsRY4Ypm7vuNxhRczjkVkIF/XFQZSKkvEzhX9rf4U/IStxLlVNqpmOmqEaj+H5G3ljjGSikmBYsybeW2PsD8QuH6HivIazLa1dcc8ZUjuPUHHzJ424CrOEeIK2glQskMh0MF+pL7HEjMMPUqVP4gb9pXdDlt33Jc9yB0GCKtyf5rLZpIxay2G2H9FkcjyJyhpU/zYMhl0ceXmnILeEgkixxBl8i2BEtw+MqqR95TeVIXDJKPoO4wW1qF8p5YYsbALvexHa/cYguQcuzaRGuv8TY4no5UnjaG+iRWJVSbA37fjYj74LMbIImeOKUqYOfLSQkNGbrf6T54e08zwWYn+GTuDuMLzU8ms69Ora6qb79satTGnp7uytqJ0r7G2NLX3NCcbqP8A5/VArta4cxna+2BGpq5JmbWxdb/zbj/1hStzCWVBBFFOkSuSSIzdjhkRb6Emt6xHbFWHFw2ZzPIzeoaHUVQ6bX91Hlh6cuGZBZpFZ2A03AJvbDKCIyG3LkF+gMZwZZPT1UFEFjpZmBYm/Lb/AGwxjXUnRbg7keRSZ5HXVhmgMVCmuSJ25bON9lHfy++IOvkhNY8lLEaaJyf4Y30/jhxTyOG5AvGOrINunS/nhGuax5Kr9LG58zglBDbiCw4hQIxjfRqsNX9OJGaNYHZFUgmw0gdD/h/PHtNlbTSQCVTBH1dwLg9T9sZUxVFTM/OZQNdyQ4YIOwuDbGlgT3ANRqCXflpqSykm/bD2fkk8tTKuw8ZUHfysOuF4J6Xnli3Mcm8khXZVv0H5YZTc6WtlejewUkIwIU2vtjOzBqJTQtGjKSS2m/Swt7YUgjkSByqXD6fw/wCf0wusExXmVepkUb38u3640iP/AHHhbwKuxI/O3ljb1PAyYoqRiIZEGvSx1AGxBAuF+9x+GPaTM5aLKamlkpqC9U/gmliu8dtrA22BF7jCGX5ukiVNPCjxgRG7Bt3Hc9OuFJKnLTGkU1VM3KAPLeMkBvsdziWm5UwhqShsSxuDeM8tioKKLNmppcwp4TTLNzCGeINqUNtvpIFvL0xE8XZtw3VDMp0parNM2nsiymUQ01IqiyhE+qQgd2IBJO2ACTkxzLJQGd5S2p5HVUA8gFF8MopnFTKrG3MurX9cXHNlfs6rqv1/OEXteMsD4bcUHhuurIo5IlppirytUPo0kD+Ve7EkDbtgn4r4tyTP+G8wo6OsJzPNKyOWqIhFykf0h5D/ACgAKqLt0v3JqZaCqpJkWSP/AOnrUqQdXljww1CuEglEkhvoiiIJ1ew6nA48ziwrWCK+PzHzDTIVHEiTNBWvk0zmFxPEHBkWwUNYGyg72tfBLxjWUlZQ0k2XyQSwyAC9tMkLbFkPYg32NuxwN5Nk1SKXmVcDrDIEcSEXG5ZQT5XKkWwUfuPn5eUVdW2wtcHGMSqkEbPvKsbkH4gvRiRahrX5fVvxwQwykH5Z42blLzJCdhv1Jv1xBR1L5fO8FRb+GbaCtt+1/PG81TUVkLlEM6FyQq7eHbv5g32xzXQsZ1EcKJtmTER1Ka95mGy+IgE9Pf8Azti8/wBm2FIs1eAvFJUkDl8y4VWB6MQb9O2Kmy/Jyqs00eqZU1bi9m6D774L+CKit4fzaKpp5VgAO1+wHXYYMAAVJmbkxM+j/D1BVxyyy1dVDJSMqCKJFtosN++98EvzlKPAoIANt8VXwDxhDnOSU80chYldwW322wSV1cxppGiJVyNj1GKlIURFFjuPuJMyamV2pWD2Xp2OOT/iR+0ZmNJmVRlGRx6ZYm0SGRTdWHUDzxetNUZnU0Uq5mEvuA6mxI87HHz0+Nctfl3xCzsUjySL8wShTsPbAJ+I24b/AIa6l25V+1JxRlo5dUqVA8z5YJOGP2sqmbNIqfPKcRxOQOYLGzE/pjij941aAGV3WTqwbqDjMvzKqqatEQsXZwAAL3OH+mB1JhmJ1PqrlfGdPnekxyglwCApv1wDcb/D6g4mzlJqyG9jZnXZgCOuGfwmy05Tw3lkeZShqhIVJA9r4swVKCYM4FnFr2viWua00rs4zayhsy+BNBQwyTQPJNEASAm1vwxUfEeWR5OJgqU4iS4IeVlcY6H+LnGi8IZRJMt2kkukIQkAtbv5D3xwRxZm2aZ3nM02bVMkxZtQF7IvoB0woYFJ6hHyHHvH1QP3lmUsnKVIwNgGvf1OEJE5cgLkhFsSD2t/xj2hUwROuo30jbrthCod9Cgm6s3uDgTtqEeul33PaGqMlYWa/UsB6/8AGJfK6dVro5aldcKEN9QGre4HtgfjcQSKY7Eqbn2xJPmaU0bNaQIviKKvXBV9WoBakMM/3hlg12oY21dfEOv44QGYUIksaJGXzLA/3xXTcSKSfHKP/iMJniCI3u8p91BxT6bSLmss+LMcu13+TUALpADH+2H8PEcVMnLKVK9wFk2+22Kh/wCokHR3t6xjC8XFrRrpBDAdNUQOM9Nx0IPNfvH+YZIyxHM6SmlbLlblpU6LK7X39geuG+S8FZ/xI2ZT5Dl5rUoKZ6yskDKBBEDuzaiB3Fh1PQXxJyfEqpbhaLKTEvKhj5LEWXWDftbbtgbkz/MMtppocsqZIKXMI1E6L9Mqq11BHezC/vhGE+UwYMADevfUldcXIcTY9/zjyopOTRxUEX+pI5acld7i2x7dDiEqRGlUaVHvEDpNhYXv+fvhegqqp41ijlkLytdvGdh54apTvzJJBG0rtIUiFv5u5PtcYtReN2YkCzFZI4YEaLVqDsNR67joMNtMAcrpN/U3Axvl1O1VVNFsbAlhfrbG1ZFHDM4RyXvZ1Itb1Hpgxo1c9uSlKyVVYx0gUCRaX17EIOuw7k/nbCVevMWWaJYoYyAEUAiy9gDh9kCL+78wo46KmmlrhHEJqgEGIXJ1ISRbtiCLzUcskKyuCvgIB2I329sJX6nIHtMqLZY6CoXQPEwZRbubHHk1AkSKt2kntqa5AAB3F/W3X3xmV1PJrYnpwUkVvC6WBBOHtRlpiq43NRrLESWINmF9x74I6fuejX5ZCgc6YnBF7MCD67Y8nplp21GRC7Hbfp/lxiUocupSbTF6mAy6pFhFyo3FhbvvhtXZa0tTK8dPPBSR3IMiaCQB6nrt3/8AREOLIngNXN6SloGyKqnrqqRq5njSiiA2J1eJmPYAbAe+JvIKCWCUVjRpVJEQrTIbcpidiD1Hv64EeeHQsxXl2ssYbsOgwYcA5zBTVYoqyJOdNIBENBZp3c2AZidKKBudrnFGDGS+2qNSj3LQyKvgmjaN4YZ+W4ZlkAKK4JIYj+axJNul8TUOXioqpHp4AYDa5sb6u5388a0s2TZFWsJJflpJAjI6prurXsQvcEruOu4IwX5TQTV1VJPT2RGPhChgB9juMW5ywwccrAtft9paa9oJ13A1LmlnnpIXbSQHC774iKf4crlk5lQysbWRm8QQemL4gypBEOZBd7bkLhvVZcArFFIHqMcdkhqZTMmQmIu0q9V6g7N6Yi5XOTyQT1i3ia3gA6+QA/wb+eLHzyn5EUjEaQGv6HALnTrMAropfSBta1uoA32PXCqhXLQ4F+J1LRPHbTECLNGHFz7+X546ByTiSDM6JJo5BIjC4W++PnyrPRzSVVOWjlBsFPQn1Hpi3OCPicctpk5sphlRFLsx2cn0xoBmhhOqMxroIlcq13I3A7DHJXxp4DHENfNmlDMtPIoO2/iH+XxZZ48mqqfXylm1j61bt3wL5nn89XcJQh1udmPuL4DnxMbxDCjOUhw/X1GYmmjQyPe2s3CnfrfFzfCj4S1UOcR1/ERjMVO11j6rfzJxKwpWUdass2WQhVO2hemDDK+L4aILzEMGx1bWv/bDWz2KEQuEKbMvbK62mgpFvDdR4SV6rhZ8zijktFJrv0xV1HxjREbVa2ZRffYj1wG8cfGGlyRWgyqRZ66RbJvcLbrfAqxMIyK/aNzyp4mraXL8oilPyJYzyJ5kdCv8wt3HTHOnJmp3+XnVopNyqm+x7fY2sfscWvlXED1czTSuX5jFmDG5Vj6424tycZvBDXUMIlrKRSbAfUnW9u5Hl6+mDD/6SIHH/UJVscpnki0qxJtqtsCMSstNzKN9brIVJKKvUH/P1xE5YHaARqoDk7g9rdsPpamDLJnNQ6cw78rzHb74nYEtQlqsAttIeSRYGLcxVK7EOCL+mGeY561TSGlpoVhiP1MpN2Hl6DGmZ14zOZZI1aOICyoxvbzOGBTTfHQTEKBYbnLyZTZCnUae+MwqYxc48Mdhh8nieM2xtpx5oONnpIaRUUTukICqbuwBv5WJ/P8AHDnLcvTMKeeFP9WNdaEvsV7g+WJMQz0lAlJChJ1F5YxfRqIsAfMgX3PmcRdHDTySSPG5ieM+IHZbdP16+2JA9gxdTxgsDRxU51TKB08x39cTtDl8k2UZjVxLHJFGUeSPVdi2oA7DtpJ3xF5DTPWZnBDF4+Y/L3WwYnp9vPD2qH7pzTNoKB2kpl1AKN0uW03NutiSAcLYm+IO/wDueTWzI+uZMnzOpfLomeGOdkhkfup3AtYX8JHXGAPX+J6FZJQtzyAwbqBuBe/UYmaSh/eeXKWV2Xotze0iDb8VNvtiMmj05hNFSAorI4BBvY/UB/8A5GMXIG0exPGriFJC1cTSU4lb5hty4uyW/Uf741pcqkiHNrAIyR4Vbr+HngphpTNnNDmckcdPRUa0ytBFtsE3F7WPiFzfcg4jMyRMxqaifkTjLqV0jM0y8vQWubkepBt12tjVy311NKkDUYZdLT0dY0rxLMgQqkTAr4r9fP8A5xs+YiU2dUWOTcaVuoI26H2wybMVa5SNVX+UsLk4yeWOJWBieOeNwWRhbSD/AIMM4m7MDdR+a+qjhVaVpYgw+tjb7LbYYXGfZkhjR66KWPdpFAVrqBcggjcWHfucRvMSaBkjUrKUJVgbXxGwowinYXuUAFx5sP8AbHlxg9iFQk1mtEKSqVJW0RSeKKyhV3/v0xvlmTtJVK3PSSzXPYj0OGkdRU19NFRursYFeV9TXLL9XfyF/wAcF+TUFNWT08tRI0ck0aBgADuo03b3sDc9b4IWomqplycEcPR18UEtfKlgAqknVsBt+A264vDhzhtKOl1xAyixYd29h/tirOCaSmoaRaahZpFRdK3W+/f88XbkFYlTFCkZ5aMS8hB2VFO+48zt7XwIYXLB1JjKqSCpmSnVCtQYBO8bqQUUmw1eRuCLeh8sTicMwPcTHXfcC22G3C4DXrJlMdTmzGdAw6RIAI1+yENbzc4OI4o441ubsRho+oXPAyvsz+GOX5rGQ0KKT1JF8AOffs6pmClKSvjhkLE3eG/bobdR3/vjohYRYHpjcU4fT4fGdul8AVuMBnGOefsw8WwUrnKq/La17EojM0ZHoCQfT/fFQ8Q8G8S8J/w+JcqloVmawkB1AkDzW4x9Oo6EOANCkqLb/niD4m4NyzPaCamrqKGeKQeJWS4+3374HhPanzWpOIMxy6kiipaluWCSV87HBBQfEKoEPLqYkdiNmtY9wcEXxj+Ddd8Ocw+boC1Xw/NJpilI8UBJJCP+gbv74qGrqYKe5Y6SviAv1uf/AFgeFwOREsSs+IkdNHTl0Mjuo1gHpvt+mBfNePedIGgjTRcgC/UeeAKfMXldg58RuVX+kedu2GjVS38XhA2G2PDAs96pk/VcS5hU8xmlaIOCAUHTEJqYsWd3Zw2rUd8e1dSirHHqFkjXbyv4j+uGfzYjcbgjzwxVFagFoW5HXGKULexbp74NKLM2TljVsQSfXFYZdWjX2FsGLZhEYqE8sQ3pwupe5BIJPqdr4Uy0RGK8LqSponq+ZFQU7VDHxStCL38ztcnErmfDOTZvTqaykinnYeGSwDA+wttivYs2kE4CX1J9LX3wT5PmtQtSj1R+kXsTjL49Q9NJ3JvhPwqYAtZla627CZz+pw+zL9n3hmqpXNGk+XykXVxKXX7g9sTeVV8UzozbKwve+C6gzCLMSkSozwRnc/1n19P19up+oZnpj7TnyX9m7PkpppMsqqGq135buWXw+gtsT5nt088VVnHD9bwzU1FHnFMaeuBKaG7Du336D7474nlkp1UUlpXkGy/0juxHkP8A1gT4syfhjiPKZaTiDkS7HQzgGdG/qBHiuTv5YIZANExTYx7ThlIucQFXUxNgB3Plh1yqKm8FQjzyD6ikwVQfIbG/vg/zfg6HKFlCSip5bNFHJflkx36tsSDuV2xCDKTb+BJQhO1oyfzKk40ZA/UTxMRWpqedSM6m1SjtBo3DnXpb8wfyxFTqgrJCkPIeZmdoybgDe9z5Wufvg3paWPLKNIHUu1AgeGQbbMPH03JuL/hgXo4HkmquZAUeWPxqyf6MeoaRb1IufQDzxEuUEtXQiysn+Hctgy2eN45W5xjEES6fqdyzM48gEAG/cnDJ8tApnEDLHzHEvIlkvLIm5Um3Ym7W2tcYditNJMjq55gdWF9vFa1revf74bxU8sq1ldEhKUsgildhcXZWZRfy/XEasxYuT3X7/WbftUe0WWVVHl3PiljvAqvHHf6piwvYdyRrHsoxHcU00VLn+YyjQJWqC8axpqAUtck+pLfbE3lNZT0sEEtU0o5r+M2BJN7bfiR9zjJ6J80zvI45lEazyrDOgA3B/mvba46+2MXIy5bPRB/f6frCH8shsvXTLUkoXSWYEkttZQybj8MP86zKbMJqKlm0RZfFQtBGkaD6dFjq82+mx6jSMOVrknllpVXUsRZYGX6YmW+wPmdye1ziOqcomeGolhsAUJCH+c3uRYd9hv5Y0Nb22oOwKEGcmyT5hq0tMqmlTx37ox0Ej/8AK/2w7GTQlSJpA8pDRSsFJ+k7EeewG+N6N5Y+ZNLpUSqIplk2IvffpfridysVWbZvTxrCJ6daf5Z1WTaMgncDtivJmcEmYADqCP7lmhpJM0pH5tNHVclEt412BJYdhuov03xJ8PUcdVnqU9YiwxUcySSurFlazi6A+p2Hlvg1zOAFKihy8PeprDHTwqttRBjMkt/K0dtzbfEXm1BBR1lTDBB8pTSuAwVW1am3Zwx6+3rgU8lsqfMZxrYgnRQ8yuhMkXJkaKpiqFTY6xq7edmUfbBHwtQSyVSyjUdYBNttiMP85pGNRFItOweOZQzWA1SGQ80jzDDQ33OHXDsAj5REckoVF1NYqRceW/44oTMG3PVRljcPVD0zaZiSrbEarsDvcj12/MYtPhLOY54P3VA38avqzCG86cXMjA+dtQ9CwxVyBKajSaEKJ7gJbfmKVIK2Pfp+uN+Fc2ENQMwjM38BmKBWAeO56LfysL9+uOa2cLkBH5Q7nXIqlizDJipt4pwFA2tyjt+QwRUVck0tr6iCQ3pY9Mc2T/FDMZGy96IrJPG8gZXshdSLHSN97bj/AGwdcD/EKkzNZ1RpUqElYtSuul1Gq2osdmufLHSTyUZqEMES9hKEiDqoexANz0F9yfbD6FNEms9+mA/Ls6kMsSlokD/yfUxHqen5YIIq0QOsLXZSp5XqB/L9v0xVY7hQhj6Gwxq4Gr+JutvET2GIOfiWkp6mii56K9Vuo1f0glv7Yk4qhKvY3RbizPsCfbrj3IGaIyz3g3K+JstqKLNYIqmnnj0vG4uCDjiD4zfsn55w7WVGacFpJmWStduTs0tMd77dWX1G+O95kgiURwyM7rYMBsBhJqeQrcSBbnzxoInitz49T5S+WVbU+ZI8btuQwKn0N/fGR5Oama8ahxDZjHe/MH9xb7239cfVPi/4QcLcf0zw8RZXT1EpuVnVAkqE9w43xyd8Tf2SuIcgllquCZBm1Gjao4GOmdFvf2a3pv6Yw2BYi6qcw5hlNJLUED+GJkVob7grpG1/MdO/TDROHY5IwIYrVCG1m8Sub7WB2Ht3xa78PuJ4qDO6KahmDcuOSpjaFRf/AMSLncXtYX388I1GRyZaoWRKaojY6Y5YLENcgXDX2t5G1r4h9cqtHR+Z6gRKtrMrkpauRKcEEtZYlNio8z/thzWUuZR5PEwictAsbDYjZrhgPOx0H74sA5QaymqRVRFKn6YGiQnx3UAn/wDId7/nh1X0c+WRwUqFaimhY81wPqQDQFB876yT22wtvKUATOMqzLqyqSQLUxyqQbXCkgj0O4wXUmZPBOgk2JuulhpO3n+OJaGgppal4a6jIVTpSVAHAktcM3c9vL8sO4OGqSnqIlkjZjLcLZdXLN76yb23PfyHrj2TyEA5QhYEIIc1pKem5aNKZIjyyAwuz+Snpbt+OCXI8zqqCserqlRZbmKIIA4UhRsBfT0Nrm57AYC0y2nEXOSapDvKUQJCfEbE6t9iDY9+xw9NXJRROsRKIl1c8hjrY3FtJtpboPIi2OefIJNmGOR2Ye1FfFUzSSy1s0RkVbvVFkE1xYHSvgXcEC5/PfDkZhDQUU/ycdOEmTcrHaRmAv8AVfv0ucV4lZUyyt84GSjmjaaN5CbmMGwG24G2xPcemEc5zmasjgmjjEsZ7wKbkWINztvsb9bYMZSD8zbjavyk1iiFbVCmJCpU7kFb73279fQdcRf/AExQKSJpYxIPqAHQ/fEhHWVdJHyqWROWqoC8imQqpuRa246AX6Y3k4RmzcJXmlWtNSuvXTwB1G5FjqNwdunrhwz3u6nlUt/KLg+MlzKaspXosrnp6WrXQjsmlQTcjSD9I7j0wxz/AIdrslq54pFjhnmQPUTmoVjJa2wAJIvbE09fKuQ1lUizq0VXJGp5jNrNwtgD5Db7YYcR0MkOXx5hWmCGNSlMXkBDmTRrew72Jt/xiHC5Z6J+PzP7MzgNwffLIaqMSTpK8YkvzlOmzAGxF9iL7W7DE/8Au1n4aighj5UE8pnnlTud1Uj7H88RmQZXW8YZxHT0Ec1YinxDoFQW3/zzxaL8OU8OUzVDV4OYQvyZKPQOUynbl7eSi9vM4LyXGPiCep4Y72JVVRk1UYQFheKClQa5GNkjW5Iv31E2wvlxlaphqRMY9E0cjvb6yCb2+9tsFlTwlmSsKKspqqEy2cRousvHYm4UdSoP23xlCj5jmVNk9PSGCGGVY6Zzp1Pbrq9ASSSemB9XkNwPSrUF6Vpsxr3ZKSOJKaN+UFjA5Sk/UT3JJN2a5N8N4IeRUygQ+JCHLhSXYEXAB9Cb2xalIuX5RPBSZNVJWR08jJWVkwULUSdCVW3QXsL998CCs8U8s1BFGVjLXMviuBtb79DjF8hMjsB0IRQfeRAy6PNz8tJTGeo8Ca1UhiHJ0+4vf2vgmThGlynN+ZQ5ikEdBCPAgLfPzvcOVPYLcbnbb1xIZZLLSQVGZmKNJ/BBTMtv4Tb3YeekE2HmcMmyzMFlhp85jlpI6iBXhDNpsBfRYdbXte474E5SxIQ66nqUdCQdSJJZatFmenhS8OtBcu4Ooj2vt+GHNHQZlUxsmb1NTOUkVRDpud7eYsPffD2mpUjmjoaiWOllmlI8bEsXsdz/AJ3xM0tbFV5MsDLIJxWu8gQ2ugQDqe5N/wAcYuT0koTyiRE2SfOV8FJTH5pIFMShTZnOrdRbuT0tvt5Y0i4ezGGctV060yK76YpgxNw9iTpO3Qge2JqmrpuFad8xptD1cqnlS2voU7Fh31C1gT74ypq6OBqNCJpZZ7LKUlLhh1AFrE9dybXvgUzMjfSL/e4QQGJVOXaMleSvpkhgjm061Zhp9/I39e+IekrLs9cJCBK+sEbB2Nz06Xtf8sEdFBJPR1YqCtRQSl4mjqSbuL37bXG3TuTfGlXleYvAI6GKKKAHwMyhNLbatK7A38I87C3QYU2YE1cZ6SsCRGNTLV5hT04SFQ8swkRkUtoAtp3/AA74d5BR1+VcQzVc1dJT0usgM9wpF9Qsb7sSBt5A4jKsV9NHetrH5crIWCLfcf026eWJLimWKuiybJcpgano8upRIZ2BlZ53sXkA6bXVQT0tthuF3Dd1FKt3csnKPjNJl1RK+Y86osjKgSDlqTqG4a9unbB8fi7S1KU7RySBgrTqrjQ1wuwBO2/vjm3NaaGhpkFRVaklcrZrlrC3QACxJufLriMNROmk2YqlyoUllO91P/3bdsWfxuUip467nRdb8RYv+rMiqqh1YJBWDlr9MGqO67eY39b4v/hKvqM7ghzKrVoo2UGCnYjUgP8AO/8A5ny/lHrfHz0jz98onXNZEWeKWdFZCN0jDguF7WsAMd5fC3PqXP8AIojlDwi6hwuoXIPTYYu8bIWJZupi7uWlTJBOrc5tKi1/M4kqWDL5hZ5CW6DUbYFnqRTqEaxlLWtfpfuThnnbTjLQYZhE8Uga/Zh/xi4sRsRoUNqEmZRx0stqWQsAPfEWuaxzLyqsWLXCv6+RxlJOjwQr4uxBO9x74bVtLA9PJGfAxOpST3wQY1YnqHRgN8R8gouJsqnoc0gilDKeTI6Asjdt8ct51wPTwcPxVrzkyxc1K/kR2AlQ+HbuCNj3uL747DzPk1dANbqNQKG53Djtjk74gcRQcL8S5nTzLHJk+eRGCsibqkw25g9QbH2xz/MQOoYdzVAB+qV/liyZTmMlVTzsxpFdwRcFlKGxvvtvsfNcQ5pKrMZictmgeBUfUXl0HSuonb1PX1OE1euy+orMonkCSLOqxv1uo36dwbdB54m6jMKXLq6VuZGVEoDpTIqsRe4QXHiv39fTHLChf3+/vN4Kx3oRLKcrrKbK5Mxr6eEUYqOSYFNpXlZSQBb+XbcjYb480SPK6LC6tqKusYAC2Nupsdr/AEnb9ceTZppWRrtHEZmlanZlsj2AUXPQ29euJKlqoP3FTyZdQTvU1U3N54RmZgD9NztYEfnibIxF/JqDQGhIekp+fkwjnDlZ5hqbUUKAE9N9jex8t7d9pOjkEU0kNXUTSUnMeN9D3WDUSLA+Qv5bb43aFMwzCdp0hEQIenpYruEa++oAEAb+vltbEzRZFFSU9HIuURZZDJOxmWdgjEE2Lbk9idtx5WwRyqAdw61owSqxPSzSZb+8JIXlh/iMb6Y4grAKvoQL7dz6Y0ocqkeOqaf+LRhQIJYZizFdNxc/rtc7dcEGYSwQ5j8zCqVqRFhDfYGxuAOlwLm19t8DiZj8r84s8bzQtLoVF8JUk7Hb1t1vhi5OY/OKJ1ub5ZzZamZlRkQwXgWWLSXIvqIFrg2DWHQ4VeVXctROTCbWsCtj3H1DC9DmMlHYTwgSRUspXnG5BYaVHck7nGsELZgrzxypGhYhVvptb0GNOQ9rMHKqUwgyrhT5fhxamsHOqZXqKhYzZi7uT1HYbqB7Yn6r4ZUkWS5VScXpHXGeKSplKklllLam027WFjjKd52gizCSBWSJ2cjfpawAGFOJM3rMnpqSmqJpHqIKIsjC/hVtyCepNz0x8kvk5+RYaN/ruVpxC2RG+TcB01TRR/I8SHJMoijEMc2gxMi672J6nWNX4XwU5dwpwrDSUdOnEgy+oSVpIYoaZJlle9/ET4vc27YA+G4q/Nqalr86Somy9WKllawMSjqR/nfFl5Fw5wjlnzmYzZ7U/Ps6yxCONTHbSbRnvptfYWxS2XI2Uoxuv6/5hKB9qEGsxo3OX8xnpp9VK0S1aFmKgEixHXp4rd9sCyVVHmdNrpIY0zSZuTUmyoV8NuYoHmLXtg7+VmyyCOnyiSDMDLO7J4NPhKE6WVvW2BrKaSpolzCqz6jo6aokj0RaogGFtwBbtjEyn0mLH8oLX7yHybgOCrZ8ry9poswgmEkk8Sq8UY7atRHucNOG/hnmXEccs9dmVPDBzGjREGp5LHrc2Ci+998FlFTf9M0T11VqqayvGtoCwW1rncX74hs0g4hqsyhkrlZKVo1cQUzeEI38h09PfDcXkvR47+5P9oHFANiP8+p6H4d5DFRZclKM2VedBXq4laPS1ix1bG5PYDFXCXN85RqrMJJ66sqTy2ma+y3ve/QE3Fh5A4OqqjoJ80mnqKQVJWMKgncWjIJJ8Ppt164Tr81y/MZpIjrZABGYXumr1su3UYrxZyo4qLJ2TAZS2hGua5NBmNLlWfVcQNTSQR0uZR06aQZUI0yepKgAkdTiQz+dOH6mtjyeApGJOc3hAL6twL9gAd/XDqmElDQ1B5I59VOgRQQLIfpBviL4odaeCngkliaSZwwLi6NHewF8e5M7izqeKMLgvm0lfmNFFSvqVVZqglUIF26W89u2MDylYaOkm5PzQ1zyFbWUCwt+H44XqqrNZZBTx/xam4ZUDBEsvTr0HTfEjR5bmk1HT1VTBBzJpN9EwZlUA9B33xY1KBcEKe/eDVfmlRTPFl1LJaOlbQgBsLE3LW7knfB2jR1mXUbiZVZBdUMtyttmv79sMMp4co5XqazNzXUlDLeOl5oUPI3crYDa/wDfHrvw9SF6GiyqZ6OCUtNMJW3YD+ZupOJ/IplKqN+8duqkfmNFlA0tVZnOjqNIGknmWPi0gdD64msg4aOZZfFJltbUCKna7iRLlkN7Am/QeeIWegqsudYYqeb5GqQvC918R6nf2ODfgGpi+QqcwrLKkcZLQ9OZqGnRbyAN7emFZc/DCrTFVe6kHxjldJz0yiimjjqZCJ2Ldb/0j0tcbYEsxzFIIqbJ6CNqSkTVGum7O5JuXv5E3xYVbkEmV1Gb1ahKvMKaU0lC0ig8xWXUSPKy/ngSy166CB5HCSrIOVBeK7JqJGkdyb32xR4+UuOJ9v6wSDyK9SCoIPmXaiLR7AaAEDBl7i3T1xY3wp4jzDgLM9UKyR5Xp5bMx0G99hp7Dytgcl4Sk4TGX1FSsd6tGmpY2Ymyf1E2FhfoMRuUUdRmKVFRmdTLCslQDPJqJ0xqb7eV72vipctN31ACFTTdzsHKfi9kme5mKNKuIKjAzIjX+nfY+eJ+Hi+j4kq/3UivPqWQq0Q2bbwEH1J+2OPctoaaizPmZZDG+XzORA7XB5gFrOfX88S/DPHlVwzmMVbS1TfvakqSHRt45IyALW9xis+YbqNQ8D9Qnd75O2U5fTRR+JkhVWBNze2K/wCMc4zClg0Q5fUVR1i5iW+nfe/lgX4Z+PK8Z5vlVPmdM+Q18yuqFZeZHI4W42IvvY7YMOLfinluT8N11PKIo80K2jjjN+ZY3LDvsATY46ePKmRCQdRZYg3K0zfP80onzETUk0sZPMWEGzggdTfobjFC/ECop+KP+4mppIqxAeZA+m5262NxjbPvjNmmZ8RZ1mWT1aihgVpY9dw0gvY29cCNf8QafiCGkety/XM5PNmkusiN5hh9Vu6nEvqAqR3U0nkLMc8L1Jg4lhqaipjmERKmEoBzG03PuQo7d8RmXLHmdfJDIzwATu0JnJtG5B0bHtfr5bYQoc5FRVVEaxRQpCBpqUh2Jbpde+HcDvR1QFa1PBA6iTU5Jsv/AIDf1G2OYUJJqYhBbczNIJZqtaWWop2lE7FwEsWPToB/l8Eaxx02XJlU+Yy12yycqCP/APjPY+PX0I6AgeeG1VlzU+YrDlEK1uZwohfXGV5rdRubqTY2I/HBPwZlMFQKkcTmThlw0kmkRgtI17AdbAb/AJYRk6HI/v8ArGrhHLuRJCUZpC6/K0VPUqsyQlVeRLX/ANTr59P6sFtHNS5hRmmXhv5ZGjdY6moqJZWLBbqLkgbkgdO+J9Mky+OnjrVv+6Av8NaikA5jWsNIUnYkX33v6YlKXIJsxp6isi5Jn06VVpdK6AQQLdb9N8crL5AApfaN9JhdQAreGhUU8Pzaq9BLrM0ity5EIG1xazi426e+AfM4K+Bpar5W+WxMCJyPFcbeIdh6jFomerOZ5hleZxvRoQqs0iWZjc7ggeEXO2G7w0FNVvQz1BzZ4hr5ESFF6bXc/V5Ww/DlyX9fUz0wRd1BaLh6mz7KzmdcZaqSKFdIQAc0qSNOnqdRt+Hrh5lfwyz3Nad3ossSSKGQwghiouvUAel7fbDpKmleCppqyOmVA6vy73UXNyDa3fBbPxbmEKwDKo3NO0QYBQoCnfbc4audgeI/6nsaIRvuDuQU1VPkeWZipesiWNedGn/1G+/QdMNs3zuRJKw5sYnqlVZVppfpUntfvfsMZjMcbEoNj/8AR/rNbSmNqfid86yaaCMPl8rqYoafT4bjrYeWGfDeXRnNYKWrV4wq8xWWU6mdbHceXXGYzBvSF1UV3/eK3QMIL1UuY0tZmJUxvVvbSdJTey/liTg+ezQV8TvDmlXEB8pDsoS17WY9T/tjMZiTEeQAPz/WGPqYA/vUZT8JZXDlYzriynkNaCqwrHMwMjG+zdh1wnxZmElFkyVGWTLQwpGIwIRcB7bX8xfGYzFD5GV8fya/WMYAKTBFcozSvgFdPJCalHASRTcTNYEhl9cOHkpkYGqy+qyyVZeZzob6S+5swtYX3xmMx1Vclqia4NqHeTimf5aj4lyDLqo16FmrpNTlR0Sw6AgdcDX7oqqvjGVaChSuy6BwiQyr/DAA6b9jjMZiA5W9Pl7xjfWgv7zfijhlaXLubDSRR17FkanhkusMZJNlJ/vjzgjhBjPFNl9WaymXS0qVAs6eXuL4zGYNcznAT8xNU0nuKuDnqqiPMc+zKmENECsMaMRoDdBftYfrgAzemgeHTkb1FdBTyjm06OCCtt2tYXH54zGY6XjjkpuVAAEmN6iup0y6GVowk1LZZFJIV1Gwtf07YmMgy6aeCqhgpngiiKVABa/MLHcX8wMZjMS+UePjMR7f5ElstowuzrKqWjqZq9YJ5GRkVVEhsZmILMQfww4g4eo63N+QJUpap5A2keLl2BJYKereuMxmJcBLGyZ0sONWAJ+3+YU51wcnEmVUlbn0vPkaFYKZYxoLqgIG3bpitM0ymnyujWnpagQaWK1aOba7HZScZjMMGRzkIvowcwBe4NVL0op4aCCSSIJL8wUiFgOguL+mFpqahyzPMxiy+inzCVYw8U5ewt2A9d8ZjMdGzo/EkChluMJ69oqmKeeqenrBIJUSEm6ODYW9cS2dZpnfEkbpV0Eysw0tPpAYkdx733xmMxev/wAxBCgxrnXC+T5XFllJPG8VdJTlJDGngJI/m62wCVPCkMM0EAkkid59KaASBrPU3xmMwjA7EA32P8zc+NQdSczz5HJ8zqMtSLRBAgHq3QE/554lcoypaeLLXzNZGpJQJIJta2sbkAAjqdtsZjMLZ2UKfv8A4iF25uK09WtJVVJy6okp6oOCzvclr9w98L0WcTVeZQTZvUc1I1IZFjuJOw1eZxmMxh+oG44MSKhZBnFVmdQrSCOFmAQoq6AI1H1Eeew3w/TPaSmq0aprJ68Q2VRGdIsOl/Q4zGYkPj4yLjj/ACXGPEnEnMeesmkkFRUOrTKF8N/5bdyMC1Jn9PT1RhqYFkmLMTpYgE2uLnzxmMw1MCVJ8h3ca0WWVudZhM9yqz7qWSyrvsD5YfVuWvFVzIKtp9LWJjnsAfLGYzE4as5HxPJjVhZn/9k=",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": null,
     "metadata": {
      "image/jpeg": {
       "width": 200
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_fn = Path('samples/puppy.jpg')\n",
    "Image(filename=img_fn, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb0ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"role\": \"user\",\n",
      " \"content\": [\n",
      "  {\n",
      "   \"type\": \"text\",\n",
      "   \"text\": \"hey what in this image?\"\n",
      "  },\n",
      "  {\n",
      "   \"type\": \"image_url\",\n",
      "   \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gxUSU...\n"
     ]
    }
   ],
   "source": [
    "msg = mk_msg(['hey what in this image?',img_fn.read_bytes()])\n",
    "print(json.dumps(msg,indent=1)[:200]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ebb192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This image shows an adorable **Cavalier King Charles Spaniel puppy**! The puppy has the breed's characteristic features:\n",
       "\n",
       "- **Coloring**: Brown (chestnut) and white coat\n",
       "- **Sweet expression**: Large, dark eyes and a gentle face\n",
       "- **Setting**: The puppy is lying on grass near some purple flowers (appear to be asters or similar blooms)\n",
       "\n",
       "The puppy looks very young and has that irresistibly cute, innocent look that Cavalier puppies are famous for. The photo has a professional quality with nice lighting and composition, capturing the puppy's endearing personality perfectly!\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=139, prompt_tokens=104, total_tokens=243, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"This image shows an adorable **Cavalier King Charles Spaniel puppy**! The puppy has the breed's characteristic features:\\n\\n- **Coloring**: Brown (chestnut) and white coat\\n- **Sweet expression**: Large, dark eyes and a gentle face\\n- **Setting**: The puppy is lying on grass near some purple flowers (appear to be asters or similar blooms)\\n\\nThe puppy looks very young and has that irresistibly cute, innocent look that Cavalier puppies are famous for. The photo has a professional quality with nice lighting and composition, capturing the puppy's endearing personality perfectly!\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=139, prompt_tokens=104, total_tokens=243, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f958cae",
   "metadata": {},
   "source": [
    "Let's also demonstrate this for PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160351e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The author of this PDF is **Jeremy Howard** from fast.ai. He explicitly introduces himself in the document with \"Hi, I'm Jeremy Howard, from fast.ai\" and goes on to describe his work co-founding fast.ai with Rachel Thomas eight years ago.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=59, prompt_tokens=1610, total_tokens=1669, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The author of this PDF is **Jeremy Howard** from fast.ai. He explicitly introduces himself in the document with \"Hi, I\\'m Jeremy Howard, from fast.ai\" and goes on to describe his work co-founding fast.ai with Rachel Thomas eight years ago.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=59, prompt_tokens=1610, total_tokens=1669, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_fn = Path('samples/solveit.pdf')\n",
    "msg = mk_msg(['Who is the author of this pdf?', pdf_fn.read_bytes()])\n",
    "c(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc01faf",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23176bd8",
   "metadata": {},
   "source": [
    "Some providers such as Anthropic require manually opting into caching. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ad26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpr(i):\n",
    "    return f'{i} '*1024 + 'This is a caching test. Report back only what number you see repeated above.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "disable_cachy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84103a63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "1\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=5, prompt_tokens=2073, total_tokens=2078, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=2070, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=2070)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='1', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=5, prompt_tokens=2073, total_tokens=2078, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=2070, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=2070))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = mk_msg(cpr(1), cache=True)\n",
    "res = c(msg)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e39380",
   "metadata": {},
   "source": [
    "Anthropic has a maximum of 4 cache checkpoints, so we remove previous ones as we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ab7b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=5, prompt_tokens=4147, total_tokens=4152, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=4144, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=4144)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='2', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=5, prompt_tokens=4147, total_tokens=4152, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=4144, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=4144))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = c([remove_cache_ckpts(msg), mk_msg(res), mk_msg(cpr(2), cache=True)])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaf17a3",
   "metadata": {},
   "source": [
    "We see that the first message was cached, and this extra message has been written to cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7e395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=4144, text_tokens=None, image_tokens=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.usage.prompt_tokens_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286d9dd",
   "metadata": {},
   "source": [
    "We can add a bunch of large messages in a loop to see how the number of cached tokens used grows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| eval: false\n",
    "# h = []\n",
    "# msg = mk_msg(cpr(1), cache=True)\n",
    "# \n",
    "# for o in range(2,25):\n",
    "#     h += [remove_cache_ckpts(msg), mk_msg(res)]\n",
    "#     msg = mk_msg(cpr(o), cache=True)\n",
    "#     res = c(h+[msg])\n",
    "#     detls = res.usage.prompt_tokens_details\n",
    "#     print(o, detls.cached_tokens, detls.cache_creation_tokens, end='; ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197632a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e9f01",
   "metadata": {},
   "source": [
    "### Reconstructing formatted outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80633d31",
   "metadata": {},
   "source": [
    "Lisette can call multiple tools in a loop. Further down this notebook, we'll provide convenience functions for formatting such a sequence of toolcalls and responses into one formatted output string.\n",
    "\n",
    "For now, we'll show an example and show how to transform such a formatted output string back into a valid LiteLLM history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee01a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt_outp = '''\n",
    "I'll solve this step-by-step, using parallel calls where possible.\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\n",
    "  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\n",
    "  \"result\": \"15\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\n",
    "  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\n",
    "  \"result\": \"3\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Now I need to multiply 15 * 3 before I can do the final division:\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\n",
    "  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\n",
    "  \"result\": \"45\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "detls_tag = \"<details class='tool-usage-details'>\"\n",
    "re_tools = re.compile(fr\"^({detls_tag}\\n+```json\\n+(.*?)\\n+```\\n+</details>)\", flags=re.DOTALL|re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec2686",
   "metadata": {},
   "source": [
    "We can split into chunks of (text,toolstr,json):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3f160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  [\"\\nI'll solve this step-by-step, using parallel calls where possible.\\n\\n\", '<details class=\\'tool-usage-details\\'>\\n\\n```json\\n{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}\\n```\\n\\n</details>', '{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}']\n",
      "-  ['\\n\\n', '<details class=\\'tool-usage-details\\'>\\n\\n```json\\n{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}\\n```\\n\\n</details>', '{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}']\n",
      "-  ['\\n\\nNow I need to multiply 15 * 3 before I can do the final division:\\n\\n', '<details class=\\'tool-usage-details\\'>\\n\\n```json\\n{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}\\n```\\n\\n</details>', '{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}']\n",
      "-  ['\\n', None, None]\n"
     ]
    }
   ],
   "source": [
    "sp = re_tools.split(fmt_outp)\n",
    "for o in list(chunked(sp, 3, pad=True)): print('- ', o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303951a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _extract_tool(text:str)->tuple[dict,dict]:\n",
    "    \"Extract tool call and results from <details> block\"\n",
    "    d = json.loads(text.strip())\n",
    "    call = d['call']\n",
    "    func = call['function']\n",
    "    tc = ChatCompletionMessageToolCall(Function(dumps(call['arguments']),func), d['id'])\n",
    "    tr = {'role': 'tool','tool_call_id': d['id'],'name': func, 'content': d['result']}\n",
    "    return tc,tr\n",
    "\n",
    "def fmt2hist(outp:str)->list:\n",
    "    \"Transform a formatted output into a LiteLLM compatible history\"\n",
    "    lm,hist = Message(),[]\n",
    "    spt = re_tools.split(outp)\n",
    "    for txt,_,tooljson in chunked(spt, 3, pad=True):\n",
    "        txt = txt.strip() if tooljson or txt.strip() else '.'\n",
    "        hist.append(lm:=Message(txt))\n",
    "        if tooljson:\n",
    "            tcr = _extract_tool(tooljson)\n",
    "            if not hist: hist.append(lm) # if LLM calls a tool without talking\n",
    "            lm.tool_calls = lm.tool_calls+[tcr[0]] if lm.tool_calls else [tcr[0]] \n",
    "            hist.append(tcr[1])\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad493d98",
   "metadata": {},
   "source": [
    "See how we can turn that one formatted output string back into a list of Messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac22b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b40b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":10,\"b\":5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n",
      " {'content': '15',\n",
      "  'name': 'simple_add',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta'},\n",
      " Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":2,\"b\":1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n",
      " {'content': '3',\n",
      "  'name': 'simple_add',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY'},\n",
      " Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":15,\"b\":3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function')], function_call=None, provider_specific_fields=None),\n",
      " {'content': '45',\n",
      "  'name': 'multiply',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C'},\n",
      " Message(content='.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n"
     ]
    }
   ],
   "source": [
    "h = fmt2hist(fmt_outp)\n",
    "pprint(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249772a",
   "metadata": {},
   "source": [
    "### `mk_msgs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375774e",
   "metadata": {},
   "source": [
    "We will skip tool use blocks and tool results during caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a8e351",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _skip_tools_cache(msgs, cache_idxs):\n",
    "    \"Skip tool use blocks and tool results in and shift cache indices\"\n",
    "    res = []\n",
    "    for idx in cache_idxs:\n",
    "        while msgs[idx].get('tool_calls', []) or msgs[idx]['role'] == 'tool': idx -= 1\n",
    "        res.append(idx)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8708dfa",
   "metadata": {},
   "source": [
    "Now lets make it easy to provide entire conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b326d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_msgs(\n",
    "    msgs,                   # List of messages (each: str, bytes, list, or dict w 'role' and 'content' fields)\n",
    "    cache=False,            # Enable Anthropic caching\n",
    "    cache_idxs=[-1],        # Cache breakpoint idxs\n",
    "    ttl=None,               # Cache TTL: '5m' (default) or '1h'\n",
    "):\n",
    "    \"Create a list of LiteLLM compatible messages.\"\n",
    "    if not msgs: return []\n",
    "    if not isinstance(msgs, list): msgs = [msgs]\n",
    "    res,role = [],'user'\n",
    "    msgs = L(msgs).map(lambda m: fmt2hist(m) if detls_tag in m else [m]).concat()\n",
    "    for m in msgs:\n",
    "        res.append(msg:=remove_cache_ckpts(mk_msg(m, role=role)))\n",
    "        role = 'assistant' if msg['role'] in ('user','function', 'tool') else 'user'\n",
    "    if cache: L(_skip_tools_cache(res, cache_idxs)).map(lambda idx: _add_cache_control(res[idx], ttl))\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b54618d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "msgs = mk_msgs(['Hey, call some functions!',fmt_outp,'How are you?','I am great!'])\n",
    "test_eq(_skip_tools_cache(msgs, [0, 1, 2, -4, -3, -2, -1]), [0, 0, 0, -10, -3, -2, -1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4624e2",
   "metadata": {},
   "source": [
    "With `mk_msgs` you can easily provide a whole conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ecc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hey!'},\n",
       " {'role': 'assistant', 'content': 'Hi there!'},\n",
       " {'role': 'user', 'content': 'How are you?'},\n",
       " {'role': 'assistant', 'content': \"I'm doing fine and you?\"}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf277c",
   "metadata": {},
   "source": [
    "By defualt the last message will be cached when `cache=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a34c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hey!'},\n",
       " {'role': 'assistant', 'content': 'Hi there!'},\n",
       " {'role': 'user', 'content': 'How are you?'},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': \"I'm doing fine and you?\",\n",
       "    'cache_control': {'type': 'ephemeral'}}]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"], cache=True)\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc360a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('cache_control' in msgs[-1]['content'][0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba87c3",
   "metadata": {},
   "source": [
    "Alternatively, users can provide custom `cache_idxs`. Tool call blocks and results are will be skipped during caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289d855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'Hello!',\n",
       "    'cache_control': {'type': 'ephemeral'}}]},\n",
       " {'role': 'assistant', 'content': 'Hi! How can I help you?'},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'Call some functions!',\n",
       "    'cache_control': {'type': 'ephemeral'}}]},\n",
       " Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":10,\"b\":5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta',\n",
       "  'name': 'simple_add',\n",
       "  'content': '15'},\n",
       " Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":2,\"b\":1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY',\n",
       "  'name': 'simple_add',\n",
       "  'content': '3'},\n",
       " Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":15,\"b\":3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function')], function_call=None, provider_specific_fields=None),\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C',\n",
       "  'name': 'multiply',\n",
       "  'content': '45'},\n",
       " Message(content=[{'type': 'text', 'text': '.', 'cache_control': {'type': 'ephemeral'}}], role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(['Hello!','Hi! How can I help you?','Call some functions!',fmt_outp], cache=True, cache_idxs=[0,-2,-1])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('cache_control' in msgs[0]['content'][0], True)\n",
    "test_eq('cache_control' in msgs[2]['content'][0], True) # shifted idxs to skip tools\n",
    "test_eq('cache_control' in msgs[-1]['content'][0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910677d4",
   "metadata": {},
   "source": [
    "Who's speaking at when is automatically inferred.\n",
    "Even when there are multiple tools being called in parallel (which LiteLLM supports!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8d1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Tell me the weather in Paris and Rome'},\n",
       " {'role': 'assistant', 'content': 'Assistant calls weather tool two times'},\n",
       " {'role': 'tool', 'content': 'Weather in Paris is ...'},\n",
       " {'role': 'tool', 'content': 'Weather in Rome is ...'},\n",
       " {'role': 'assistant', 'content': 'Assistant returns weather'},\n",
       " {'role': 'user', 'content': 'Thanks!'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(['Tell me the weather in Paris and Rome',\n",
    "                'Assistant calls weather tool two times',\n",
    "                {'role':'tool','content':'Weather in Paris is ...'},\n",
    "                {'role':'tool','content':'Weather in Rome is ...'},\n",
    "                'Assistant returns weather',\n",
    "                'Thanks!'])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq([m['role'] for m in msgs],['user','assistant','tool','tool','assistant','user'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475862e9",
   "metadata": {},
   "source": [
    "For ease of use, if `msgs` is not already in a `list`, it will automatically be wrapped inside one. This way you can pass a single prompt into `mk_msgs` and get back a LiteLLM compatible msg history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e7663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hey'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(\"Hey\")\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8c2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'tool', 'content': 'fake tool result'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "msgs = mk_msgs({'role':'tool','content':'fake tool result'})\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c828056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hey!'},\n",
       " {'role': 'assistant', 'content': 'Hi there!'},\n",
       " {'role': 'user', 'content': 'How are you?'},\n",
       " {'role': 'assistant', 'content': \"I'm fine, you?\"}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm fine, you?\"])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284fbfc3",
   "metadata": {},
   "source": [
    "However, beware that if you use `mk_msgs` for a single message, consisting of multiple parts.\n",
    "Then you should be explicit, and make sure to wrap those multiple messages in two lists:\n",
    "\n",
    "1. One list to show that they belong together in one message (the inner list).\n",
    "2. Another, because mk_msgs expects a list of multiple messages (the outer list).\n",
    "\n",
    "This is common when working with images for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d315d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " {\n",
      "  \"role\": \"user\",\n",
      "  \"content\": [\n",
      "   {\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"Whats in this img?\"\n",
      "   },\n",
      "   {\n",
      "    \"type\": \"image_url\",\n",
      "    \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...\n"
     ]
    }
   ],
   "source": [
    "msgs = mk_msgs([['Whats in this img?',img_fn.read_bytes()]])\n",
    "print(json.dumps(msgs,indent=1)[:200]+\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad470e4",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3dc5d",
   "metadata": {},
   "source": [
    "LiteLLM supports streaming responses. That's really useful if you want to show intermediate results, instead of having to wait until the whole response is finished.\n",
    "\n",
    "We create this helper function that returns the entire response at the end of the stream. This is useful when you want to store the whole response somewhere after having displayed the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stream_with_complete(gen, postproc=noop):\n",
    "    \"Extend streaming response chunks with the complete response\"\n",
    "    chunks = []\n",
    "    for chunk in gen:\n",
    "        chunks.append(chunk)\n",
    "        yield chunk\n",
    "    postproc(chunks)\n",
    "    return stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f16571",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = c(mk_msgs(\"Hey!\"), stream=True)\n",
    "r2 = SaveReturn(stream_with_complete(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey! How's it going? ðŸ˜Š What can I help you with today?"
     ]
    }
   ],
   "source": [
    "for o in r2:\n",
    "    cts = o.choices[0].delta.content\n",
    "    if cts: print(cts, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ec073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hey! How's it going? ðŸ˜Š What can I help you with today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=22, prompt_tokens=9, total_tokens=31, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hey! How's it going? ðŸ˜Š What can I help you with today?\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=22, prompt_tokens=9, total_tokens=31, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e11c0",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4301402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lite_mk_func(f):\n",
    "    if isinstance(f, dict): return f\n",
    "    return {'type':'function', 'function':get_schema(f, pname='parameters')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b103600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_add(\n",
    "    a: int,   # first operand\n",
    "    b: int=0  # second operand\n",
    ") -> int:\n",
    "    \"Add two numbers together\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fc27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'simple_add',\n",
       "  'description': 'Add two numbers together\\n\\nReturns:\\n- type: integer',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'a': {'type': 'integer', 'description': 'first operand'},\n",
       "    'b': {'type': 'integer', 'description': 'second operand', 'default': 0}},\n",
       "   'required': ['a']}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolsc = lite_mk_func(simple_add)\n",
    "toolsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2af29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmsg = mk_msg(\"What is 5478954793+547982745? How about 5479749754+9875438979? Always use tools for calculations, and describe what you'll do before using a tool. Where multiple tool calls are required, do them in a single response where possible. \")\n",
    "r = c(tmsg, tools=[toolsc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb1817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'll help you calculate both of those sums using the addition tool.\n",
       "\n",
       "Let me break down what I'll do:\n",
       "1. First calculation: 5478954793 + 547982745\n",
       "2. Second calculation: 5479749754 + 9875438979\n",
       "\n",
       "Since these are independent calculations, I'll perform both at the same time.\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 5478954793, \"b\": 547982745})\n",
       "\n",
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 5479749754, \"b\": 9875438979})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=211, prompt_tokens=659, total_tokens=870, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll help you calculate both of those sums using the addition tool.\\n\\nLet me break down what I'll do:\\n1. First calculation: 5478954793 + 547982745\\n2. Second calculation: 5479749754 + 9875438979\\n\\nSince these are independent calculations, I'll perform both at the same time.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 5478954793, \"b\": 547982745}', name='simple_add'), id='toolu_01KATe5b5tmd4tK5D9BUZE5S', type='function'), ChatCompletionMessageToolCall(index=2, function=Function(arguments='{\"a\": 5479749754, \"b\": 9875438979}', name='simple_add'), id='toolu_01E4WQj8RkQj8Z7QLJ6ireTe', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=211, prompt_tokens=659, total_tokens=870, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d81d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _lite_call_func(tc,ns,raise_on_err=True):\n",
    "    try: fargs = json.loads(tc.function.arguments)\n",
    "    except Exception as e: raise ValueError(f\"Failed to parse function arguments: {tc.function.arguments}\") from e\n",
    "    res = call_func(tc.function.name, fargs,ns=ns)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": str(res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af607c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tool_call_id': 'toolu_01KATe5b5tmd4tK5D9BUZE5S',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '6026937538'},\n",
       " {'tool_call_id': 'toolu_01E4WQj8RkQj8Z7QLJ6ireTe',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '15355188733'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcs = [_lite_call_func(o, ns=globals()) for o in r.choices[0].message.tool_calls]\n",
    "tcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd9871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_text(msg):\n",
    "    \"Extract printable content from streaming delta, return None if nothing to print\"\n",
    "    c = msg.choices[0]\n",
    "    if not c: return c\n",
    "    if not hasattr(c,'delta'): return None #f'{c}'\n",
    "    delta = c.delta\n",
    "    if delta.content: return delta.content\n",
    "    if delta.tool_calls:\n",
    "        res = ''.join(f\"ðŸ”§ {tc.function.name}\" for tc in delta.tool_calls if tc.id and tc.function.name)\n",
    "        if res: return f'\\n{res}\\n'\n",
    "    if hasattr(delta,'reasoning_content'): return 'ðŸ§ ' if delta.reasoning_content else '\\n\\n'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4790a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help you calculate those two sums using the addition tool.\n",
      "\n",
      "Let me break down what I need to do:\n",
      "1. Calculate 5478954793 + 547982745\n",
      "2. Calculate 5479749754 + 9875438979\n",
      "\n",
      "Since these are independent calculations, I'll perform both additions at once.\n",
      "ðŸ”§ simple_add\n",
      "\n",
      "ðŸ”§ simple_add\n"
     ]
    }
   ],
   "source": [
    "r = c(tmsg, stream=True, tools=[toolsc])\n",
    "r2 = SaveReturn(stream_with_complete(r))\n",
    "for o in r2: print(delta_text(o) or '', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21118c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'll help you calculate those two sums using the addition tool.\n",
       "\n",
       "Let me break down what I need to do:\n",
       "1. Calculate 5478954793 + 547982745\n",
       "2. Calculate 5479749754 + 9875438979\n",
       "\n",
       "Since these are independent calculations, I'll perform both additions at once.\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 5478954793, \"b\": 547982745})\n",
       "\n",
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 5479749754, \"b\": 9875438979})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=206, prompt_tokens=659, total_tokens=865, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll help you calculate those two sums using the addition tool.\\n\\nLet me break down what I need to do:\\n1. Calculate 5478954793 + 547982745\\n2. Calculate 5479749754 + 9875438979\\n\\nSince these are independent calculations, I'll perform both additions at once.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5478954793, \"b\": 547982745}', name='simple_add'), id='toolu_0154DKJLM7bWtdgYMpLnQtF1', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5479749754, \"b\": 9875438979}', name='simple_add'), id='toolu_01PKwe1vvvuYpjr32NgwHSio', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=206, prompt_tokens=659, total_tokens=865, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f355ecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
      "\n",
      "# Derivative Solution\n",
      "\n",
      "To find the derivative of **f(x) = xÂ³ + 2xÂ² - 5x + 1**, I'll apply the power rule to each term.\n",
      "\n",
      "## Using the Power Rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹\n",
      "\n",
      "**Term by term:**\n",
      "- d/dx(xÂ³) = 3xÂ²\n",
      "- d/dx(2xÂ²) = 4x\n",
      "- d/dx(-5x) = -5\n",
      "- d/dx(1) = 0\n",
      "\n",
      "## Answer:\n",
      "**f'(x) = 3xÂ² + 4x - 5**"
     ]
    }
   ],
   "source": [
    "msg = mk_msg(\"Solve this complex math problem: What is the derivative of x^3 + 2x^2 - 5x + 1?\")\n",
    "r = c(msg, stream=True, reasoning_effort=\"low\")\n",
    "r2 = SaveReturn(stream_with_complete(r))\n",
    "for o in r2: print(delta_text(o) or '', end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff5f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Derivative Solution\n",
       "\n",
       "To find the derivative of **f(x) = xÂ³ + 2xÂ² - 5x + 1**, I'll apply the power rule to each term.\n",
       "\n",
       "## Using the Power Rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹\n",
       "\n",
       "**Term by term:**\n",
       "- d/dx(xÂ³) = 3xÂ²\n",
       "- d/dx(2xÂ²) = 4x\n",
       "- d/dx(-5x) = -5\n",
       "- d/dx(1) = 0\n",
       "\n",
       "## Answer:\n",
       "**f'(x) = 3xÂ² + 4x - 5**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=328, prompt_tokens=66, total_tokens=394, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=148, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"# Derivative Solution\\n\\nTo find the derivative of **f(x) = xÂ³ + 2xÂ² - 5x + 1**, I'll apply the power rule to each term.\\n\\n## Using the Power Rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹\\n\\n**Term by term:**\\n- d/dx(xÂ³) = 3xÂ²\\n- d/dx(2xÂ²) = 4x\\n- d/dx(-5x) = -5\\n- d/dx(1) = 0\\n\\n## Answer:\\n**f'(x) = 3xÂ² + 4x - 5**\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, thinking_blocks=[{'type': 'thinking', 'thinking': \"This is a straightforward calculus problem asking for the derivative of a polynomial function.\\n\\nThe function is: f(x) = xÂ³ + 2xÂ² - 5x + 1\\n\\nTo find the derivative, I'll use the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹\\n\\nTaking the derivative term by term:\\n- d/dx(xÂ³) = 3xÂ²\\n- d/dx(2xÂ²) = 2Â·2x = 4x\\n- d/dx(-5x) = -5\\n- d/dx(1) = 0\\n\\nTherefore: f'(x) = 3xÂ² + 4x - 5\", 'signature': 'EpQECkYICBgCKkC8OXjjPuKScw7XEosvPn2d3T4+1PzMpkm8gQqxDDPkuAmt2k/jbI43NVuCDBSapnp/44GCkuyAc8NlfO/zk7l+Egwmsj1XvkX02uOrSUEaDKZGs1IsHyZ2+JpigCIw8QOV/cJVJrgTJj2NZjDHkocDllkhOZIcispY2AHndoAlAzJJ/Oh0dT6BeQgx38F/KvsCsitqDnL3Zmhnc7siagi1Z1KlYz0/YbiR6KYXF4/CH/Kje9RYVjx7rnBeN7DZCmU8n1nkCGnfyu5ZpY5+opQ0MGgrPKRQWqy3ABZcY5N7udWxg8VQF53binGxgpNNoT4XIWthJywxgnmvIg7SW0lrP8enOrS/tY40h1lA88C7WIhu3XdxLPqUa57KzBzC/x8LOUFGlRK3+QUKg8GECHBWTjbwUrXavMcvs66/CGfdaVSlDOtxrvk+xlepK041218TMa+oLf8frq7YWgEX1vtsxzGM4EHo5tJnmNZrpKyB+ODksIQAbwGd2vjH9+s4zpRIgJmJPgc5Gfy4cQFB+3PLAvZI6OE5jq9PHo5if3oWItoZznpvEHBIfPlC4Tnmpwgmq56QPDuwssJFrQyxp6xnsBt9Vn/+SIFX7o01vt4wLlNqzTRGW2ebT+JqQmpPEGvgr8pwc4tRzi20MqEUJPLDtElR5jR3/Aamgtd5W/BENPdnkAhDaSoh7sxX8BgB'}], reasoning_content=\"This is a straightforward calculus problem asking for the derivative of a polynomial function.\\n\\nThe function is: f(x) = xÂ³ + 2xÂ² - 5x + 1\\n\\nTo find the derivative, I'll use the power rule: d/dx(xâ¿) = nÂ·xâ¿â»Â¹\\n\\nTaking the derivative term by term:\\n- d/dx(xÂ³) = 3xÂ²\\n- d/dx(2xÂ²) = 2Â·2x = 4x\\n- d/dx(-5x) = -5\\n- d/dx(1) = 0\\n\\nTherefore: f'(x) = 3xÂ² + 4x - 5\"))], usage=Usage(completion_tokens=328, prompt_tokens=66, total_tokens=394, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=148, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92cdb20",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7128c",
   "metadata": {},
   "source": [
    "LiteLLM provides search, not via tools, but via the special `web_search_options` param."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76423a3",
   "metadata": {},
   "source": [
    "**Note:** Not all models support web search. LiteLLM's `supports_web_search` field should indicate this, but it's unreliable for some models like `claude-sonnet-4-20250514`. Checking both `supports_web_search` and `search_context_cost_per_query` provides more accurate detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _has_search(m):\n",
    "    i = get_model_info(m)\n",
    "    return bool(i.get('search_context_cost_per_query') or i.get('supports_web_search'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43417017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini/gemini-2.5-flash True\n",
      "claude-sonnet-4-5 True\n",
      "openai/gpt-4.1 False\n"
     ]
    }
   ],
   "source": [
    "for m in ms: print(m, _has_search(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8611e2",
   "metadata": {},
   "source": [
    "When search is supported it can be used like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0bead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Otters are carnivorous mammals in the subfamily Lutrinae and members of the weasel family. The 14 extant otter species are all semiaquatic, both freshwater and marine. They're found on every continent except Australia and Antarctica.\n",
       "\n",
       "Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water. In fact, otters have the densest fur of any animalâ€”as many as a million hairs per square inch in places.\n",
       "\n",
       "All otters are expert hunters that eat fish, crustaceans, and other critters. They're known for being playful animals and sea otters famously use rocks as tools to crack open shellfish. When it's time to nap, sea otters entangle themselves in kelp so they don't float away.\n",
       "\n",
       "Many otter species were historically hunted nearly to extinction for their fur but have since recovered in some areas, though several species remain threatened by pollution and habitat loss.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=382, prompt_tokens=18089, total_tokens=18471, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Otters are carnivorous mammals in the subfamily Lutrinae and members of the weasel family. The 14 extant otter species are all semiaquatic, both freshwater and marine. They're found on every continent except Australia and Antarctica.\\n\\nOtters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water. In fact, otters have the densest fur of any animalâ€”as many as a million hairs per square inch in places.\\n\\nAll otters are expert hunters that eat fish, crustaceans, and other critters. They're known for being playful animals and sea otters famously use rocks as tools to crack open shellfish. When it's time to nap, sea otters entangle themselves in kelp so they don't float away.\\n\\nMany otter species were historically hunted nearly to extinction for their fur but have since recovered in some areas, though several species remain threatened by pollution and habitat loss.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': [[{'type': 'web_search_result_location', 'cited_text': 'Otters are carnivorous mammals in the subfamily Lutrinae. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioICBgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDMlacTT8THSDML7nuhoMyB3Xp2StEfWJOx72IjATEIYmZbwZDH+a0KRLuOHQx4nipGzmvy//B4ItZEaDN4t55aF0a+SnmlUY390IN18qE+y/CtqixJ/kgvGL2GCYkFhQRxMYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'The 14 extant otter species are all semiaquatic, both freshwater and marine. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioICBgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDPMklISdrn7SPQleQRoMEXiAAHkqYiNW6OmtIjAUj585hJpTzaemfnQBq2I0iPlxldp5kF/v0PJF3KJEmsIpOifKxCRiZ6XVIVSJVvAqE0/Ky4Flk1kURB4OI5fE+veaT+kYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in wate...', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioICBgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDC2N75wJ2uuikSjQ+RoMxoG92lQN3j7/D3fbIjDyvsSFyTtKCRBLNLAf6cUxIMGtA8tbvjaHc/2tsL3XjM08GdUZ+iMFEO+3ZCHfaNYqExAv5qFrACrD2ahXqmuKizgLC/AYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'Otters have the densest fur of any animalâ€”as many as a million hairs per square inch in places. ', 'url': 'https://www.nationalgeographic.com/animals/mammals/facts/otters-1', 'title': 'Otters, facts and information | National Geographic', 'encrypted_index': 'Eo8BCioICBgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDDTxBQIf+32DfjrHOxoMULZCEGXBmEY/R9FvIjBTh9+auWoiUGcucTuMRHoHXwTYNOYI8AjmTSSPMB1jjAVLo5YvNnoauO0UMsBi/c0qE95gX2JsrU9Y/t/vhQJV2iEiOo8YBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'All otters are expert hunters that eat fish, crustaceans, and other critters. ', 'url': 'https://www.nationalgeographic.com/animals/mammals/facts/otters-1', 'title': 'Otters, facts and information | National Geographic', 'encrypted_index': 'EpABCioICBgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDIuhSw92LHD8nUDQdBoMgmaSnh/ue9kqbM01IjCm/W+w2Lich4J00PDlqdH3x2eoIbQRGDrNWweVv0twtOeWsM1O/QomKi2pJPwbQJAqFCP63tti/ySNFcfQz6ggQOzPX/IoGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'When itâ€™s time to nap, sea otters entangle themselves in kelp so they donâ€™t float away. ', 'url': 'https://www.nationalgeographic.com/animals/mammals/facts/otters-1', 'title': 'Otters, facts and information | National Geographic', 'encrypted_index': 'EpABCioICBgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDDT4gacjn5D/bQNpcBoMildal+nQtCvqwhGTIjAZzep3bcsy5VNzXzwKYsOwaWCCOcYsGEQw4IpZs2ltKT0jG4Eggs7MS5sEZ4VfB2IqFFFWSM3zexw8+KMk7oWUpCA140b5GAQ='}]], 'thinking_blocks': None}))], usage=Usage(completion_tokens=382, prompt_tokens=18089, total_tokens=18471, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smsg = mk_msg(\"Search the web and tell me very briefly about otters\")\n",
    "r = c(smsg, web_search_options={\"search_context_size\": \"low\"})  # or 'medium' / 'high'\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af925d",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaeeee1",
   "metadata": {},
   "source": [
    "Next, lets handle Anthropic's search citations.\n",
    "\n",
    "When not using streaming, all citations are placed in a separate key in the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84f8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'web_search_result_location',\n",
       "  'cited_text': 'Otters are carnivorous mammals in the subfamily Lutrinae. ',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Otter',\n",
       "  'title': 'Otter - Wikipedia',\n",
       "  'encrypted_index': 'Eo8BCioICBgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDMlacTT8THSDML7nuhoMyB3Xp2StEfWJOx72IjATEIYmZbwZDH+a0KRLuOHQx4nipGzmvy//B4ItZEaDN4t55aF0a+SnmlUY390IN18qE+y/CtqixJ/kgvGL2GCYkFhQRxMYBA=='}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.choices[0].message.provider_specific_fields['citations'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a8559",
   "metadata": {},
   "source": [
    "However, when streaming the results are not captured this way.\n",
    "Instead, we provide this helper function that adds the citation to the `content` field in markdown format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc341e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cite_footnote(msg):\n",
    "    if not (delta:=nested_idx(msg, 'choices', 0, 'delta')): return\n",
    "    if citation:= nested_idx(delta, 'provider_specific_fields', 'citation'):\n",
    "        title = citation['title'].replace('\"', '\\\\\"')\n",
    "        delta.content = f'[*]({citation[\"url\"]} \"{title}\") '\n",
    "        \n",
    "def cite_footnotes(stream_list):\n",
    "    \"Add markdown footnote citations to stream deltas\"\n",
    "    for msg in stream_list: cite_footnote(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2150365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Otters are [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") charismatic members of the weasel family, found on every continent except Australia and Antarctica. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") [*](https://en.wikipedia.org/wiki/Otter \"Otter - Wikipedia\") There are 13-14 species in total, ranging from the small-clawed otter to the giant otter.\n",
       "\n",
       "These aquatic mammals are known for [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") their short ears and noses, elongated bodies, long tails, and soft, dense fur. In fact, [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") otters have the densest fur of any animalâ€”as many as a million hairs per square inch, which keeps them warm in water since they lack blubber.\n",
       "\n",
       "[*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") All otters are expert hunters that eat fish, crustaceans, and other critters. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") Sea otters will float on their backs, place a rock on their chests, then smash mollusks down on it until they break open. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") River otters are especially playful, gamboling on land and splashing into rivers and streams. They're highly adapted for water with webbed feet, and [*](https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week \"12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior\") can stay submerged for more than 5 minutes, with river otters able to hold their breath for up to 8 minutes.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=431, prompt_tokens=15055, total_tokens=15486, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Otters are [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") charismatic members of the weasel family, found on every continent except Australia and Antarctica. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") [*](https://en.wikipedia.org/wiki/Otter \"Otter - Wikipedia\") There are 13-14 species in total, ranging from the small-clawed otter to the giant otter.\\n\\nThese aquatic mammals are known for [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") their short ears and noses, elongated bodies, long tails, and soft, dense fur. In fact, [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") otters have the densest fur of any animalâ€”as many as a million hairs per square inch, which keeps them warm in water since they lack blubber.\\n\\n[*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") All otters are expert hunters that eat fish, crustaceans, and other critters. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") Sea otters will float on their backs, place a rock on their chests, then smash mollusks down on it until they break open. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") River otters are especially playful, gamboling on land and splashing into rivers and streams. They\\'re highly adapted for water with webbed feet, and [*](https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week \"12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior\") can stay submerged for more than 5 minutes, with river otters able to hold their breath for up to 8 minutes.', role='assistant', tool_calls=[], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=431, prompt_tokens=15055, total_tokens=15486, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = list(c(smsg, stream=True, web_search_options={\"search_context_size\": \"low\"}))\n",
    "cite_footnotes(r)\n",
    "stream_chunk_builder(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29018310",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9223dc",
   "metadata": {},
   "source": [
    "LiteLLM is pretty bare bones. It doesnt keep track of conversation history or what tools have been added in the conversation so far.\n",
    "\n",
    "So lets make a Claudette style wrapper so we can do streaming, toolcalling, and toolloops without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "effort = AttrDict({o[0]:o for o in ('low','medium','high')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _mk_prefill(pf): return ModelResponseStream([StreamingChoices(delta=Delta(content=pf,role='assistant'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fba85",
   "metadata": {},
   "source": [
    "When the tool uses are about to be exhausted it is important to alert the AI so that it knows to use its final steps for communicating the user current progress and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_final_prompt = \"You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Chat:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model:str,                # LiteLLM compatible model name \n",
    "        sp='',                    # System prompt\n",
    "        temp=0,                   # Temperature\n",
    "        search=False,             # Search (l,m,h), if model supports it\n",
    "        tools:list=None,          # Add tools\n",
    "        hist:list=None,           # Chat history\n",
    "        ns:Optional[dict]=None,   # Custom namespace for tool calling \n",
    "        cache=False,              # Anthropic prompt caching\n",
    "        cache_idxs:list=[-1],     # Anthropic cache breakpoint idxs excl. sys prompt incl. final prompt\n",
    "        ttl=None,                 # Anthropic prompt caching ttl\n",
    "    ):\n",
    "        \"LiteLLM chat client.\"\n",
    "        self.model = model\n",
    "        hist,tools = mk_msgs(hist,cache,cache_idxs,ttl),listify(tools)\n",
    "        if ns is None and tools: ns = mk_ns(tools)\n",
    "        elif ns is None: ns = globals()\n",
    "        self.tool_schemas = [lite_mk_func(t) for t in tools] if tools else None\n",
    "        store_attr()\n",
    "    \n",
    "    def _prep_msg(self, msg=None, prefill=None):\n",
    "        \"Prepare the messages list for the API call\"\n",
    "        sp = [{\"role\": \"system\", \"content\": self.sp}] if self.sp else []\n",
    "        if msg: self.hist = mk_msgs(self.hist+[msg], self.cache, self.cache_idxs, self.ttl)\n",
    "        pf = [{\"role\":\"assistant\",\"content\":prefill}] if prefill else []\n",
    "        return sp + self.hist + pf\n",
    "\n",
    "    def _call(self, msg=None, prefill=None, temp=None, think=None, search=None, stream=False, max_steps=2, step=1, final_prompt=None, tool_choice=None, **kwargs):\n",
    "        \"Internal method that always yields responses\"\n",
    "        if step>max_steps: return\n",
    "        if not get_model_info(self.model).get(\"supports_assistant_prefill\"): prefill=None\n",
    "        if _has_search(self.model) and (s:=ifnone(search,self.search)): kwargs['web_search_options'] = {\"search_context_size\": effort[s]}\n",
    "        else: _=kwargs.pop('web_search_options',None)\n",
    "        res = completion(model=self.model, messages=self._prep_msg(msg, prefill), stream=stream, \n",
    "                         tools=self.tool_schemas, reasoning_effort = effort.get(think), tool_choice=tool_choice,\n",
    "                         # temperature is not supported when reasoning\n",
    "                         temperature=None if think else ifnone(temp,self.temp),\n",
    "                         **kwargs)\n",
    "        if stream:\n",
    "            if prefill: yield _mk_prefill(prefill)\n",
    "            res = yield from stream_with_complete(res,postproc=cite_footnotes)\n",
    "        m = res.choices[0].message\n",
    "        if prefill: m.content = prefill + m.content\n",
    "        self.hist.append(m)\n",
    "        yield res\n",
    "\n",
    "        if tcs := m.tool_calls:\n",
    "            tool_results=[_lite_call_func(tc, ns=self.ns) for tc in tcs]\n",
    "            self.hist+=tool_results\n",
    "            for r in tool_results: yield r\n",
    "            if step>=max_steps-1: prompt,tool_choice,search = final_prompt,'none',False\n",
    "            else: prompt = None\n",
    "            yield from self._call(\n",
    "                prompt, prefill, temp, think, search, stream, max_steps, step+1,\n",
    "                final_prompt, tool_choice, **kwargs)\n",
    "    \n",
    "    def __call__(self,\n",
    "                 msg=None,          # Message str, or list of multiple message parts\n",
    "                 prefill=None,      # Prefill AI response if model supports it\n",
    "                 temp=None,         # Override temp set on chat initialization\n",
    "                 think=None,        # Thinking (l,m,h)\n",
    "                 search=None,       # Override search set on chat initialization (l,m,h)\n",
    "                 stream=False,      # Stream results\n",
    "                 max_steps=2, # Maximum number of tool calls\n",
    "                 final_prompt=_final_prompt, # Final prompt when tool calls have ran out \n",
    "                 return_all=False,  # Returns all intermediate ModelResponses if not streaming and has tool calls\n",
    "                 **kwargs):\n",
    "        \"Main call method - handles streaming vs non-streaming\"\n",
    "        result_gen = self._call(msg, prefill, temp, think, search, stream, max_steps, 1, final_prompt, **kwargs)     \n",
    "        if stream: return result_gen              # streaming\n",
    "        elif return_all: return list(result_gen)  # toolloop behavior\n",
    "        else: return last(result_gen)             # normal chat behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69419bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(as_prop=True)\n",
    "def cost(self: Chat):\n",
    "    \"Total cost of all responses in conversation history\"\n",
    "    return sum(getattr(r, '_hidden_params', {}).get('response_cost')  or 0\n",
    "               for r in self.h if hasattr(r, 'choices'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce163563",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf319a",
   "metadata": {},
   "source": [
    "### History tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ccd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hey Rens! Nice to meet you. How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=20, prompt_tokens=14, total_tokens=34, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hey Rens! Nice to meet you. How can I help you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=20, prompt_tokens=14, total_tokens=34, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model)\n",
    "res = chat(\"Hey my name is Rens\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978da56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Rens! You told me that when you introduced yourself at the start of our conversation.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=25, prompt_tokens=42, total_tokens=67, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Your name is Rens! You told me that when you introduced yourself at the start of our conversation.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=25, prompt_tokens=42, total_tokens=67, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"Whats my name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb77c7",
   "metadata": {},
   "source": [
    "See now we keep track of history!\n",
    "\n",
    "History is stored in the `hist` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c29d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hey my name is Rens'},\n",
       " Message(content='Hey Rens! Nice to meet you. How can I help you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}),\n",
       " {'role': 'user', 'content': 'Whats my name'},\n",
       " Message(content='Your name is Rens! You told me that when you introduced yourself at the start of our conversation.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38015b",
   "metadata": {},
   "source": [
    "You can also pass an old chat history into new Chat objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f575f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Rens! You've asked me a couple times now - just checking if I'm paying attention? ðŸ˜Š\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=30, prompt_tokens=76, total_tokens=106, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Your name is Rens! You've asked me a couple times now - just checking if I'm paying attention? ðŸ˜Š\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=30, prompt_tokens=76, total_tokens=106, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat2 = Chat(model, hist=chat.hist)\n",
    "chat2(\"What was my name again?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26748132",
   "metadata": {},
   "source": [
    "### Synthetic History Creation\n",
    "\n",
    "Lets build chat history step by step. That way we can tweak anything we need to during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = \"What is 5 + 7? Use the tool to calculate it.\"\n",
    "c = Chat(model, tools=[simple_add])\n",
    "res = c(pr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ef0aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def print_hist(self:Chat):\n",
    "    \"Print each message on a different line\"\n",
    "    for r in self.hist: print(r, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde51fc9",
   "metadata": {},
   "source": [
    "Whereas normally without tools we would get one user input and one assistant response. Here we get two extra messages in between.\n",
    "- An assistant message requesting the tools with arguments.\n",
    "- A tool response with the result to the tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49792a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n",
      "\n",
      "Message(content=None, role='assistant', tool_calls=[{'index': 0, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_012bi9eSyzhwaG3TgGpytJbc', 'type': 'function'}], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})\n",
      "\n",
      "{'tool_call_id': 'toolu_012bi9eSyzhwaG3TgGpytJbc', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n",
      "\n",
      "{'role': 'assistant', 'content': 'You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.'}\n",
      "\n",
      "Message(content='\\n\\nThe result of 5 + 7 is **12**.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2eb0a2",
   "metadata": {},
   "source": [
    "Lets try to build this up manually so we have full control over the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def random_tool_id():\n",
    "    \"Generate a random tool ID with 'toolu_' prefix\"\n",
    "    random_part = ''.join(random.choices(string.ascii_letters + string.digits, k=25))\n",
    "    return f'toolu_{random_part}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0bd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toolu_0UAqFzWsDK4FrUMp48Y3tT3QD'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tool_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e52b7",
   "metadata": {},
   "source": [
    "A tool call request can contain one more or more tool calls. Lets make one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc(func, args, tcid=None, idx=1):\n",
    "    if not tcid: tcid = random_tool_id()\n",
    "    return {'index': idx, 'function': {'arguments': args, 'name': func}, 'id': tcid, 'type': 'function'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b9182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 1,\n",
       " 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'},\n",
       " 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n",
       " 'type': 'function'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = mk_tc(simple_add.__name__, json.dumps(dict(a=5, b=7)))\n",
    "tc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da6222",
   "metadata": {},
   "source": [
    "This can then be packged into the full Message object produced by the assitant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436abceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_tc_req(content, tcs): return Message(content=content, role='assistant', tool_calls=tcs, function_call=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c031e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 5, \"b\": 7}', name='simple_add'), id='toolu_gAL47D1qXIaSyZPaE1pu1lJo7', type='function')], function_call=None, provider_specific_fields=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc_cts = \"I'll use the simple_add tool to calculate 5 + 7 for you.\"\n",
    "tcq = mk_tc_req(tc_cts, [tc])\n",
    "tcq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a0364",
   "metadata": {},
   "source": [
    "Notice how Message instantiation creates a list of ChatCompletionMessageToolCalls by default. When the tools are executed this is converted back\n",
    "to a dictionary, for consistency we want to keep these as dictionaries from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cebbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc_req(content, tcs):\n",
    "    msg = Message(content=content, role='assistant', tool_calls=tcs, function_call=None)\n",
    "    msg.tool_calls = [{**dict(tc), 'function': dict(tc['function'])} for tc in msg.tool_calls]\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3468d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcq = mk_tc_req(tc_cts, [tc])\n",
    "tcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75dc3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chat(model, tools=[simple_add], hist=[pr, tcq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd673382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n",
      "\n",
      "Message(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490dcfb",
   "metadata": {},
   "source": [
    "Looks good so far! Now we will want to provide the actual result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e69d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc_result(tc, result): return {'tool_call_id': tc['id'], 'role': 'tool', 'name': tc['function']['name'], 'content': result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94067b82",
   "metadata": {},
   "source": [
    "Note we might have more than one tool call if more than one was passed in, here we just will make one result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b9d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 1,\n",
       " 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'},\n",
       " 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n",
       " 'type': 'function'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcq.tool_calls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f969e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '12'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_tc_result(tcq.tool_calls[0], '12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc_results(tcq, results): return [mk_tc_result(a,b) for a,b in zip(tcq.tool_calls, results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d8c658",
   "metadata": {},
   "source": [
    "Same for here tcq.tool_calls will match the number of results passed in the results list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e2307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2f72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '12'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcr = mk_tc_results(tcq, ['12'])\n",
    "tcr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b90d2",
   "metadata": {},
   "source": [
    "Now we can call it with this synthetic data to see what the response is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed96b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The result of 5 + 7 is **12**.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=720, total_tokens=737, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of 5 + 7 is **12**.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=17, prompt_tokens=720, total_tokens=737, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(tcr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e06d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n",
      "\n",
      "Message(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n",
      "\n",
      "{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n",
      "\n",
      "Message(content='The result of 5 + 7 is **12**.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b6af73",
   "metadata": {},
   "source": [
    "Lets try this again, but lets give it something that is clearly wrong for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a9049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chat(model, tools=[simple_add], hist=[pr, tcq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f546c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '13'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcr = mk_tc_results(tcq, ['13'])\n",
    "tcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7befdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The result of 5 + 7 is **12**.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=720, total_tokens=737, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of 5 + 7 is **12**.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=17, prompt_tokens=720, total_tokens=737, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(tcr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84387429",
   "metadata": {},
   "source": [
    "Lets make sure this works with multiple tool calls in the same assistant Message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs = [\n",
    "    mk_tc(simple_add.__name__, json.dumps({\"a\": 5, \"b\": 7})), \n",
    "    mk_tc(simple_add.__name__, json.dumps({\"a\": 6, \"b\": 7})), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44baa92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content='I will calculate these for you!', role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'type': 'function'}, {'index': 1, 'function': {'arguments': '{\"a\": 6, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'type': 'function'}], function_call=None, provider_specific_fields=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcq = mk_tc_req(\"I will calculate these for you!\", tcs)\n",
    "tcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr = mk_tc_results(tcq, ['12', '13'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866aa31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chat(model, tools=[simple_add], hist=[pr, tcq, tcr[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d9ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "5 + 7 = **12**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=13, prompt_tokens=812, total_tokens=825, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 7 = **12**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=13, prompt_tokens=812, total_tokens=825, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(tcr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee111193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n",
      "\n",
      "Message(content='I will calculate these for you!', role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'type': 'function'}, {'index': 1, 'function': {'arguments': '{\"a\": 6, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n",
      "\n",
      "{'tool_call_id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n",
      "\n",
      "{'tool_call_id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'role': 'tool', 'name': 'simple_add', 'content': '13'}\n",
      "\n",
      "Message(content='5 + 7 = **12**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b97b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The result of 5 + 3 is **8**.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=18, prompt_tokens=742, total_tokens=760, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='\\n\\nThe result of 5 + 3 is **8**.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=18, prompt_tokens=742, total_tokens=760, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(ms[1], tools=[simple_add])\n",
    "res = chat(\"What's 5 + 3? Use the `simple_add` tool.\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84d6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Here's a joke based on the number 8:\n",
       "\n",
       "Why was 6 afraid of 7?\n",
       "\n",
       "Because 7 8 (ate) 9!\n",
       "\n",
       "But since we got 8 as our answer, here's another one:\n",
       "\n",
       "What do you call an 8 that's been working out?\n",
       "\n",
       "An \"ate\" with great figure! ðŸ’ª\n",
       "\n",
       "(Get it? Because 8 already has a great figure with those curves!)\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=100, prompt_tokens=774, total_tokens=874, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Here\\'s a joke based on the number 8:\\n\\nWhy was 6 afraid of 7?\\n\\nBecause 7 8 (ate) 9!\\n\\nBut since we got 8 as our answer, here\\'s another one:\\n\\nWhat do you call an 8 that\\'s been working out?\\n\\nAn \"ate\" with great figure! ðŸ’ª\\n\\n(Get it? Because 8 already has a great figure with those curves!)', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=100, prompt_tokens=774, total_tokens=874, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(\"Now, tell me a joke based on that result.\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a8bec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"What's 5 + 3? Use the `simple_add` tool.\"},\n",
       " Message(content=None, role='assistant', tool_calls=[{'index': 0, 'function': {'arguments': '{\"a\": 5, \"b\": 3}', 'name': 'simple_add'}, 'id': 'toolu_016dgFwdeaQXSwLPnJzufcWq', 'type': 'function'}], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}),\n",
       " {'tool_call_id': 'toolu_016dgFwdeaQXSwLPnJzufcWq',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '8'},\n",
       " {'role': 'assistant',\n",
       "  'content': 'You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.'},\n",
       " Message(content='\\n\\nThe result of 5 + 3 is **8**.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}),\n",
       " {'role': 'user', 'content': 'Now, tell me a joke based on that result.'},\n",
       " Message(content='Here\\'s a joke based on the number 8:\\n\\nWhy was 6 afraid of 7?\\n\\nBecause 7 8 (ate) 9!\\n\\nBut since we got 8 as our answer, here\\'s another one:\\n\\nWhat do you call an 8 that\\'s been working out?\\n\\nAn \"ate\" with great figure! ðŸ’ª\\n\\n(Get it? Because 8 already has a great figure with those curves!)', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678268a",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60942eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# Image Description\n",
       "\n",
       "This adorable image shows a **Cavalier King Charles Spaniel puppy** with the classic Blenheim coloring (chestnut and white markings). \n",
       "\n",
       "## Key features visible:\n",
       "- **Puppy with expressive brown eyes** looking directly at the camera\n",
       "- **Soft, fluffy coat** with rich brown/chestnut patches on the ears and around the eyes\n",
       "- **White blaze** down the center of the face\n",
       "- **Lying on grass** in what appears to be a garden setting\n",
       "- **Purple flowers** (possibly asters) visible in the background\n",
       "- The puppy has a sweet, gentle expression typical of the breed\n",
       "\n",
       "The photo has a warm, professional quality with nice depth of field that keeps the focus on the puppy's endearing face while softly blurring the floral background.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=188, prompt_tokens=105, total_tokens=293, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"# Image Description\\n\\nThis adorable image shows a **Cavalier King Charles Spaniel puppy** with the classic Blenheim coloring (chestnut and white markings). \\n\\n## Key features visible:\\n- **Puppy with expressive brown eyes** looking directly at the camera\\n- **Soft, fluffy coat** with rich brown/chestnut patches on the ears and around the eyes\\n- **White blaze** down the center of the face\\n- **Lying on grass** in what appears to be a garden setting\\n- **Purple flowers** (possibly asters) visible in the background\\n- The puppy has a sweet, gentle expression typical of the breed\\n\\nThe photo has a warm, professional quality with nice depth of field that keeps the focus on the puppy's endearing face while softly blurring the floral background.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=188, prompt_tokens=105, total_tokens=293, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(ms[1])\n",
    "chat(['Whats in this img?',img_fn.read_bytes()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43244a6",
   "metadata": {},
   "source": [
    "### Prefill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fe330",
   "metadata": {},
   "source": [
    "Prefill works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034582f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is R E D A C T E D\n",
       "\n",
       "I don't actually know your name - you haven't told me what it is yet! If you'd like me to spell your name, please let me know what it is first.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=47, prompt_tokens=16, total_tokens=63, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Your name is R E D A C T E D\\n\\nI don't actually know your name - you haven't told me what it is yet! If you'd like me to spell your name, please let me know what it is first.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=47, prompt_tokens=16, total_tokens=63, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(ms[1])\n",
    "chat(\"Spell my name\",prefill=\"Your name is R E\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac64b334",
   "metadata": {},
   "source": [
    "And the entire message is stored in the history, not just the generated part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf54ca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content=\"Your name is R E D A C T E D\\n\\nI don't actually know your name - you haven't told me what it is yet! If you'd like me to spell your name, please let me know what it is first.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46716033",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe74496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1, 2, 3, 4, 5\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=11, total_tokens=28, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='1, 2, 3, 4, 5', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=17, prompt_tokens=11, total_tokens=28, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Chat(model)\n",
    "stream_gen = chat(\"Count to 5\", stream=True)\n",
    "for chunk in stream_gen:\n",
    "    if isinstance(chunk, ModelResponse): display(chunk)\n",
    "    else: print(delta_text(chunk) or '',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e55c0",
   "metadata": {},
   "source": [
    "Lets try prefill with streaming too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c058f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Okay! 6, 7, 8, 9, 10"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Okay! 6, 7, 8, 9, 10\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=12, prompt_tokens=44, total_tokens=56, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Okay! 6, 7, 8, 9, 10', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=12, prompt_tokens=44, total_tokens=56, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "stream_gen = chat(\"Continue counting to 10\",\"Okay! 6, 7\",stream=True)\n",
    "for chunk in stream_gen:\n",
    "    if isinstance(chunk, ModelResponse): display(chunk)\n",
    "    else: print(delta_text(chunk) or '',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c3666",
   "metadata": {},
   "source": [
    "### Tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17377a",
   "metadata": {},
   "source": [
    "Ok now lets test tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4cf429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**gemini/gemini-2.5-flash:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "I used the `simple_add` tool with `a=5` and `b=3`. The tool returned `8`.\n",
       "\n",
       "Therefore, 5 + 3 = 8.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=118, prompt_tokens=159, total_tokens=277, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=79, rejected_prediction_tokens=None, text_tokens=39), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=159, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='\\nI used the `simple_add` tool with `a=5` and `b=3`. The tool returned `8`.\\n\\nTherefore, 5 + 3 = 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=118, prompt_tokens=159, total_tokens=277, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=79, rejected_prediction_tokens=None, text_tokens=39), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=159, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**claude-sonnet-4-5:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**Result: 5 + 3 = 8**\n",
       "\n",
       "**Explanation:**\n",
       "The `simple_add` function takes two parameters:\n",
       "- `a` (first operand): I provided 5\n",
       "- `b` (second operand): I provided 3\n",
       "\n",
       "The function added these two numbers together and returned 8, which is the correct sum of 5 and 3.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=89, prompt_tokens=764, total_tokens=853, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='\\n\\n**Result: 5 + 3 = 8**\\n\\n**Explanation:**\\nThe `simple_add` function takes two parameters:\\n- `a` (first operand): I provided 5\\n- `b` (second operand): I provided 3\\n\\nThe function added these two numbers together and returned 8, which is the correct sum of 5 and 3.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=89, prompt_tokens=764, total_tokens=853, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**openai/gpt-4.1:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The result of 5 + 3 is 8.\n",
       "\n",
       "Explanation: I used the simple_add tool, which takes two numbers and adds them together. By inputting 5 and 3, the tool calculated the sum as 8.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=48, prompt_tokens=155, total_tokens=203, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_422e2d36a8', choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of 5 + 3 is 8.\\n\\nExplanation: I used the simple_add tool, which takes two numbers and adds them together. By inputting 5 and 3, the tool calculated the sum as 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=48, prompt_tokens=155, total_tokens=203, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for m in ms:\n",
    "    display(Markdown(f'**{m}:**'))\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's 5 + 3? Use the `simple_add` tool. Explain.\")\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f62a8",
   "metadata": {},
   "source": [
    "### Thinking w tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe375b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 5, \"b\": 3})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=125, prompt_tokens=638, total_tokens=763, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=43, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 3}', 'name': 'simple_add'}, 'id': 'toolu_01SY1R38L37vhWpgNgQz2B5h', 'type': 'function'}], function_call=None, reasoning_content='The user is asking for the sum of 5 and 3. I have access to a simple_add function that can add two numbers together. I need to call it with a=5 and b=3.', thinking_blocks=[{'type': 'thinking', 'thinking': 'The user is asking for the sum of 5 and 3. I have access to a simple_add function that can add two numbers together. I need to call it with a=5 and b=3.', 'signature': 'EsECCkYICBgCKkAAt4o53ref9vo+AM5o8rsH4EUfXF9CD7g7Kq9x1jh6kIBPC7jCJrAqyEo1ft0HVR+xVchZCwkojynhHDrBVORUEgyhI3za9d2OzRkou/EaDH3qx5BNu0Ic7HCS3yIw+HjMTO/LPZch6arIUGKbD3TOqfc/GQuIMxBQANo4AwrAydtd2AgnrhF+cfCJlzfJKqgB5BV2k7v2O80y4A+m05cKsVOEyB/1XvAydhXhW0rf58jVxNlOEqhXK6+9iVtSFmJIuN91Z09aBzYFSmGg3Tx47Lj3lgotQuAmGVSBNPpV2e7pYk4K4HLen+8lG6uPtPKjH5dixoX/aKxqS3wQq5WIiME2XDkJSheX3dyWze9cSoxQ1CP5zfe2an1X0VEUoC70NW1EwYMyHT9mT4ZvCeuEFqUWmCQMFyj6GAE='}], provider_specific_fields={'citations': None, 'thinking_blocks': [{'type': 'thinking', 'thinking': 'The user is asking for the sum of 5 and 3. I have access to a simple_add function that can add two numbers together. I need to call it with a=5 and b=3.', 'signature': 'EsECCkYICBgCKkAAt4o53ref9vo+AM5o8rsH4EUfXF9CD7g7Kq9x1jh6kIBPC7jCJrAqyEo1ft0HVR+xVchZCwkojynhHDrBVORUEgyhI3za9d2OzRkou/EaDH3qx5BNu0Ic7HCS3yIw+HjMTO/LPZch6arIUGKbD3TOqfc/GQuIMxBQANo4AwrAydtd2AgnrhF+cfCJlzfJKqgB5BV2k7v2O80y4A+m05cKsVOEyB/1XvAydhXhW0rf58jVxNlOEqhXK6+9iVtSFmJIuN91Z09aBzYFSmGg3Tx47Lj3lgotQuAmGVSBNPpV2e7pYk4K4HLen+8lG6uPtPKjH5dixoX/aKxqS3wQq5WIiME2XDkJSheX3dyWze9cSoxQ1CP5zfe2an1X0VEUoC70NW1EwYMyHT9mT4ZvCeuEFqUWmCQMFyj6GAE='}]}))], usage=Usage(completion_tokens=125, prompt_tokens=638, total_tokens=763, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=43, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_01SY1R38L37vhWpgNgQz2B5h',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '8'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "5 + 3 = **8**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=14, prompt_tokens=816, total_tokens=830, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='\\n\\n5 + 3 = **8**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=14, prompt_tokens=816, total_tokens=830, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Chat(model, tools=[simple_add])\n",
    "res = chat(\"What's 5 + 3?\",think='l',return_all=True)\n",
    "display(*res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fcaf1d",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otters are charismatic members of the weasel family found on every continent except Australia and Antarctica. There are 13 species in total, including sea otters and river otters.\n",
      "\n",
      "These aquatic mammals have elongated bodies, long tails, and soft, dense fur. In fact, otters have the densest fur of any animalâ€”as many as a million hairs per square inch. Webbed feet and powerful tails make otters strong swimmers.\n",
      "\n",
      "All otters are expert hunters that eat fish, crustaceans, and other critters. Sea otters float on their backs, place a rock on their chest, then smash mollusks down on it until it breaks open. They're also known for being playful animals, engaging in activities like sliding into water on natural slides."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Otters are [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") charismatic members of the weasel family found on every continent except Australia and Antarctica. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") There are 13 species in total, including sea otters and river otters.\n",
       "\n",
       "These aquatic mammals have [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") elongated bodies, long tails, and soft, dense fur. In fact, [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") otters have the densest fur of any animalâ€”as many as a million hairs per square inch. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") Webbed feet and powerful tails make otters strong swimmers.\n",
       "\n",
       "[*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") All otters are expert hunters that eat fish, crustaceans, and other critters. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") Sea otters float on their backs, place a rock on their chest, then smash mollusks down on it until it breaks open. They're also known for being [*](https://en.wikipedia.org/wiki/Otter \"Otter - Wikipedia\") playful animals, engaging in activities like sliding into water on natural slides.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=362, prompt_tokens=15055, total_tokens=15417, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Otters are [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") charismatic members of the weasel family found on every continent except Australia and Antarctica. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") There are 13 species in total, including sea otters and river otters.\\n\\nThese aquatic mammals have [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") elongated bodies, long tails, and soft, dense fur. In fact, [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") otters have the densest fur of any animalâ€”as many as a million hairs per square inch. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") Webbed feet and powerful tails make otters strong swimmers.\\n\\n[*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") All otters are expert hunters that eat fish, crustaceans, and other critters. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") Sea otters float on their backs, place a rock on their chest, then smash mollusks down on it until it breaks open. They\\'re also known for being [*](https://en.wikipedia.org/wiki/Otter \"Otter - Wikipedia\") playful animals, engaging in activities like sliding into water on natural slides.', role='assistant', tool_calls=[], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=362, prompt_tokens=15055, total_tokens=15417, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Chat(model)\n",
    "res = chat(\"Search the web and tell me very briefly about otters\", search='l', stream=True)\n",
    "for o in res:\n",
    "    if isinstance(o, ModelResponse): sleep(0.01); display(o)\n",
    "    else: print(delta_text(o) or '',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cce2d9",
   "metadata": {},
   "source": [
    "### Multi tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb42380",
   "metadata": {},
   "source": [
    "We can let the model call multiple tools in sequence using the `max_steps` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f7aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'll solve this step by step using the addition function.\n",
       "\n",
       "**Step 1:** First, let me calculate 5 + 3\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 5, \"b\": 3})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=100, prompt_tokens=617, total_tokens=717, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll solve this step by step using the addition function.\\n\\n**Step 1:** First, let me calculate 5 + 3\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 3}', 'name': 'simple_add'}, 'id': 'toolu_01SykhkA2BGKXm9J56KCkz2B', 'type': 'function'}], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=100, prompt_tokens=617, total_tokens=717, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_01SykhkA2BGKXm9J56KCkz2B',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '8'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Step 2:** Now I'll add 7 to that result (8 + 7)\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 8, \"b\": 7})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=93, prompt_tokens=730, total_tokens=823, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"**Step 2:** Now I'll add 7 to that result (8 + 7)\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 8, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_013LrGqASqf9Bsk38scV5Pu7', 'type': 'function'}], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=93, prompt_tokens=730, total_tokens=823, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_013LrGqASqf9Bsk38scV5Pu7',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '15'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Step 3:** Finally, I'll add 11 to that result (15 + 11)\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 15, \"b\": 11})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=94, prompt_tokens=836, total_tokens=930, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"**Step 3:** Finally, I'll add 11 to that result (15 + 11)\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 15, \"b\": 11}', 'name': 'simple_add'}, 'id': 'toolu_01RtpzYFxji9ZbQJtTjKwaCi', 'type': 'function'}], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=94, prompt_tokens=836, total_tokens=930, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_01RtpzYFxji9ZbQJtTjKwaCi',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '26'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Answer:** ((5 + 3) + 7) + 11 = **26**\n",
       "\n",
       "Here's the breakdown:\n",
       "- 5 + 3 = 8\n",
       "- 8 + 7 = 15\n",
       "- 15 + 11 = 26\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=67, prompt_tokens=943, total_tokens=1010, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"**Answer:** ((5 + 3) + 7) + 11 = **26**\\n\\nHere's the breakdown:\\n- 5 + 3 = 8\\n- 8 + 7 = 15\\n- 15 + 11 = 26\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=67, prompt_tokens=943, total_tokens=1010, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Chat(model, tools=[simple_add])\n",
    "res = chat(\"What's ((5 + 3)+7)+11? Work step by step\", return_all=True, max_steps=5)\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f66101",
   "metadata": {},
   "source": [
    "Some models support parallel tool calling. I.e. sending multiple tool call requests in one conversation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec77539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 5, \"b\": 3})\n",
       "\n",
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 7, \"b\": 2})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=52, prompt_tokens=110, total_tokens=162, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_e24a1fec47', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"a\": 5, \"b\": 3}', 'name': 'simple_add'}, 'id': 'call_qJXSxYvc2ZVHmyIxqQ9OocWM', 'type': 'function'}, {'function': {'arguments': '{\"a\": 7, \"b\": 2}', 'name': 'simple_add'}, 'id': 'call_hCgeAPtd0RhmeADBRWRvY0sG', 'type': 'function'}], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=52, prompt_tokens=110, total_tokens=162, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_qJXSxYvc2ZVHmyIxqQ9OocWM',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '8'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_hCgeAPtd0RhmeADBRWRvY0sG',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '9'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ multiply({\"a\":8,\"b\":9})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=178, total_tokens=195, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_e24a1fec47', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"a\":8,\"b\":9}', 'name': 'multiply'}, 'id': 'call_1nwxhn7RXLNl9FcsS8pfn6OZ', 'type': 'function'}], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=17, prompt_tokens=178, total_tokens=195, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_1nwxhn7RXLNl9FcsS8pfn6OZ',\n",
       " 'role': 'tool',\n",
       " 'name': 'multiply',\n",
       " 'content': '72'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "(5 + 3) = 8 and (7 + 2) = 9. Multiplying them gives: 8 Ã— 9 = 72. \n",
       "\n",
       "So, (5 + 3) Ã— (7 + 2) = 72.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=55, prompt_tokens=203, total_tokens=258, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_e24a1fec47', choices=[Choices(finish_reason='stop', index=0, message=Message(content='(5 + 3) = 8 and (7 + 2) = 9. Multiplying them gives: 8 Ã— 9 = 72. \\n\\nSo, (5 + 3) Ã— (7 + 2) = 72.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=55, prompt_tokens=203, total_tokens=258, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"Multiply two numbers\"\n",
    "    return a * b\n",
    "\n",
    "chat = Chat('openai/gpt-4.1', tools=[simple_add, multiply])\n",
    "res = chat(\"Calculate (5 + 3) * (7 + 2)\", max_steps=5, return_all=True)\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4958f",
   "metadata": {},
   "source": [
    "See it did the additions in one go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b17e71",
   "metadata": {},
   "source": [
    "We don't want the model to keep running tools indefinitely. Lets showcase how we can force thee model to stop after our specified number of toolcall rounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7298c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'll calculate this step by step, following the order of operations.\n",
       "\n",
       "**Step 1:** Calculate the inner parentheses first\n",
       "- (10 + 5) = ?\n",
       "- (2 + 1) = ?\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 10, \"b\": 5})\n",
       "\n",
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 2, \"b\": 1})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=173, prompt_tokens=792, total_tokens=965, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate this step by step, following the order of operations.\\n\\n**Step 1:** Calculate the inner parentheses first\\n- (10 + 5) = ?\\n- (2 + 1) = ?\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 10, \"b\": 5}', 'name': 'simple_add'}, 'id': 'toolu_01NZjJc2q4tMJZcS93T1WQHM', 'type': 'function'}, {'index': 2, 'function': {'arguments': '{\"a\": 2, \"b\": 1}', 'name': 'simple_add'}, 'id': 'toolu_013qQVARNY8a6shg4zo2TpNr', 'type': 'function'}], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=173, prompt_tokens=792, total_tokens=965, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_01NZjJc2q4tMJZcS93T1WQHM',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '15'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_013qQVARNY8a6shg4zo2TpNr',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '3'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**Step 2:** Multiply 15 * 3\n",
       "\n",
       "ðŸ”§ multiply({\"a\": 15, \"b\": 3})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=82, prompt_tokens=1030, total_tokens=1112, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content='**Step 2:** Multiply 15 * 3', role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 15, \"b\": 3}', 'name': 'multiply'}, 'id': 'toolu_01Uf17eEfZPHcqFo1C3PYZ5E', 'type': 'function'}], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=82, prompt_tokens=1030, total_tokens=1112, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_01Uf17eEfZPHcqFo1C3PYZ5E',\n",
       " 'role': 'tool',\n",
       " 'name': 'multiply',\n",
       " 'content': '45'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "**Step 3:** Divide 45 / 3\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=23, prompt_tokens=1139, total_tokens=1162, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='\\n\\n**Step 3:** Divide 45 / 3', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=23, prompt_tokens=1139, total_tokens=1162, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def divide(a: int, b: int) -> float:\n",
    "    \"Divide two numbers\"\n",
    "    return a / b\n",
    "\n",
    "chat = Chat(model, tools=[simple_add, multiply, divide])\n",
    "res = chat(\"Calculate ((10 + 5) * 3) / (2 + 1) step by step.\", \n",
    "           max_steps=3, return_all=True,\n",
    "           final_prompt=\"Please wrap-up for now and summarize how far we got.\")\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffeeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(len([o for o in res if isinstance(o,ModelResponse)]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58dd00b",
   "metadata": {},
   "source": [
    "### Tool call exhaustion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d02210",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = \"What is 1+2, and then the result of adding +2, and then +3 to it? Use tools to calculate!\"\n",
    "c = Chat(model, tools=[simple_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82a43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "Let me continue with the next calculation. Now I'll add 2 to the result (3+2):\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=33, prompt_tokens=777, total_tokens=810, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"\\n\\nLet me continue with the next calculation. Now I'll add 2 to the result (3+2):\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=33, prompt_tokens=777, total_tokens=810, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = c(pr, max_steps=2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert c.hist[-2]['content'] == _final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca77d9d",
   "metadata": {},
   "source": [
    "## Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494116db",
   "metadata": {},
   "source": [
    "### AsyncChat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25921ffa",
   "metadata": {},
   "source": [
    "If you want to use LiteLLM in a webapp you probably want to use their async function `acompletion`.\n",
    "To make that easier we will implement our version of `AsyncChat` to complement it. It follows the same implementation as Chat as much as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _alite_call_func(tc, ns, raise_on_err=True):\n",
    "    try: fargs = json.loads(tc.function.arguments)\n",
    "    except Exception as e: raise ValueError(f\"Failed to parse function arguments: {tc.function.arguments}\") from e\n",
    "    res = await call_func_async(tc.function.name, fargs, ns=ns)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": str(res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@asave_iter\n",
    "async def astream_with_complete(self, agen, postproc=noop):\n",
    "    chunks = []\n",
    "    async for chunk in agen:\n",
    "        chunks.append(chunk)\n",
    "        postproc(chunk)\n",
    "        yield chunk\n",
    "    self.value = stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc01816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AsyncChat(Chat):\n",
    "    async def _call(self, msg=None, prefill=None, temp=None, think=None, search=None, stream=False, max_steps=2, step=1, final_prompt=None, tool_choice=None, **kwargs):\n",
    "        if step>max_steps+1: return\n",
    "        if not get_model_info(self.model).get(\"supports_assistant_prefill\"): prefill=None\n",
    "        if _has_search(self.model) and (s:=ifnone(search,self.search)): kwargs['web_search_options'] = {\"search_context_size\": effort[s]}\n",
    "        else: _=kwargs.pop('web_search_options',None)\n",
    "        res = await acompletion(model=self.model, messages=self._prep_msg(msg, prefill), stream=stream,\n",
    "                         tools=self.tool_schemas, reasoning_effort=effort.get(think), tool_choice=tool_choice,\n",
    "                         # temperature is not supported when reasoning\n",
    "                         temperature=None if think else ifnone(temp,self.temp), \n",
    "                         **kwargs)\n",
    "        if stream:\n",
    "            if prefill: yield _mk_prefill(prefill)\n",
    "            res = astream_with_complete(res,postproc=cite_footnote)\n",
    "            async for chunk in res: yield chunk\n",
    "            res = res.value\n",
    "        m=res.choices[0].message\n",
    "        if prefill: m.content = prefill + m.content\n",
    "        yield res\n",
    "        self.hist.append(m)\n",
    "\n",
    "        if tcs := m.tool_calls:\n",
    "            tool_results = []\n",
    "            for tc in tcs:\n",
    "                result = await _alite_call_func(tc, ns=self.ns)\n",
    "                tool_results.append(result)\n",
    "                yield result\n",
    "            self.hist+=tool_results\n",
    "            if step>=max_steps-1: prompt,tool_choice,search = final_prompt,'none',False\n",
    "            else: prompt = None\n",
    "            async for result in self._call(\n",
    "                prompt, prefill, temp, think, search, stream, max_steps, step+1,\n",
    "                final_prompt, tool_choice=tool_choice, **kwargs):\n",
    "                    yield result\n",
    "    \n",
    "    async def __call__(self,\n",
    "                       msg=None,          # Message str, or list of multiple message parts\n",
    "                       prefill=None,      # Prefill AI response if model supports it\n",
    "                       temp=None,         # Override temp set on chat initialization\n",
    "                       think=None,        # Thinking (l,m,h)\n",
    "                       search=None,       # Override search set on chat initialization (l,m,h)\n",
    "                       stream=False,      # Stream results\n",
    "                       max_steps=2, # Maximum number of tool calls\n",
    "                       final_prompt=_final_prompt, # Final prompt when tool calls have ran out \n",
    "                       return_all=False,  # Returns all intermediate ModelResponses if not streaming and has tool calls\n",
    "                       **kwargs):\n",
    "        result_gen = self._call(msg, prefill, temp, think, search, stream, max_steps, 1, final_prompt, **kwargs)\n",
    "        if stream or return_all: return result_gen\n",
    "        async for res in result_gen: pass\n",
    "        return res # normal chat behavior only return last msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa22b71",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5955051",
   "metadata": {},
   "source": [
    "Basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142f088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2+2 = 4\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=11, prompt_tokens=14, total_tokens=25, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='2+2 = 4', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=11, prompt_tokens=14, total_tokens=25, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = AsyncChat(model)\n",
    "await chat(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b5855",
   "metadata": {},
   "source": [
    "With tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e178ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_add(a: int, b: int) -> int:\n",
    "    \"Add two numbers asynchronously\"\n",
    "    await asyncio.sleep(0.1)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f99c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ async_add({\"a\": 5, \"b\": 7})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=70, prompt_tokens=607, total_tokens=677, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"a\": 5, \"b\": 7}', name='async_add'), id='toolu_01NHDNkcpwxW66XRuRFChLxe', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=70, prompt_tokens=607, total_tokens=677, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_01NHDNkcpwxW66XRuRFChLxe',\n",
       " 'role': 'tool',\n",
       " 'name': 'async_add',\n",
       " 'content': '12'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "The result of 5 + 7 is **12**.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=18, prompt_tokens=731, total_tokens=749, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='\\n\\nThe result of 5 + 7 is **12**.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=18, prompt_tokens=731, total_tokens=749, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_tools = AsyncChat(model, tools=[async_add])\n",
    "res = await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\", return_all=True)\n",
    "async for r in res: display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68aff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is 2+2?'},\n",
       " Message(content='2+2 = 4', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93874ba",
   "metadata": {},
   "source": [
    "## Async Streaming Display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150a316",
   "metadata": {},
   "source": [
    "This is what our outputs look like with streaming results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57698c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ async_add\n",
      "{'tool_call_id': 'toolu_011RxwEK3HSc3VQwwsBZnXnV', 'role': 'tool', 'name': 'async_add', 'content': '12'}\n",
      "\n",
      "\n",
      "The result of 5 + 7 is **12**."
     ]
    }
   ],
   "source": [
    "chat_with_tools = AsyncChat(model, tools=[async_add])\n",
    "res = await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\n",
    "async for o in res:\n",
    "    if isinstance(o,ModelResponseStream): print(delta_text(o) or '',end='')\n",
    "    elif isinstance(o,dict): print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9280710",
   "metadata": {},
   "source": [
    "We use this one quite a bit so we want to provide some utilities to better format these outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99789ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _trunc_str(s, mx=2000, replace=\"<TRUNCATED>\"):\n",
    "    \"Truncate `s` to `mx` chars max, adding `replace` if truncated\"\n",
    "    s = str(s).strip()\n",
    "    return s[:mx]+replace if len(s)>mx else s"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309f775",
   "metadata": {},
   "source": [
    "Here's a complete `ModelResponse` taken from the response stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203c123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01CWqrNQvoRjf1Q1GLpTUgQR', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=228, prompt_tokens=794, total_tokens=1022, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "resp = ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01CWqrNQvoRjf1Q1GLpTUgQR', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=228, prompt_tokens=794, total_tokens=1022, prompt_tokens_details=None))\n",
    "print(repr(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69f062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc=resp.choices[0].message.tool_calls[0]\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c0add",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr={'tool_call_id': 'toolu_018BGyenjiRkDQFU1jWP6qRo', 'role': 'tool','name': 'simple_add',\n",
    "    'content': '15 is the answerrrr' +'r'*2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7476563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tr_details(tr, tc, mx=2000):\n",
    "    \"Create <details> block for tool call as JSON\"\n",
    "    args = {k:_trunc_str(v, mx=mx) for k,v in json.loads(tc.function.arguments).items()}\n",
    "    res = {'id':tr['tool_call_id'], \n",
    "           'call':{'function': tc.function.name, 'arguments': args},\n",
    "           'result':_trunc_str(tr.get('content'), mx=mx),}\n",
    "    return f\"\\n\\n{detls_tag}\\n\\n```json\\n{dumps(res, indent=2)}\\n```\\n\\n</details>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973a7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<details class=\\'tool-usage-details\\'>\\n\\n```json\\n{\\n  \"id\": \"toolu_018BGyenjiRkDQFU1jWP6qRo\",\\n  \"call\": {\\n    \"function\": \"simple_add\",\\n    \"arguments\": {\\n      \"a\": \"10\",\\n      \"b\": \"5\"\\n    }\\n  },\\n  \"result\": \"15 is the answerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr<TRUNCATED>\"\\n}\\n```\\n\\n</details>\\n\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_tr_details(tr,tc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AsyncStreamFormatter:\n",
    "    def __init__(self, include_usage=False, mx=2000):\n",
    "        self.outp,self.tcs,self.include_usage,self.think,self.mx = '',{},include_usage,False,mx\n",
    "    \n",
    "    def format_item(self, o):\n",
    "        \"Format a single item from the response stream.\"\n",
    "        res = ''\n",
    "        if isinstance(o, ModelResponseStream):\n",
    "            d = o.choices[0].delta\n",
    "            if nested_idx(d, 'reasoning_content'): \n",
    "                self.think = True\n",
    "                res += 'ðŸ§ '\n",
    "            elif self.think:\n",
    "                self.think = False\n",
    "                res += '\\n\\n'\n",
    "            if c:=d.content: res+=c\n",
    "        elif isinstance(o, ModelResponse):\n",
    "            if self.include_usage: res += f\"\\nUsage: {o.usage}\"\n",
    "            if c:=getattr(o.choices[0].message,'tool_calls',None):\n",
    "                self.tcs = {tc.id:tc for tc in c}\n",
    "        elif isinstance(o, dict) and 'tool_call_id' in o:\n",
    "            res += mk_tr_details(o, self.tcs.pop(o['tool_call_id']), mx=self.mx)\n",
    "        self.outp+=res\n",
    "        return res\n",
    "    \n",
    "    async def format_stream(self, rs):\n",
    "        \"Format the response stream for markdown display.\"\n",
    "        async for o in rs: yield self.format_item(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ecbcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "stream_msg = ModelResponseStream([StreamingChoices(delta=Delta(content=\"Hello world!\"))])\n",
    "print(repr(AsyncStreamFormatter().format_item(stream_msg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fafd181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ðŸ§ '\n"
     ]
    }
   ],
   "source": [
    "reasoning_msg = ModelResponseStream([StreamingChoices(delta=Delta(reasoning_content=\"thinking...\"))])\n",
    "print(repr(AsyncStreamFormatter().format_item(reasoning_msg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_tool_call = ChatCompletionMessageToolCall(\n",
    "    id=\"toolu_123abc456def\", type=\"function\", \n",
    "    function=Function( name=\"simple_add\", arguments='{\"a\": 5, \"b\": 3}' )\n",
    ")\n",
    "\n",
    "mock_response = ModelResponse()\n",
    "mock_response.choices = [type('Choice', (), {\n",
    "    'message': type('Message', (), {\n",
    "        'tool_calls': [mock_tool_call]\n",
    "    })()\n",
    "})()]\n",
    "\n",
    "mock_tool_result = {\n",
    "    'tool_call_id': 'toolu_123abc456def', 'role': 'tool', \n",
    "    'name': 'simple_add', 'content': '8'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9362bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<details class='tool-usage-details'>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"id\": \"toolu_123abc456def\",\n",
      "  \"call\": {\n",
      "    \"function\": \"simple_add\",\n",
      "    \"arguments\": {\n",
      "      \"a\": \"5\",\n",
      "      \"b\": \"3\"\n",
      "    }\n",
      "  },\n",
      "  \"result\": \"8\"\n",
      "}\n",
      "```\n",
      "\n",
      "</details>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fmt = AsyncStreamFormatter()\n",
    "fmt.format_item(mock_response)\n",
    "print(fmt.format_item(mock_tool_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea8c05",
   "metadata": {},
   "source": [
    "In jupyter it's nice to use this `AsyncStreamFormatter` in combination with the `Markdown` `display`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def adisplay_stream(rs):\n",
    "    \"Use IPython.display to markdown display the response stream.\"\n",
    "    fmt = AsyncStreamFormatter()\n",
    "    md = ''\n",
    "    async for o in fmt.format_stream(rs): \n",
    "        md+=o\n",
    "        display(Markdown(md),clear=True)\n",
    "    return fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac772065",
   "metadata": {},
   "source": [
    "## Streaming examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c2ce8",
   "metadata": {},
   "source": [
    "Now we can demonstrate `AsyncChat` with `stream=True`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae17b5b",
   "metadata": {},
   "source": [
    "### Tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375953eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "<details class='tool-usage-details'>\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"id\": \"toolu_011RxwEK3HSc3VQwwsBZnXnV\",\n",
       "  \"call\": {\n",
       "    \"function\": \"async_add\",\n",
       "    \"arguments\": {\n",
       "      \"a\": \"5\",\n",
       "      \"b\": \"7\"\n",
       "    }\n",
       "  },\n",
       "  \"result\": \"12\"\n",
       "}\n",
       "```\n",
       "\n",
       "</details>\n",
       "\n",
       "\n",
       "\n",
       "The result of 5 + 7 is **12**."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = AsyncChat(model, tools=[async_add])\n",
    "res = await chat(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\n",
    "fmt = await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cdab76",
   "metadata": {},
   "source": [
    "### Thinking tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97da43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
       "\n",
       "# Use your language's built-in sort\n",
       "\n",
       "For 1000 random integers, **use your language's built-in sort function** (e.g., Python's `sorted()`, Java's `Arrays.sort()`, C++'s `std::sort()`).\n",
       "\n",
       "These implementations use highly optimized algorithms like:\n",
       "- **Timsort** (Python/Java) \n",
       "- **Introsort** (C++)\n",
       "- **Dual-pivot Quicksort** (Java primitives)\n",
       "\n",
       "All are O(n log n) and will outperform hand-coded solutions for this dataset size.\n",
       "\n",
       "---\n",
       "\n",
       "**If implementing yourself**: Use **Quicksort** or **Mergesort** â€” both O(n log n) average case and efficient for this size."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = AsyncChat(model)\n",
    "res = await chat(\"Briefly, what's the most efficient way to sort a list of 1000 random integers?\",\n",
    "                 think='l',stream=True)\n",
    "_ = await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5580e7f",
   "metadata": {},
   "source": [
    "### Multiple tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc5b196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'll calculate ((10 + 5) * 3) / (2 + 1) step by step, using parallel calls where possible.\n",
       "\n",
       "**Batch 1: Calculate the independent additions**\n",
       "Let me start by calculating the two addition operations that don't depend on each other:\n",
       "- 10 + 5 (for the numerator)\n",
       "- 2 + 1 (for the denominator)\n",
       "\n",
       "<details class='tool-usage-details'>\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"id\": \"toolu_017kxrJ18BiKbYNV5vuJ4Lpf\",\n",
       "  \"call\": {\n",
       "    \"function\": \"simple_add\",\n",
       "    \"arguments\": {\n",
       "      \"a\": \"10\",\n",
       "      \"b\": \"5\"\n",
       "    }\n",
       "  },\n",
       "  \"result\": \"15\"\n",
       "}\n",
       "```\n",
       "\n",
       "</details>\n",
       "\n",
       "\n",
       "\n",
       "<details class='tool-usage-details'>\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"id\": \"toolu_01XnKDfoAU6uqAL1V8CFwUwS\",\n",
       "  \"call\": {\n",
       "    \"function\": \"simple_add\",\n",
       "    \"arguments\": {\n",
       "      \"a\": \"2\",\n",
       "      \"b\": \"1\"\n",
       "    }\n",
       "  },\n",
       "  \"result\": \"3\"\n",
       "}\n",
       "```\n",
       "\n",
       "</details>\n",
       "\n",
       "**After Batch 1:** We have:\n",
       "- 10 + 5 = 15\n",
       "- 2 + 1 = 3\n",
       "- So our expression is now: (15 * 3) / 3\n",
       "\n",
       "**Batch 2: Calculate the multiplication**\n",
       "Now I need to multiply 15 * 3 before I can do the final division:\n",
       "\n",
       "<details class='tool-usage-details'>\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"id\": \"toolu_01P6iHCianQyB3sfxXquqKik\",\n",
       "  \"call\": {\n",
       "    \"function\": \"multiply\",\n",
       "    \"arguments\": {\n",
       "      \"a\": \"15\",\n",
       "      \"b\": \"3\"\n",
       "    }\n",
       "  },\n",
       "  \"result\": \"45\"\n",
       "}\n",
       "```\n",
       "\n",
       "</details>\n",
       "\n",
       "\n",
       "\n",
       "**After Batch 2:** We have:\n",
       "- 15 * 3 = 45\n",
       "- So our expression is now: 45 / 3\n",
       "\n",
       "**Batch 3: Calculate the final division**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "chat = AsyncChat(model, tools=[simple_add, multiply, divide])\n",
    "res = await chat(\"Calculate ((10 + 5) * 3) / (2 + 1) Use parallel tool calls, but explain where we are after each batch.\", \n",
    "           max_steps=3, stream=True,\n",
    "           final_prompt=\"Please wrap-up for now and summarize how far we got.\")\n",
    "fmt = await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a72c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step, using parallel calls where possible.\\n\\n**Batch 1: Calculate the independent additions**\\nLet me start by calculating the two addition operations that don't depend on each other:\\n- 10 + 5 (for the numerator)\\n- 2 + 1 (for the denominator)\", role='assistant', tool_calls=[{'function': {'arguments': '{\"a\": 10, \"b\": 5}', 'name': 'simple_add'}, 'id': 'toolu_017kxrJ18BiKbYNV5vuJ4Lpf', 'type': 'function'}, {'function': {'arguments': '{\"a\": 2, \"b\": 1}', 'name': 'simple_add'}, 'id': 'toolu_01XnKDfoAU6uqAL1V8CFwUwS', 'type': 'function'}], function_call=None, provider_specific_fields=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea0f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_017kxrJ18BiKbYNV5vuJ4Lpf',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '15'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81c0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_01XnKDfoAU6uqAL1V8CFwUwS',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '3'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456910e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content='**After Batch 1:** We have:\\n- 10 + 5 = 15\\n- 2 + 1 = 3\\n- So our expression is now: (15 * 3) / 3\\n\\n**Batch 2: Calculate the multiplication**\\nNow I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[{'function': {'arguments': '{\"a\": 15, \"b\": 3}', 'name': 'multiply'}, 'id': 'toolu_01P6iHCianQyB3sfxXquqKik', 'type': 'function'}], function_call=None, provider_specific_fields=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf490ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_01P6iHCianQyB3sfxXquqKik',\n",
       " 'role': 'tool',\n",
       " 'name': 'multiply',\n",
       " 'content': '45'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364a8bbe",
   "metadata": {},
   "source": [
    "Now to demonstrate that we can load back the formatted output back into a new `Chat` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55653287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "We just calculated the mathematical expression **((10 + 5) * 3) / (2 + 1)** step by step!\n",
       "\n",
       "Here's what happened:\n",
       "\n",
       "1. **First**, I calculated the two additions in parentheses:\n",
       "   - 10 + 5 = 15\n",
       "   - 2 + 1 = 3\n",
       "\n",
       "2. **Then**, I multiplied the first result by 3:\n",
       "   - 15 * 3 = 45\n",
       "\n",
       "3. **Finally**, I was about to divide 45 by 3 to get the final answer (which would be 15), but you asked me this question before I completed that last step!\n",
       "\n",
       "So we were working through a math problem using function calls, following the proper order of operations (parentheses first, then multiplication/division from left to right).\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=188, prompt_tokens=1262, total_tokens=1450, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"We just calculated the mathematical expression **((10 + 5) * 3) / (2 + 1)** step by step!\\n\\nHere's what happened:\\n\\n1. **First**, I calculated the two additions in parentheses:\\n   - 10 + 5 = 15\\n   - 2 + 1 = 3\\n\\n2. **Then**, I multiplied the first result by 3:\\n   - 15 * 3 = 45\\n\\n3. **Finally**, I was about to divide 45 by 3 to get the final answer (which would be 15), but you asked me this question before I completed that last step!\\n\\nSo we were working through a math problem using function calls, following the proper order of operations (parentheses first, then multiplication/division from left to right).\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=188, prompt_tokens=1262, total_tokens=1450, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat5 = Chat(model,hist=fmt2hist(fmt.outp),tools=[simple_add, multiply, divide])\n",
    "chat5('what did we just do?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc0df26",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124da784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Otters are [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") charismatic members of the weasel family found on every continent except Australia and Antarctica. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") There are 13 species in total, including sea otters and river otters.\n",
       "\n",
       "These aquatic mammals have [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") elongated bodies, long tails, and soft, dense fur. In fact, [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") otters have the densest fur of any animalâ€”as many as a million hairs per square inch. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") Webbed feet and powerful tails make otters strong swimmers.\n",
       "\n",
       "[*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") All otters are expert hunters that eat fish, crustaceans, and other critters. [*](https://www.nationalgeographic.com/animals/mammals/facts/otters-1 \"Otters, facts and information | National Geographic\") Sea otters float on their backs, place a rock on their chest, then smash mollusks down on it until it breaks open. They're also known for being [*](https://en.wikipedia.org/wiki/Otter \"Otter - Wikipedia\") playful animals, engaging in activities like sliding into water on natural slides."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_stream_tools = AsyncChat(model, search='l')\n",
    "res = await chat_stream_tools(\"Search the web and tell me very briefly about otters\", stream=True)\n",
    "_=await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1fb99",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "811571e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "a,b = random.randint(0,100), random.randint(0,100)\n",
    "hist = [[f\"What is {a}+{b}?\\n\" * 200], f\"It's {a+b}\", ['hi'], \"Hello\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bc2dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AsyncChat(model, cache=True, hist=hist)\n",
    "rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f9390d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage(completion_tokens=13, prompt_tokens=3, total_tokens=16, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=1623, cache_read_input_tokens=0)\n"
     ]
    }
   ],
   "source": [
    "async for o in rs: \n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fac1d3",
   "metadata": {},
   "source": [
    "In this first api call we will see cache creation until the last user msg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f372cd99",
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_read_toks = o.usage.cache_creation_input_tokens\n",
    "test_eq(cache_read_toks > 1000, True)\n",
    "test_eq(o.usage.cache_read_input_tokens, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd079aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "hist.extend([['hi again'], 'how may i help you?'])\n",
    "chat = AsyncChat(model, cache=True, hist=hist)\n",
    "rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a940a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage(completion_tokens=17, prompt_tokens=1626, total_tokens=1643, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=1623, text_tokens=None, image_tokens=None), cache_creation_input_tokens=14, cache_read_input_tokens=1623)\n"
     ]
    }
   ],
   "source": [
    "async for o in rs:\n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c29a5ac",
   "metadata": {},
   "source": [
    "The subsequent call should re-use the existing cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ae13c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(o.usage.cache_read_input_tokens, cache_read_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9764dc",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e677e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hides\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e942ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "solveit_dialog_mode": "learning",
  "solveit_ver": 2
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
