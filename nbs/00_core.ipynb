{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "798b1171",
   "metadata": {},
   "source": [
    "# Core\n",
    "\n",
    "> Lisette Core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from cachy import enable_cachy,disable_cachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82380377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import asyncio, base64, json, litellm, mimetypes, random, string\n",
    "from typing import Optional,Callable\n",
    "from html import escape\n",
    "from litellm import (acompletion, completion, stream_chunk_builder, Message,\n",
    "                     ModelResponse, ModelResponseStream, get_model_info, register_model, Usage)\n",
    "from litellm.utils import function_to_dict, StreamingChoices, Delta, ChatCompletionMessageToolCall, Function, Choices\n",
    "from toolslm.funccall import mk_ns, call_func, call_func_async, get_schema\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import delegates\n",
    "from fastcore import imghdr\n",
    "from dataclasses import dataclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "from IPython.display import Markdown, Image, Audio, Video\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f20759",
   "metadata": {},
   "source": [
    "# LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c07a774f",
   "metadata": {},
   "source": [
    "## Deterministic outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4dfd53d",
   "metadata": {},
   "source": [
    "LiteLLM `ModelResponse(Stream)` objects have `id` and `created_at` fields that are generated dynamically. Even when we use [`cachy`](https://github.com/answerdotai/cachy) to cache the LLM response these dynamic fields create diffs which makes code review more challenging. The patches below ensure that `id` and `created_at` fields are fixed and won't generate diffs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c28e8ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def patch_litellm(seed=0):\n",
    "    \"Patch litellm.ModelResponseBase such that `id` and `created` are fixed.\"\n",
    "    from litellm.types.utils import ModelResponseBase\n",
    "    @patch\n",
    "    def __init__(self: ModelResponseBase, id=None, created=None, *args, **kwargs): \n",
    "        self._orig___init__(id='chatcmpl-xxx', created=1000000000, *args, **kwargs)\n",
    "\n",
    "    @patch\n",
    "    def __setattr__(self: ModelResponseBase, name, value):\n",
    "        if name == 'id': value = 'chatcmpl-xxx'\n",
    "        elif name == 'created': value = 1000000000\n",
    "        self._orig___setattr__(name, value)\n",
    "\n",
    "    if seed is not None: random.seed(seed) # ensures random ids like tool call ids are deterministic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e05085",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_litellm()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfbbaf6",
   "metadata": {},
   "source": [
    "## Completion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c4ea2",
   "metadata": {},
   "source": [
    "LiteLLM provides an convenient unified interface for most big LLM providers. Because it's so useful to be able to switch LLM providers with just one argument. We want to make it even easier to by adding some more convenience functions and classes. \n",
    "\n",
    "This is very similar to our other wrapper libraries for popular AI providers: [claudette](https://claudette.answer.ai/) (Anthropic), [gaspard](https://github.com/AnswerDotAI/gaspard) (Gemini), [cosette](https://answerdotai.github.io/cosette/) (OpenAI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61cf441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _repr_markdown_(self: litellm.ModelResponse):\n",
    "    message = self.choices[0].message\n",
    "    content = ''\n",
    "    if mc:=message.content: content += mc[0]['text'] if isinstance(mc,list) else mc\n",
    "    if message.tool_calls:\n",
    "        tool_calls = [f\"\\n\\nðŸ”§ {nested_idx(tc,'function','name')}({nested_idx(tc,'function','arguments')})\\n\" for tc in message.tool_calls]\n",
    "        content += \"\\n\".join(tool_calls)\n",
    "    if not content: content = str(message)\n",
    "    details = [\n",
    "        f\"id: `{self.id}`\",\n",
    "        f\"model: `{self.model}`\",\n",
    "        f\"finish_reason: `{self.choices[0].finish_reason}`\"\n",
    "    ]\n",
    "    if hasattr(self, 'usage') and self.usage: details.append(f\"usage: `{self.usage}`\")\n",
    "    det_str = '\\n- '.join(details)\n",
    "    \n",
    "    return f\"\"\"{content}\n",
    "\n",
    "<details>\n",
    "\n",
    "- {det_str}\n",
    "\n",
    "</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fac6cee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "register_model({\n",
    "    \"claude-opus-4-5\": {\n",
    "        \"litellm_provider\": \"anthropic\", \"mode\": \"chat\",\n",
    "        \"max_tokens\": 64000, \"max_input_tokens\": 200000, \"max_output_tokens\": 64000,\n",
    "        \"input_cost_per_token\": 0.000005, \"output_cost_per_token\": 0.000025,\n",
    "        \"cache_creation_input_token_cost\": 0.000005*1.25, \"cache_read_input_token_cost\": 0.000005*0.1,\n",
    "        \"supports_function_calling\": True, \"supports_parallel_function_calling\": True,\n",
    "        \"supports_vision\": True, \"supports_prompt_caching\": True, \"supports_response_schema\": True,\n",
    "        \"supports_system_messages\": True, \"supports_reasoning\": True, \"supports_assistant_prefill\": True,\n",
    "        \"supports_tool_choice\": True, \"supports_computer_use\": True, \"supports_web_search\": True\n",
    "    }\n",
    "});\n",
    "sonn45 = \"claude-sonnet-4-5\"\n",
    "opus45 = \"claude-opus-4-5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf2d2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# litellm._turn_on_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a6f62b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**gemini/gemini-3-pro-preview:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I help you today?\n",
       "\n",
       "Whether you need help with a creative project, have a burning question, or just want to chat, I'm all ears. What's on your mind?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-3-pro-preview`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=163, prompt_tokens=4, total_tokens=167, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=120, rejected_prediction_tokens=None, text_tokens=43, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-3-pro-preview', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hello! How can I help you today?\\n\\nWhether you need help with a creative project, have a burning question, or just want to chat, I'm all ears. What's on your mind?\", role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"text\": \"Hello! How can I help you today?\\\\n\\\\nWhether you need help with a creative project, have a burning question, or just want to chat, I\\'m all ears. What\\'s on your mind?\"}', 'signature': 'EvoDCvcDAXLI2nwStssJVsA/41VhNGBtTYcO/AB6v51G/mWckRl8dTv7yc9sBdsfL/ZyZS46ncXHwgzkascEc6y3GrwXfb3U6h7+DjZQmYIx1ezMlYqP4rsYcpVU6laTtU8IpcGLj66SigR5a+JW9bAU3res5GL+3yXZcnCY7cSzGVG134A7FkFqaVsRjiU06e9UihOGiK6C+t54GrA4ITdLnMFo1RvReBArJFYjPhj5oXuvITyfhlkdKc4nH36eT55766GHpzjHn+l+T6/MwPiwr+kxqDI72O8IgxjBUyTCToQQm9Av5FE4syBSWcCGqJAGOjMe4aNIakMh5E/f5FTyJD7GJJANiKINDe9rjDFcgF7bwJQmeLCOY3DlseMXCfDegZtYupK1Jm948xza7GkUZzp5BxdjqsRdvbD8UeBcCIg5cxbjZAV2bRl97lrNhHGZw7wWxlOek3nhiqDdkyuz2UpsEKJ3wCaDXdH7EHoSirsLP5dyAezmVh+N8lcVpvqrNZmnVemF31X+R++BWmLyaS8vDyZWQ4SKKEiY171zvP8rZvO5eNNVwmhzLlyDiJJrB1TYdwt7L4utcJWFjTirdw96Ju+I7vjgmBetQiIWBqfvHRuy7o3z7jGHY1C6nW3/STJK3y0Q4WKbYi9ehoq0B1ii2NdMMyxrTgQ='}], provider_specific_fields=None))], usage=Usage(completion_tokens=163, prompt_tokens=4, total_tokens=167, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=120, rejected_prediction_tokens=None, text_tokens=43, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gemini/gemini-2.5-pro:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hey there! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=1051, prompt_tokens=4, total_tokens=1055, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=1041, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hey there! How can I help you today?', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=1051, prompt_tokens=4, total_tokens=1055, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=1041, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gemini/gemini-2.5-flash:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hey there! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=896, prompt_tokens=4, total_tokens=900, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=886, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hey there! How can I help you today?', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=896, prompt_tokens=4, total_tokens=900, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=886, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**claude-sonnet-4-5:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=12, prompt_tokens=10, total_tokens=22, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I help you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=12, prompt_tokens=10, total_tokens=22, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**openai/gpt-4.1:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hey! How can I help you today? ðŸ˜Š\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=10, prompt_tokens=10, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_433e8c8649', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hey! How can I help you today? ðŸ˜Š', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=10, prompt_tokens=10, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ms = [\"gemini/gemini-3-pro-preview\", \"gemini/gemini-2.5-pro\", \"gemini/gemini-2.5-flash\", \"claude-sonnet-4-5\", \"openai/gpt-4.1\"]\n",
    "msg = [{'role':'user','content':'Hey there!', 'cache_control': {'type': 'ephemeral'}}]\n",
    "for m in ms:\n",
    "    display(Markdown(f'**{m}:**'))\n",
    "    display(completion(m,msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715fff5a",
   "metadata": {},
   "source": [
    "## Messages formatting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5671bcb0",
   "metadata": {},
   "source": [
    "Let's start with making it easier to pass messages into litellm's `completion` function (including images, and pdf files)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c8b8f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "#| export\n",
    "def _bytes2content(data):\n",
    "    \"Convert bytes to litellm content dict (image, pdf, audio, video)\"\n",
    "    mtype = detect_mime(data)\n",
    "    if not mtype: raise ValueError(f'Data must be a supported file type, got {data[:10]}')\n",
    "    encoded = base64.b64encode(data).decode(\"utf-8\")    \n",
    "    if mtype.startswith('image/'): return {'type': 'image_url', 'image_url': f'data:{mtype};base64,{encoded}'}\n",
    "    return {'type': 'file', 'file': {'file_data': f'data:{mtype};base64,{encoded}'}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef65f38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _add_cache_control(msg,          # LiteLLM formatted msg\n",
    "                       ttl=None):    # Cache TTL: '5m' (default) or '1h'\n",
    "    \"cache `msg` with default time-to-live (ttl) of 5minutes ('5m'), but can be set to '1h'.\"\n",
    "    if isinstance(msg[\"content\"], str): \n",
    "        msg[\"content\"] = [{\"type\": \"text\", \"text\": msg[\"content\"]}]\n",
    "    cache_control = {\"type\": \"ephemeral\"}\n",
    "    if ttl is not None: cache_control[\"ttl\"] = ttl\n",
    "    if isinstance(msg[\"content\"], list) and msg[\"content\"]:\n",
    "        msg[\"content\"][-1][\"cache_control\"] = cache_control\n",
    "    return msg\n",
    "\n",
    "def _has_cache(msg):\n",
    "    return msg[\"content\"] and isinstance(msg[\"content\"], list) and ('cache_control' in msg[\"content\"][-1])\n",
    "\n",
    "def remove_cache_ckpts(msg):\n",
    "    \"remove cache checkpoints and return msg.\"\n",
    "    if _has_cache(msg): msg[\"content\"][-1].pop('cache_control', None)\n",
    "    return msg\n",
    "\n",
    "def _mk_content(o):\n",
    "    if isinstance(o, str): return {'type':'text','text':o.strip() or '.'}\n",
    "    elif isinstance(o,bytes): return _bytes2content(o)\n",
    "    return o\n",
    "\n",
    "def contents(r):\n",
    "    \"Get message object from response `r`.\"\n",
    "    return r.choices[0].message"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb67a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_msg(\n",
    "    content,      # Content: str, bytes (image), list of mixed content, or dict w 'role' and 'content' fields\n",
    "    role=\"user\",  # Message role if content isn't already a dict/Message\n",
    "    cache=False,  # Enable Anthropic caching\n",
    "    ttl=None      # Cache TTL: '5m' (default) or '1h'\n",
    "):\n",
    "    \"Create a LiteLLM compatible message.\"\n",
    "    if isinstance(content, dict) or isinstance(content, Message): return content\n",
    "    if isinstance(content, ModelResponse): return contents(content)\n",
    "    if isinstance(content, list) and len(content) == 1 and isinstance(content[0], str): c = content[0]\n",
    "    elif isinstance(content, list): c = [_mk_content(o) for o in content]\n",
    "    else: c = content\n",
    "    msg = {\"role\": role, \"content\": c}\n",
    "    return _add_cache_control(msg, ttl=ttl) if cache else msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da39f96f",
   "metadata": {},
   "source": [
    "Now we can use mk_msg to create different types of messages.\n",
    "\n",
    "Simple text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6217f4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'role': 'user', 'content': 'hey'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = mk_msg(\"hey\")\n",
    "msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a922030e",
   "metadata": {},
   "source": [
    "Which can be passed to litellm's `completion` function like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb1295ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ms[1] # use 2.5-pro, 3-pro is very slow even to run tests as of making"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a28656f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hey there! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=767, prompt_tokens=2, total_tokens=769, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=757, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=2, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hey there! How can I help you today?', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=767, prompt_tokens=2, total_tokens=769, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=757, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=2, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = completion(model, [msg])\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68485b58",
   "metadata": {},
   "source": [
    "We'll add a little shortcut to make examples and testing easier here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d8df32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(msgs, m=model, **kw):\n",
    "    msgs = [msgs] if isinstance(msgs,dict) else listify(msgs)\n",
    "    return completion(m, msgs, **kw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f0186ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hey there! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=767, prompt_tokens=2, total_tokens=769, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=757, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=2, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hey there! How can I help you today?', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=767, prompt_tokens=2, total_tokens=769, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=757, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=2, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "552e1298",
   "metadata": {},
   "source": [
    "Lists w just one string element are flattened for conciseness:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1a0955a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(mk_msg(\"hey\"), mk_msg([\"hey\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "001e399e",
   "metadata": {},
   "source": [
    "(LiteLLM ignores these fields when sent to other providers)\n",
    "\n",
    "Text and images:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923e6c93",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/jpeg": "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",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": null,
     "metadata": {
      "image/jpeg": {
       "width": 200
      }
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_fn = Path('samples/puppy.jpg')\n",
    "Image(filename=img_fn, width=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb0ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      " \"role\": \"user\",\n",
      " \"content\": [\n",
      "  {\n",
      "   \"type\": \"text\",\n",
      "   \"text\": \"hey what in this image?\"\n",
      "  },\n",
      "  {\n",
      "   \"type\": \"image_url\",\n",
      "   \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gxUSU...\n"
     ]
    }
   ],
   "source": [
    "msg = mk_msg(['hey what in this image?',img_fn.read_bytes()])\n",
    "print(json.dumps(msg,indent=1)[:200]+\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9ebb192",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This is an absolutely adorable picture of a puppy!\n",
       "\n",
       "Here's a breakdown of what's in the image:\n",
       "\n",
       "*   **The Puppy:** The main subject is a very young puppy, most likely a **Cavalier King Charles Spaniel**. It has the breed's characteristic features: large, dark, expressive eyes, long, floppy ears with silky, wavy fur, and a sweet expression. The coloring, white with chestnut or reddish-brown patches, is known as \"Blenheim\" in this breed.\n",
       "*   **The Pose:** The puppy is lying down in the green grass, peeking out from behind a bush of flowers. It's looking directly at the camera with a curious and gentle gaze.\n",
       "*   **The Flowers:** To the left of the puppy is a cluster of small, delicate purple or lavender-colored flowers, which look like asters or a similar daisy-like flower.\n",
       "*   **The Setting:** The scene is outdoors, likely in a garden or yard. The focus is sharp on the puppy, while the background is softly blurred, which makes the puppy stand out as the main subject.\n",
       "\n",
       "Overall, it's a very charming and heartwarming photograph capturing the innocence and cuteness of a young puppy.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=1332, prompt_tokens=265, total_tokens=1597, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=1075, rejected_prediction_tokens=None, text_tokens=257, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=7, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='This is an absolutely adorable picture of a puppy!\\n\\nHere\\'s a breakdown of what\\'s in the image:\\n\\n*   **The Puppy:** The main subject is a very young puppy, most likely a **Cavalier King Charles Spaniel**. It has the breed\\'s characteristic features: large, dark, expressive eyes, long, floppy ears with silky, wavy fur, and a sweet expression. The coloring, white with chestnut or reddish-brown patches, is known as \"Blenheim\" in this breed.\\n*   **The Pose:** The puppy is lying down in the green grass, peeking out from behind a bush of flowers. It\\'s looking directly at the camera with a curious and gentle gaze.\\n*   **The Flowers:** To the left of the puppy is a cluster of small, delicate purple or lavender-colored flowers, which look like asters or a similar daisy-like flower.\\n*   **The Setting:** The scene is outdoors, likely in a garden or yard. The focus is sharp on the puppy, while the background is softly blurred, which makes the puppy stand out as the main subject.\\n\\nOverall, it\\'s a very charming and heartwarming photograph capturing the innocence and cuteness of a young puppy.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=1332, prompt_tokens=265, total_tokens=1597, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=1075, rejected_prediction_tokens=None, text_tokens=257, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=7, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f958cae",
   "metadata": {},
   "source": [
    "Let's also demonstrate this for PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8160351e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO this gave me an error (jph Nov 30)\n",
    "# pdf_fn = Path('samples/solveit.pdf')\n",
    "# msg = mk_msg(['Who is the author of this pdf?', pdf_fn.read_bytes()])\n",
    "# c(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0e5e1e2",
   "metadata": {},
   "source": [
    "Some models like Gemini support audio and video:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38313733",
   "metadata": {},
   "outputs": [],
   "source": [
    "wav_data = httpx.get(\"https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav\").content\n",
    "# Audio(wav_data)  # uncomment to preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75a81fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The audio says: \"The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.\"\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=351, prompt_tokens=230, total_tokens=581, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=321, rejected_prediction_tokens=None, text_tokens=30, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=223, cached_tokens=None, text_tokens=7, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The audio says: \"The sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.\"', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=351, prompt_tokens=230, total_tokens=581, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=321, rejected_prediction_tokens=None, text_tokens=30, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=223, cached_tokens=None, text_tokens=7, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = mk_msg(['What is this audio saying?', wav_data])\n",
    "completion(ms[1], [msg])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b32b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "vid_data = httpx.get(\"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\").content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e904bf9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This video is an advertisement for the Google Pixel 8 Pro smartphone, featuring a photographer named Saeka Shimada. She walks through Tokyo at night, demonstrating the phone's new \"Video Boost\" feature, which uses \"Night Sight\" to capture high-quality, vibrant video in low-light conditions. She is visibly impressed by the clarity and beauty of the footage she records in the city's atmospheric alleys.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=402, prompt_tokens=17402, total_tokens=17804, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=318, rejected_prediction_tokens=None, text_tokens=84, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=1873, cached_tokens=None, text_tokens=12, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='This video is an advertisement for the Google Pixel 8 Pro smartphone, featuring a photographer named Saeka Shimada. She walks through Tokyo at night, demonstrating the phone\\'s new \"Video Boost\" feature, which uses \"Night Sight\" to capture high-quality, vibrant video in low-light conditions. She is visibly impressed by the clarity and beauty of the footage she records in the city\\'s atmospheric alleys.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=402, prompt_tokens=17402, total_tokens=17804, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=318, rejected_prediction_tokens=None, text_tokens=84, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=1873, cached_tokens=None, text_tokens=12, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msg = mk_msg(['Concisely, what is happening in this video?', vid_data])\n",
    "completion(ms[1], [msg])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc01faf",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23176bd8",
   "metadata": {},
   "source": [
    "Some providers such as Anthropic require manually opting into caching. Let's try it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e42ad26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cpr(i): return f'{i} '*1024 + 'This is a caching test. Report back only what number you see repeated above.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b7f481",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| eval: false\n",
    "disable_cachy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84103a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# msg = mk_msg(cpr(1), cache=True)\n",
    "# res = c(msg, ms[2])\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e39380",
   "metadata": {},
   "source": [
    "Anthropic has a maximum of 4 cache checkpoints, so we remove previous ones as we go:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ab7b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res = c([remove_cache_ckpts(msg), mk_msg(res), mk_msg(cpr(2), cache=True)], ms[2])\n",
    "# res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aaf17a3",
   "metadata": {},
   "source": [
    "We see that the first message was cached, and this extra message has been written to cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c7e395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# res.usage.prompt_tokens_details"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f286d9dd",
   "metadata": {},
   "source": [
    "We can add a bunch of large messages in a loop to see how the number of cached tokens used grows.\n",
    "\n",
    "We do this for 25 times to ensure it still works for more than >20 content blocks, [which is a known anthropic issue](https://docs.claude.com/en/docs/build-with-claude/prompt-caching).\n",
    "\n",
    "The code below is commented by default, because it's slow. Please uncomment when working on caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf0f771",
   "metadata": {},
   "outputs": [],
   "source": [
    "# h = []\n",
    "# msg = mk_msg(cpr(1), cache=True)\n",
    "\n",
    "# for o in range(2,25):\n",
    "#     h += [remove_cache_ckpts(msg), mk_msg(res)]\n",
    "#     msg = mk_msg(cpr(o), cache=True)\n",
    "#     res = c(h+[msg])\n",
    "#     detls = res.usage.prompt_tokens_details\n",
    "#     print(o, detls.cached_tokens, detls.cache_creation_tokens, end='; ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197632a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef5e9f01",
   "metadata": {},
   "source": [
    "### Reconstructing formatted outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80633d31",
   "metadata": {},
   "source": [
    "Lisette can call multiple tools in a loop. Further down this notebook, we'll provide convenience functions for formatting such a sequence of toolcalls and responses into one formatted output string.\n",
    "\n",
    "For now, we'll show an example and show how to transform such a formatted output string back into a valid LiteLLM history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ee01a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "fmt_outp = '''\n",
    "I'll solve this step-by-step, using parallel calls where possible.\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\n",
    "  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\n",
    "  \"result\": \"15\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\n",
    "  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\n",
    "  \"result\": \"3\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "\n",
    "Now I need to multiply 15 * 3 before I can do the final division:\n",
    "\n",
    "<details class='tool-usage-details'>\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\n",
    "  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\n",
    "  \"result\": \"45\"\n",
    "}\n",
    "```\n",
    "\n",
    "</details>\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8886f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "detls_tag = \"<details class='tool-usage-details'>\"\n",
    "re_tools = re.compile(fr\"^({detls_tag}\\n+```json\\n+(.*?)\\n+```\\n+</details>)\", flags=re.DOTALL|re.MULTILINE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ec2686",
   "metadata": {},
   "source": [
    "We can split into chunks of (text,toolstr,json):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a3f160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-  [\"\\nI'll solve this step-by-step, using parallel calls where possible.\\n\\n\", '<details class=\\'tool-usage-details\\'>\\n\\n```json\\n{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}\\n```\\n\\n</details>', '{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}']\n",
      "-  ['\\n\\n', '<details class=\\'tool-usage-details\\'>\\n\\n```json\\n{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}\\n```\\n\\n</details>', '{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}']\n",
      "-  ['\\n\\nNow I need to multiply 15 * 3 before I can do the final division:\\n\\n', '<details class=\\'tool-usage-details\\'>\\n\\n```json\\n{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}\\n```\\n\\n</details>', '{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}']\n",
      "-  ['\\n', None, None]\n"
     ]
    }
   ],
   "source": [
    "sp = re_tools.split(fmt_outp)\n",
    "for o in list(chunked(sp, 3, pad=True)): print('- ', o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303951a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _extract_tool(text:str)->tuple[dict,dict]:\n",
    "    \"Extract tool call and results from <details> block\"\n",
    "    d = json.loads(text.strip())\n",
    "    call = d['call']\n",
    "    func = call['function']\n",
    "    tc = ChatCompletionMessageToolCall(Function(dumps(call['arguments']),func), d['id'])\n",
    "    tr = {'role': 'tool','tool_call_id': d['id'],'name': func, 'content': d['result']}\n",
    "    return tc,tr\n",
    "\n",
    "def fmt2hist(outp:str)->list:\n",
    "    \"Transform a formatted output into a LiteLLM compatible history\"\n",
    "    lm,hist = Message(),[]\n",
    "    spt = re_tools.split(outp)\n",
    "    for txt,_,tooljson in chunked(spt, 3, pad=True):\n",
    "        txt = txt.strip() if tooljson or txt.strip() else '.'\n",
    "        hist.append(lm:=Message(txt))\n",
    "        if tooljson:\n",
    "            tcr = _extract_tool(tooljson)\n",
    "            if not hist: hist.append(lm) # if LLM calls a tool without talking\n",
    "            lm.tool_calls = lm.tool_calls+[tcr[0]] if lm.tool_calls else [tcr[0]] \n",
    "            hist.append(tcr[1])\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad493d98",
   "metadata": {},
   "source": [
    "See how we can turn that one formatted output string back into a list of Messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac22b2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b40b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":10,\"b\":5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n",
      " {'content': '15',\n",
      "  'name': 'simple_add',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta'},\n",
      " Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":2,\"b\":1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n",
      " {'content': '3',\n",
      "  'name': 'simple_add',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY'},\n",
      " Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":15,\"b\":3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function')], function_call=None, provider_specific_fields=None),\n",
      " {'content': '45',\n",
      "  'name': 'multiply',\n",
      "  'role': 'tool',\n",
      "  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C'},\n",
      " Message(content='.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n"
     ]
    }
   ],
   "source": [
    "h = fmt2hist(fmt_outp)\n",
    "pprint(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5249772a",
   "metadata": {},
   "source": [
    "### `mk_msgs`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375774e",
   "metadata": {},
   "source": [
    "We will skip tool use blocks and tool results during caching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02cb84da",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_cache_idxs(msgs, cache_idxs=[-1], ttl=None):\n",
    "    'Add cache control to idxs after filtering tools'\n",
    "    ms = L(msgs).filter(lambda m: not (m.get('tool_calls', []) or m['role'] == 'tool'))\n",
    "    for i in cache_idxs:\n",
    "        try: _add_cache_control(ms[i], ttl)\n",
    "        except IndexError: continue"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ad6deec",
   "metadata": {},
   "source": [
    "Now lets make it easy to provide entire conversations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b326d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_msgs(\n",
    "    msgs,                   # List of messages (each: str, bytes, list, or dict w 'role' and 'content' fields)\n",
    "    cache=False,            # Enable Anthropic caching\n",
    "    cache_idxs=[-1],        # Cache breakpoint idxs\n",
    "    ttl=None,               # Cache TTL: '5m' (default) or '1h'\n",
    "):\n",
    "    \"Create a list of LiteLLM compatible messages.\"\n",
    "    if not msgs: return []\n",
    "    if not isinstance(msgs, list): msgs = [msgs]\n",
    "    res,role = [],'user'\n",
    "    msgs = L(msgs).map(lambda m: fmt2hist(m) if detls_tag in m else [m]).concat()\n",
    "    for m in msgs:\n",
    "        res.append(msg:=remove_cache_ckpts(mk_msg(m, role=role)))\n",
    "        role = 'assistant' if msg['role'] in ('user','function', 'tool') else 'user'\n",
    "    if cache: _apply_cache_idxs(res, cache_idxs, ttl)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4624e2",
   "metadata": {},
   "source": [
    "With `mk_msgs` you can easily provide a whole conversation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263ecc52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hey!'},\n",
       " {'role': 'assistant', 'content': 'Hi there!'},\n",
       " {'role': 'user', 'content': 'How are you?'},\n",
       " {'role': 'assistant', 'content': \"I'm doing fine and you?\"}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38bf277c",
   "metadata": {},
   "source": [
    "By defualt the last message will be cached when `cache=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5a34c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hey!'},\n",
       " {'role': 'assistant', 'content': 'Hi there!'},\n",
       " {'role': 'user', 'content': 'How are you?'},\n",
       " {'role': 'assistant',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': \"I'm doing fine and you?\",\n",
       "    'cache_control': {'type': 'ephemeral'}}]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"], cache=True)\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc360a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('cache_control' in msgs[-1]['content'][0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ba87c3",
   "metadata": {},
   "source": [
    "Alternatively, users can provide custom `cache_idxs`. Tool call blocks and results are skipped during caching:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289d855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'Hello!',\n",
       "    'cache_control': {'type': 'ephemeral'}}]},\n",
       " {'role': 'assistant', 'content': 'Hi! How can I help you?'},\n",
       " {'role': 'user',\n",
       "  'content': [{'type': 'text',\n",
       "    'text': 'Call some functions!',\n",
       "    'cache_control': {'type': 'ephemeral'}}]},\n",
       " Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":10,\"b\":5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta',\n",
       "  'name': 'simple_add',\n",
       "  'content': '15'},\n",
       " Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":2,\"b\":1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY',\n",
       "  'name': 'simple_add',\n",
       "  'content': '3'},\n",
       " Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":15,\"b\":3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function')], function_call=None, provider_specific_fields=None),\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C',\n",
       "  'name': 'multiply',\n",
       "  'content': '45'},\n",
       " Message(content=[{'type': 'text', 'text': '.', 'cache_control': {'type': 'ephemeral'}}], role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(['Hello!','Hi! How can I help you?','Call some functions!',fmt_outp], cache=True, cache_idxs=[0,-2,-1])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f38eeaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq('cache_control' in msgs[0]['content'][0], True)\n",
    "test_eq('cache_control' in msgs[2]['content'][0], True) # shifted idxs to skip tools\n",
    "test_eq('cache_control' in msgs[-1]['content'][0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910677d4",
   "metadata": {},
   "source": [
    "Who's speaking at when is automatically inferred.\n",
    "Even when there are multiple tools being called in parallel (which LiteLLM supports!)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf8d1f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Tell me the weather in Paris and Rome'},\n",
       " {'role': 'assistant', 'content': 'Assistant calls weather tool two times'},\n",
       " {'role': 'tool', 'content': 'Weather in Paris is ...'},\n",
       " {'role': 'tool', 'content': 'Weather in Rome is ...'},\n",
       " {'role': 'assistant', 'content': 'Assistant returns weather'},\n",
       " {'role': 'user', 'content': 'Thanks!'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(['Tell me the weather in Paris and Rome',\n",
    "                'Assistant calls weather tool two times',\n",
    "                {'role':'tool','content':'Weather in Paris is ...'},\n",
    "                {'role':'tool','content':'Weather in Rome is ...'},\n",
    "                'Assistant returns weather',\n",
    "                'Thanks!'])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3bf25",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq([m['role'] for m in msgs],['user','assistant','tool','tool','assistant','user'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "475862e9",
   "metadata": {},
   "source": [
    "For ease of use, if `msgs` is not already in a `list`, it will automatically be wrapped inside one. This way you can pass a single prompt into `mk_msgs` and get back a LiteLLM compatible msg history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa4e7663",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hey'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(\"Hey\")\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea8c2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'tool', 'content': 'fake tool result'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#| hide\n",
    "msgs = mk_msgs({'role':'tool','content':'fake tool result'})\n",
    "msgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c828056",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hey!'},\n",
       " {'role': 'assistant', 'content': 'Hi there!'},\n",
       " {'role': 'user', 'content': 'How are you?'},\n",
       " {'role': 'assistant', 'content': \"I'm fine, you?\"}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "msgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm fine, you?\"])\n",
    "msgs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284fbfc3",
   "metadata": {},
   "source": [
    "However, beware that if you use `mk_msgs` for a single message, consisting of multiple parts.\n",
    "Then you should be explicit, and make sure to wrap those multiple messages in two lists:\n",
    "\n",
    "1. One list to show that they belong together in one message (the inner list).\n",
    "2. Another, because mk_msgs expects a list of multiple messages (the outer list).\n",
    "\n",
    "This is common when working with images for example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d315d964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      " {\n",
      "  \"role\": \"user\",\n",
      "  \"content\": [\n",
      "   {\n",
      "    \"type\": \"text\",\n",
      "    \"text\": \"Whats in this img?\"\n",
      "   },\n",
      "   {\n",
      "    \"type\": \"image_url\",\n",
      "    \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...\n"
     ]
    }
   ],
   "source": [
    "msgs = mk_msgs([['Whats in this img?',img_fn.read_bytes()]])\n",
    "print(json.dumps(msgs,indent=1)[:200]+\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad470e4",
   "metadata": {},
   "source": [
    "## Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec3dc5d",
   "metadata": {},
   "source": [
    "LiteLLM supports streaming responses. That's really useful if you want to show intermediate results, instead of having to wait until the whole response is finished.\n",
    "\n",
    "We create this helper function that returns the entire response at the end of the stream. This is useful when you want to store the whole response somewhere after having displayed the intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stream_with_complete(gen, postproc=noop):\n",
    "    \"Extend streaming response chunks with the complete response\"\n",
    "    chunks = []\n",
    "    for chunk in gen:\n",
    "        chunks.append(chunk)\n",
    "        yield chunk\n",
    "    postproc(chunks)\n",
    "    return stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f16571",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = c(mk_msgs(\"Hey!\"), stream=True)\n",
    "r2 = SaveReturn(stream_with_complete(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hey there! How can I help you today?"
     ]
    }
   ],
   "source": [
    "for o in r2:\n",
    "    cts = o.choices[0].delta.content\n",
    "    if cts: print(cts, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ec073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hey there! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=818, prompt_tokens=3, total_tokens=821, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hey there! How can I help you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=818, prompt_tokens=3, total_tokens=821, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e11c0",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4301402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def lite_mk_func(f):\n",
    "    if isinstance(f, dict): return f\n",
    "    return {'type':'function', 'function':get_schema(f, pname='parameters')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b103600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_add(\n",
    "    a: int,   # first operand\n",
    "    b: int=0  # second operand\n",
    ") -> int:\n",
    "    \"Add two numbers together\"\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fc27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'simple_add',\n",
       "  'description': 'Add two numbers together\\n\\nReturns:\\n- type: integer',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'a': {'type': 'integer', 'description': 'first operand'},\n",
       "    'b': {'type': 'integer', 'description': 'second operand', 'default': 0}},\n",
       "   'required': ['a']}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolsc = lite_mk_func(simple_add)\n",
    "toolsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2af29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmsg = mk_msg(\"What is 5478954793+547982745? How about 5479749754+9875438979? Always use tools for calculations, and describe what you'll do before using a tool. Where multiple tool calls are required, do them in a single response where possible. \")\n",
    "r = c(tmsg, tools=[toolsc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb1817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I will use the `simple_add` tool to perform the two requested additions. First, I'll add 5478954793 and 547982745. Then, I'll add 5479749754 and 9875438979.\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 5478954793, \"b\": 547982745})\n",
       "\n",
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"b\": 9875438979, \"a\": 5479749754})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=674, prompt_tokens=149, total_tokens=823, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=523, rejected_prediction_tokens=None, text_tokens=151, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=149, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I will use the `simple_add` tool to perform the two requested additions. First, I'll add 5478954793 and 547982745. Then, I'll add 5479749754 and 9875438979.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"a\": 5478954793, \"b\": 547982745}', name='simple_add'), id='call_4ed59558dd1f4136881e73efed47', type='function'), ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"b\": 9875438979, \"a\": 5479749754}', name='simple_add'), id='call_422313a0783844a1b2779065365d', type='function')], function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"text\": \"I will use the `simple_add` tool to perform the two requested additions. First, I\\'ll add 5478954793 and 547982745. Then, I\\'ll add 5479749754 and 9875438979.\"}', 'signature': 'CsYNAXLI2nxQDg2H7IQFKRfAB++XIjr6+QF5y7Bm79+vB52Ig/hIU2JzHavfsUJq/hLHh24tKaj4NtwCC5IEytQW4hWc53exlWACJevkSQGxCyUIefD4KF6hhSLXcltG7PzHdLUdTAfX6Vc2ocluBKEAGIpzhKL13gwHACM7GPldMgA3YYuFJ88qOIEk9TVE4Pzt4DI6/PT3Ztm18dNA6lF8Px28dfB6N1EVmWtWfcKN69tiOK/aEbrg+1yvzqugWCOS8/X+/x/IToz5hHlNpG3t2MYec82v/5wmFFicYc6+AtzjTFIcXKgGXMBY9RtLpYdn0yRPseNbJoi9zbHAfYIB/uJKSfU3xs9jSLOTLwIcj4BPJxo59t6JKEnwqKA2f/ZCkqu5c9L09uRfJ6CV/5qRKhqDQ502zqkC0VlYvTH6dFWBaOz0xiUSd6G2BpBBaYaCQY4ieI4l7U7N6zv1K8RW248A4xyN4oZXOjtFplcvxLaqdXkc1qFYDVnM9dsk1QGOmXQg6uPJkZQplSIF5LaG8N5FdEf2qPyGrYgqreeyxlU0oJit43HP70FS44q9it92ugCtUtgOcAVK86KlAe8wRlvbcUq8Fq+W/2fMTF1btc1oXHYaoMS8uwZ8YBPKmkQ9v/bCm2CQNp9Hqmbcs889Q9CIoL4KpDf1O2ULHHUKjAayb89/xV6lYD0qZv8+5asM0pGrod/FUsZJI6UKdsV/zIqbbJ9t5TxHR8LubJqZuo2l5q4RrHWICRMXTcGPPftFw/GK6cyfD4j0bXmXMIqx17OxIrBBIYAfMatGNIN9Sk5fzjkg+ah9BfaSdXxft6qWE9t1ETips3N1eFiVb9W1c3N6CUZFzRGtWo/X2LWi4/ZEwnIUpPN5jPMwlLGvcrnlPJZXivYbnOfI76iE9A4kunfQV6CM+QPofhwO033js+jvpbab1hb3a9QeZRPm2h8PQAiGRfhHzTkDnpUUOYu7AUg/vYWY75dUbD4v2srWNYXdqJ1JChUYV5Wf40L05GPLsUqyIXYJsGfLcICbK3YEla/5laSmV3aO7b8KNMLgZ/oatCnFd9p3kWyC/dEAgjIe2WzWSAXt/BIRYF7z6hMHltp3q6KQH3ov3/tTmQ7xltlt4cJANTzu8puzkzRYZU6tLYe9bej/Y2R7Yd1jLscQ6bRhXSIYo4M4+q7XRPZGHPa2eSCgVC9+ZiWVGKEFcd0dAwtLNA99FFTvtqtgsJgcgMjZXvxh5ewbu9jl3JHGB1IAayj6D6E8MPd38+5LVsWCSF8R4+urLiK62yru2RPL/NKcpb4FCXErGoPx9ywJXR0x3SvW5HXg0sNPhuUCrFpxmcCq/KJzWydzXnhCR37W6HgPzGvD3mMlwDHk59LFPQZ4q253tu5nOu+afphP6FlPwL7d8lrNT67Q4i2FMTk9X13IHJED4t6qAcGH0pBhuuLK25EZVVnJowc3V3pebGtAg4tpIoUVJ100YJ2b0g9pZZ+V9fgWJsGoYCWrZw3ZmZv1CLAufnpJKnC6J+jxqCgRwjjeq8y3Rqi7iDKNOWqu0AmyT1gmHgvgxbrC/LSakUTQR8lwPQq8ab/LKPRM6EvyiuxrsssjONiXiglxbUUzODzZF5wHvqmZ12BbtrndAFmOsI+P8IooYzJXeOrpF54+A5+1blqOY8rcL3Ur9rJjwqNpieW7QVKYGmURCGMbYngJINfbXr9Qm1N5GLUmWNLSA5nPrFXnzHBxBF7hKGdRT993UQBaWtYf4letLeeTTd1bClcD//lIA9guzVx0hL5PAZZmgV51T6/iOVCSETD5WE7ljlw74haDch9l8ZtLFw1BS9xSqmG6pK5M62NHxcEQvGcXAZvEbXrfUTqj6QEpUJQnntoExDczr3pGdJqYpIC6wuvW2wHaFVIisnveA13u/WtZ1rFy8fDZjo2oNVtBvuCuRUblAo+BadCk8T0wh52ZykRs0EZhkvo82YQFjZmEvv8AYBSPmYz/69H1Cqbbtu3TSaCmIIeqxZF9abMATtc6MfNE1bTST0m4/C/x+IWJJSCRLCdcJSiz0++FvhWCLeNI5zTem85LNPx0cB+JlvdfsJBt1KAPBXDV6XJWZZkc7e0L+amCRFR9PBfkoMORDrfswlwOCWyHc7uKO3rU8E4hh1lgKl9rWJCMT7SY8QSlB88tt92sr/wKVaSkqvCqU0WtCqTLO33aQyWYyfKxdCp57NN87AVk2BGwapaFMd/gSirJAhNmZ/lcHG1RnN/c2/85+iu2shLmeeEFBOh4tDA6vjOBhH5npM8g6C3qz+r2AB+PVaku'}], provider_specific_fields=None))], usage=Usage(completion_tokens=674, prompt_tokens=149, total_tokens=823, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=523, rejected_prediction_tokens=None, text_tokens=151, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=149, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c274a8",
   "metadata": {},
   "source": [
    "A tool response can be a string or a list of tool blocks (e.g., an image url block). To allow users to specify if a response should not be immediately stringified, we provide the ToolResponse datatype users can wrap their return statement in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac2035b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@dataclass\n",
    "class ToolResponse:\n",
    "    content: list[str,str]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d81d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _lite_call_func(tc,ns,raise_on_err=True):\n",
    "    try: fargs = json.loads(tc.function.arguments)\n",
    "    except Exception as e: raise ValueError(f\"Failed to parse function arguments: {tc.function.arguments}\") from e\n",
    "    res = call_func(tc.function.name, fargs,ns=ns)\n",
    "    if isinstance(res, ToolResponse): res = res.content\n",
    "    else: res = str(res)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af607c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tool_call_id': 'call_4ed59558dd1f4136881e73efed47',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '6026937538'},\n",
       " {'tool_call_id': 'call_422313a0783844a1b2779065365d',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '15355188733'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcs = [_lite_call_func(o, ns=globals()) for o in r.choices[0].message.tool_calls]\n",
    "tcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd9871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_text(msg):\n",
    "    \"Extract printable content from streaming delta, return None if nothing to print\"\n",
    "    c = msg.choices[0]\n",
    "    if not c: return c\n",
    "    if not hasattr(c,'delta'): return None #f'{c}'\n",
    "    delta = c.delta\n",
    "    if delta.content: return delta.content\n",
    "    if delta.tool_calls:\n",
    "        res = ''.join(f\"ðŸ”§ {tc.function.name}\" for tc in delta.tool_calls if tc.id and tc.function.name)\n",
    "        if res: return f'\\n{res}\\n'\n",
    "    if hasattr(delta,'reasoning_content'): return 'ðŸ§ ' if delta.reasoning_content else '\\n\\n'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4790a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will add the two pairs of numbers for you. First, I'll add 5478954793 and 547982745. Then, I'll add 5479749754 and 9875438979.\n",
      "ðŸ”§ simple_add\n",
      "call:simple_add{a:5479749754,b:9875"
     ]
    }
   ],
   "source": [
    "r = c(tmsg, stream=True, tools=[toolsc])\n",
    "r2 = SaveReturn(stream_with_complete(r))\n",
    "for o in r2: print(delta_text(o) or '', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21118c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I will add the two pairs of numbers for you. First, I'll add 5478954793 and 547982745. Then, I'll add 5479749754 and 9875438979.call:simple_add{a:5479749754,b:9875\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 5478954793, \"b\": 547982745})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=288, prompt_tokens=149, total_tokens=437, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=25, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I will add the two pairs of numbers for you. First, I'll add 5478954793 and 547982745. Then, I'll add 5479749754 and 9875438979.call:simple_add{a:5479749754,b:9875\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5478954793, \"b\": 547982745}', name='simple_add'), id='call_2cba00ff880e4f148e314ecf0cba', type='function')], function_call=None, provider_specific_fields=None, reasoning_content='{\"text\": \"I will add the two pairs of numbers for you. First, I\\'ll add 547895479\"}'))], usage=Usage(completion_tokens=288, prompt_tokens=149, total_tokens=437, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=25, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bca992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§ ðŸ§ ðŸ§ ðŸ§ "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of course! While it might seem complex at first, this is a classic calculus problem that becomes quite simple once you know the rules.\n",
      "\n",
      "The derivative of **xÂ³ + 2xÂ² - 5x + 1** is:\n",
      "\n",
      "**3xÂ² + 4x - 5**\n",
      "\n",
      "---\n",
      "\n",
      "### Step-by-Step Solution:\n",
      "\n",
      "To solve this, we use a few fundamental rules of differentiation. The main idea is that we can take the derivative of each part of the expression (each term) separately and then add them together.\n",
      "\n",
      "The function is: `f(x) = xÂ³ + 2xÂ² - 5x + 1`\n",
      "\n",
      "Let's break it down term by term.\n",
      "\n",
      "#### 1. The Power Rule\n",
      "The most important rule we'll use is the **Power Rule**, which states:\n",
      "The derivative of `xâ¿` is `n * xâ¿â»Â¹`\n",
      "(In simple terms: bring the exponent down to the front as a multiplier, then subtract one from the original exponent).\n",
      "\n",
      "---\n",
      "\n",
      "**Term 1: `xÂ³`**\n",
      "*   Using the Power Rule, `n = 3`.\n",
      "*   Bring the `3` to the front and subtract 1 from the exponent.\n",
      "*   Derivative = `3 * xÂ³â»Â¹` = **3xÂ²**\n",
      "\n",
      "**Term 2: `2xÂ²`**\n",
      "*   We use the Power Rule on `xÂ²` and keep the constant `2` as a multiplier.\n",
      "*   The derivative of `xÂ²` is `2 * xÂ²â»Â¹` = `2x`.\n",
      "*   Now, multiply by the constant `2`: `2 * (2x)` = **4x**\n",
      "\n",
      "**Term 3: `-5x`**\n",
      "*   You can think of `x` as `xÂ¹`.\n",
      "*   Using the Power Rule, the derivative of `xÂ¹` is `1 * xÂ¹â»Â¹` = `1 * xâ°`.\n",
      "*   Anything to the power of 0 is 1, so the derivative is `1`.\n",
      "*   Now, multiply by the constant `-5`: `-5 * 1` = **-5**\n",
      "\n",
      "**Term 4: `+1`**\n",
      "*   There is a rule for constants: **The derivative of any constant number is always 0.**\n",
      "*   The derivative of `1` is **0**.\n",
      "\n",
      "---\n",
      "\n",
      "### Putting It All Together:\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now, we just combine the derivatives of each term:\n",
      "\n",
      "**3xÂ² + 4x - 5 + 0**\n",
      "\n",
      "Which simplifies to our final answer:\n",
      "\n",
      "### **3xÂ² + 4x - 5**"
     ]
    }
   ],
   "source": [
    "msg = mk_msg(\"Solve this complex math problem: What is the derivative of x^3 + 2x^2 - 5x + 1?\")\n",
    "r = c(msg, stream=True, reasoning_effort=\"low\")\n",
    "r2 = SaveReturn(stream_with_complete(r))\n",
    "for o in r2: print(delta_text(o) or '', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff5f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course! While it might seem complex at first, this is a classic calculus problem that becomes quite simple once you know the rules.\n",
       "\n",
       "The derivative of **xÂ³ + 2xÂ² - 5x + 1** is:\n",
       "\n",
       "**3xÂ² + 4x - 5**\n",
       "\n",
       "---\n",
       "\n",
       "### Step-by-Step Solution:\n",
       "\n",
       "To solve this, we use a few fundamental rules of differentiation. The main idea is that we can take the derivative of each part of the expression (each term) separately and then add them together.\n",
       "\n",
       "The function is: `f(x) = xÂ³ + 2xÂ² - 5x + 1`\n",
       "\n",
       "Let's break it down term by term.\n",
       "\n",
       "#### 1. The Power Rule\n",
       "The most important rule we'll use is the **Power Rule**, which states:\n",
       "The derivative of `xâ¿` is `n * xâ¿â»Â¹`\n",
       "(In simple terms: bring the exponent down to the front as a multiplier, then subtract one from the original exponent).\n",
       "\n",
       "---\n",
       "\n",
       "**Term 1: `xÂ³`**\n",
       "*   Using the Power Rule, `n = 3`.\n",
       "*   Bring the `3` to the front and subtract 1 from the exponent.\n",
       "*   Derivative = `3 * xÂ³â»Â¹` = **3xÂ²**\n",
       "\n",
       "**Term 2: `2xÂ²`**\n",
       "*   We use the Power Rule on `xÂ²` and keep the constant `2` as a multiplier.\n",
       "*   The derivative of `xÂ²` is `2 * xÂ²â»Â¹` = `2x`.\n",
       "*   Now, multiply by the constant `2`: `2 * (2x)` = **4x**\n",
       "\n",
       "**Term 3: `-5x`**\n",
       "*   You can think of `x` as `xÂ¹`.\n",
       "*   Using the Power Rule, the derivative of `xÂ¹` is `1 * xÂ¹â»Â¹` = `1 * xâ°`.\n",
       "*   Anything to the power of 0 is 1, so the derivative is `1`.\n",
       "*   Now, multiply by the constant `-5`: `-5 * 1` = **-5**\n",
       "\n",
       "**Term 4: `+1`**\n",
       "*   There is a rule for constants: **The derivative of any constant number is always 0.**\n",
       "*   The derivative of `1` is **0**.\n",
       "\n",
       "---\n",
       "\n",
       "### Putting It All Together:\n",
       "\n",
       "Now, we just combine the derivatives of each term:\n",
       "\n",
       "**3xÂ² + 4x - 5 + 0**\n",
       "\n",
       "Which simplifies to our final answer:\n",
       "\n",
       "### **3xÂ² + 4x - 5**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=1465, prompt_tokens=29, total_tokens=1494, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=333, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Of course! While it might seem complex at first, this is a classic calculus problem that becomes quite simple once you know the rules.\\n\\nThe derivative of **xÂ³ + 2xÂ² - 5x + 1** is:\\n\\n**3xÂ² + 4x - 5**\\n\\n---\\n\\n### Step-by-Step Solution:\\n\\nTo solve this, we use a few fundamental rules of differentiation. The main idea is that we can take the derivative of each part of the expression (each term) separately and then add them together.\\n\\nThe function is: `f(x) = xÂ³ + 2xÂ² - 5x + 1`\\n\\nLet's break it down term by term.\\n\\n#### 1. The Power Rule\\nThe most important rule we'll use is the **Power Rule**, which states:\\nThe derivative of `xâ¿` is `n * xâ¿â»Â¹`\\n(In simple terms: bring the exponent down to the front as a multiplier, then subtract one from the original exponent).\\n\\n---\\n\\n**Term 1: `xÂ³`**\\n*   Using the Power Rule, `n = 3`.\\n*   Bring the `3` to the front and subtract 1 from the exponent.\\n*   Derivative = `3 * xÂ³â»Â¹` = **3xÂ²**\\n\\n**Term 2: `2xÂ²`**\\n*   We use the Power Rule on `xÂ²` and keep the constant `2` as a multiplier.\\n*   The derivative of `xÂ²` is `2 * xÂ²â»Â¹` = `2x`.\\n*   Now, multiply by the constant `2`: `2 * (2x)` = **4x**\\n\\n**Term 3: `-5x`**\\n*   You can think of `x` as `xÂ¹`.\\n*   Using the Power Rule, the derivative of `xÂ¹` is `1 * xÂ¹â»Â¹` = `1 * xâ°`.\\n*   Anything to the power of 0 is 1, so the derivative is `1`.\\n*   Now, multiply by the constant `-5`: `-5 * 1` = **-5**\\n\\n**Term 4: `+1`**\\n*   There is a rule for constants: **The derivative of any constant number is always 0.**\\n*   The derivative of `1` is **0**.\\n\\n---\\n\\n### Putting It All Together:\\n\\nNow, we just combine the derivatives of each term:\\n\\n**3xÂ² + 4x - 5 + 0**\\n\\nWhich simplifies to our final answer:\\n\\n### **3xÂ² + 4x - 5**\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=\"**Initiating Derivative Exploration**\\n\\nI'm now fully immersed in the process of formulating a clear explanation of the derivative for the given polynomial. I've broken down the user's request and am carefully considering the most accessible way to present this information, ensuring it's comprehensive and understandable. The focus is on clarity and step-by-step breakdown.\\n\\n\\n**Decomposing the Polynomial**\\n\\nI've just finished the preliminary analysis of the problem. I'm focusing on dissecting the user's need for a comprehensive, step-by-step breakdown due to their perception of complexity. I've begun to list down the initial steps and have zeroed in on the polynomial expression. Now, I'm identifying the necessary calculus rules to accurately solve this derivative problem. I'm prioritizing the power and constant multiple rules as the key components for a concise and accessible derivation.\\n\\n\\n**Constructing the Solution**\\n\\nI've structured my approach to address the user's perception of complexity. I'll provide both the immediate answer and a detailed, encouraging explanation. The core of my explanation revolves around applying the power, constant multiple, sum/difference, and constant rules step-by-step. I will ensure each step is clear, concise, and easy to follow. I plan to use the original expression in the response to enhance clarity.\\n\\n\\n**Detailing the Power Rule**\\n\\nI've just focused on refining the explanation of the power rule, emphasizing how it simplifies finding derivatives of polynomial terms. The breakdown will directly address how to calculate the derivative for each term in the polynomial, ensuring an easily understandable and comprehensive tutorial. The structure is now designed to start with the fundamental rules and apply them step-by-step to the given problem.\\n\\n\\n\"))], usage=Usage(completion_tokens=1465, prompt_tokens=29, total_tokens=1494, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=333, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf97e55",
   "metadata": {},
   "source": [
    "## Structured Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4688cf77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@delegates(completion)\n",
    "def structured(\n",
    "    m:str,          # LiteLLM model string\n",
    "    msgs:list,      # List of messages \n",
    "    tool:Callable,  # Tool to be used for creating the structured output (class, dataclass or Pydantic, function, etc)\n",
    "    **kwargs):\n",
    "    \"Return the value of the tool call (generally used for structured outputs)\"\n",
    "    t = lite_mk_func(tool)\n",
    "    r = completion(m, msgs, tools=[t], tool_choice=t, **kwargs)\n",
    "    args = json.loads(r.choices[0].message.tool_calls[0].function.arguments)\n",
    "    return tool(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49621b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class President:\n",
    "    \"Information about a president of the United States\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        first:str, # first name\n",
    "        last:str, # last name\n",
    "        spouse:str, # name of spouse\n",
    "        years_in_office:str, # format: \"{start_year}-{end_year}\"\n",
    "        birthplace:str, # name of city\n",
    "        birth_year:int # year of birth, `0` if unknown\n",
    "    ):\n",
    "        assert re.match(r'\\d{4}-\\d{4}', years_in_office), \"Invalid format: `years_in_office`\"\n",
    "        store_attr()\n",
    "\n",
    "    __repr__ = basic_repr('first, last, spouse, years_in_office, birthplace, birth_year')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ecd07e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "President(first='Thomas', last='Jefferson', spouse='Martha Jefferson', years_in_office='1801-1809', birthplace='Shadwell', birth_year=1743)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structured(model, [mk_msg(\"Tell me something about the third president of the USA.\")], President)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d92cdb20",
   "metadata": {},
   "source": [
    "## Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a7128c",
   "metadata": {},
   "source": [
    "LiteLLM provides search, not via tools, but via the special `web_search_options` param.\n",
    "\n",
    "**Note:** Not all models support web search. LiteLLM's `supports_web_search` field should indicate this, but it's unreliable for some models like `claude-sonnet-4-20250514`. Checking both `supports_web_search` and `search_context_cost_per_query` provides more accurate detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530af43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _has_search(m):\n",
    "    i = get_model_info(m)\n",
    "    return bool(i.get('search_context_cost_per_query') or i.get('supports_web_search'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43417017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gemini/gemini-3-pro-preview True\n",
      "gemini/gemini-2.5-pro True\n",
      "gemini/gemini-2.5-flash True\n",
      "claude-sonnet-4-5 True\n",
      "openai/gpt-4.1 False\n"
     ]
    }
   ],
   "source": [
    "for m in ms: print(m, _has_search(m))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8611e2",
   "metadata": {},
   "source": [
    "When search is supported it can be used like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89e0bead",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Otters are carnivorous mammals known for their playful behavior and adaptations to a semi-aquatic life. There are 14 known species of otters, which are part of the weasel family.\n",
       "\n",
       "**Physical Characteristics:** Otters typically have long, slender bodies with short legs and powerful webbed feet perfect for swimming. They possess dense, waterproof fur that keeps them warm. Their size varies by species, ranging from about 2 to 6 feet in length and weighing between 6 and 100 pounds.\n",
       "\n",
       "**Habitat and Diet:** Most otters live in and around freshwater rivers, lakes, and wetlands, while two species are marine. Their diet is primarily carnivorous and consists of fish, crayfish, crabs, and other aquatic invertebrates. Some species are adept at using tools, such as rocks, to break open shellfish.\n",
       "\n",
       "**Behavior and Social Structure:** Otters are known for their playful nature, often seen sliding down riverbanks. Their social structure varies; some species are mostly solitary, while others live in groups. They communicate through a variety of sounds, including whistles and chirps. Otters can live up to 16 years in the wild.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=465, prompt_tokens=12, total_tokens=578, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=194, rejected_prediction_tokens=None, text_tokens=271, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=12, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Otters are carnivorous mammals known for their playful behavior and adaptations to a semi-aquatic life. There are 14 known species of otters, which are part of the weasel family.\\n\\n**Physical Characteristics:** Otters typically have long, slender bodies with short legs and powerful webbed feet perfect for swimming. They possess dense, waterproof fur that keeps them warm. Their size varies by species, ranging from about 2 to 6 feet in length and weighing between 6 and 100 pounds.\\n\\n**Habitat and Diet:** Most otters live in and around freshwater rivers, lakes, and wetlands, while two species are marine. Their diet is primarily carnivorous and consists of fish, crayfish, crabs, and other aquatic invertebrates. Some species are adept at using tools, such as rocks, to break open shellfish.\\n\\n**Behavior and Social Structure:** Otters are known for their playful nature, often seen sliding down riverbanks. Their social structure varies; some species are mostly solitary, while others live in groups. They communicate through a variety of sounds, including whistles and chirps. Otters can live up to 16 years in the wild.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None, annotations=[{'type': 'url_citation', 'url_citation': {'end_index': 178, 'start_index': 104, 'title': 'wikipedia.org', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2pn0bF_6oFB3sdMwWq6AhdM4zuO-Xps0_S1bLBeXmWli7HPTvNASqRFBdloU1Si-pU2Guj-4yGn8t2lY3znWVGeG1ZI8R93cajVmHVmeytR74QRLYVH77UwL_hiPz'}}, {'type': 'url_citation', 'url_citation': {'end_index': 315, 'start_index': 180, 'title': 'wikipedia.org', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2pn0bF_6oFB3sdMwWq6AhdM4zuO-Xps0_S1bLBeXmWli7HPTvNASqRFBdloU1Si-pU2Guj-4yGn8t2lY3znWVGeG1ZI8R93cajVmHVmeytR74QRLYVH77UwL_hiPz'}}, {'type': 'url_citation', 'url_citation': {'end_index': 372, 'start_index': 316, 'title': 'wikipedia.org', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2pn0bF_6oFB3sdMwWq6AhdM4zuO-Xps0_S1bLBeXmWli7HPTvNASqRFBdloU1Si-pU2Guj-4yGn8t2lY3znWVGeG1ZI8R93cajVmHVmeytR74QRLYVH77UwL_hiPz'}}, {'type': 'url_citation', 'url_citation': {'end_index': 482, 'start_index': 373, 'title': 'study.com', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFFQo0zU7gp9wrrxaxTWtv8TcphvYKvxSL0ejmYdfsEAIJ9Dmy5pQwTOxqQbqW-sLKrQoPE4T1yQ9hl4oYgD5pc__Fd-lWZn4bfLFUgMdnRXKNpoaO8ZymBoGLtzTOqJg5lVnwtVKvNTNqCTWwdCI_U5pMgerGQNZqV6MB6U3N8VLVeGho='}}, {'type': 'url_citation', 'url_citation': {'end_index': 606, 'start_index': 484, 'title': 'wikipedia.org', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2pn0bF_6oFB3sdMwWq6AhdM4zuO-Xps0_S1bLBeXmWli7HPTvNASqRFBdloU1Si-pU2Guj-4yGn8t2lY3znWVGeG1ZI8R93cajVmHVmeytR74QRLYVH77UwL_hiPz'}}, {'type': 'url_citation', 'url_citation': {'end_index': 714, 'start_index': 607, 'title': 'crittercontrol.com', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFQi65Rnx7jTizXrLDnx9N61o8IEohpWVC-j22j9M5V2gDyfSLclxAvBrmbohJuXvgMiCSSeGF19YqgezmNicOEyDGdXuSN7BtDvqCQ8MeYqQldnPzK_oNNW8ta0tnw2LQmf0brWZHmclXib9JVj7JN6faMg=='}}, {'type': 'url_citation', 'url_citation': {'end_index': 793, 'start_index': 715, 'title': 'wikipedia.org', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2pn0bF_6oFB3sdMwWq6AhdM4zuO-Xps0_S1bLBeXmWli7HPTvNASqRFBdloU1Si-pU2Guj-4yGn8t2lY3znWVGeG1ZI8R93cajVmHVmeytR74QRLYVH77UwL_hiPz'}}, {'type': 'url_citation', 'url_citation': {'end_index': 908, 'start_index': 795, 'title': 'wikipedia.org', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2pn0bF_6oFB3sdMwWq6AhdM4zuO-Xps0_S1bLBeXmWli7HPTvNASqRFBdloU1Si-pU2Guj-4yGn8t2lY3znWVGeG1ZI8R93cajVmHVmeytR74QRLYVH77UwL_hiPz'}}, {'type': 'url_citation', 'url_citation': {'end_index': 1002, 'start_index': 909, 'title': 'wikipedia.org', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2pn0bF_6oFB3sdMwWq6AhdM4zuO-Xps0_S1bLBeXmWli7HPTvNASqRFBdloU1Si-pU2Guj-4yGn8t2lY3znWVGeG1ZI8R93cajVmHVmeytR74QRLYVH77UwL_hiPz'}}, {'type': 'url_citation', 'url_citation': {'end_index': 1079, 'start_index': 1003, 'title': 'ukwildottertrust.org', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRF0VW6hoXzWmAVA2B0Jmxy9-NCSur9G5Hhq8Nw6374Hcs01Keya_v6SCRSw-eSXk74OlD-BD4tvVj1tPGJkGQeCQdvNdc3z51uUssEeYzJquEt4YbWGQEZZvNHjUjow7tpOg='}}, {'type': 'url_citation', 'url_citation': {'end_index': 1123, 'start_index': 1080, 'title': 'wikipedia.org', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2pn0bF_6oFB3sdMwWq6AhdM4zuO-Xps0_S1bLBeXmWli7HPTvNASqRFBdloU1Si-pU2Guj-4yGn8t2lY3znWVGeG1ZI8R93cajVmHVmeytR74QRLYVH77UwL_hiPz'}}]))], usage=Usage(completion_tokens=465, prompt_tokens=12, total_tokens=578, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=194, rejected_prediction_tokens=None, text_tokens=271, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=12, image_tokens=None)), vertex_ai_grounding_metadata=[{'searchEntryPoint': {'renderedContent': '<style>\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n</style>\\n<div class=\"container\">\\n  <div class=\"headline\">\\n    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\\n      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\\n      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\\n      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\\n      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\\n  </div>\\n  <div class=\"carousel\">\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFoqNcRYz4EnKTDr9rOFtM_QL8fkC9srSnO_pO3Kx3V-kztEHlBXjKTZmwvAT1QVim_wGNWj61kRC38vJmJaMPBQQi58FwA0X9f65W8veorMP0m1VIwW4WwQW9NZuipK4Q9zdSIUAlgOiQDjFUW_-PslRgwXxWrmGXu5jUIN2i6HP6W2G3x7EFJDV-AWX0jvo0Zmj942zxINEq0ZrAowWhQqAIvduvUhH2h\">what are the characteristics of otters</a>\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHr13jWszEdjCzQ_VmuUxmeWQAswGZE1Z1qXGQUJ3WJ2K2V0lptvAWJxT8BH01LDSKROIVEuOZCUaoTJgtiV2t4O23q3HmdUhHYTCu97daYMl_CbvDGZD0VddcXk9RqryyYYJPxc5Eec_OIihpsc9UxO3KhHGmkNU-ItKfxBzY4Og4v_frQLz7OR2yyrdFEy6ME\">otter overview</a>\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEGh6-Kc-LFdE22rpN0twxDOvbmj2XMMYRwxH1EB8ABOXwOlRtbf7NCPMNeUm0mCi8uNCaCpplqfL6UtWg6QZ7zGhT-8ZvbkQ2FTGn8Usty51WIUC8tQfXWCZ-rDlCDtEa6CNjjGn9meE104hS9wSfZqNYuovvp600RstKn62EgsFge0w9ofRvsjNEJ8dHuHd0IHdtSXA==\">what do otters eat</a>\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEzEOrUH6o3JZfbGF3n8ESXGUurDYTYi9utM22v1XvGww8ypUZiVJwqNDIHaiURuf2uaY_r4fekzjsCJ_swjBPoMRSH068D8jtgn-REwLOaC_agYjK758f6hQ9shaOzBwAbkM_YkaloHm0e9NurjADhUWnnJzyPpo-xY6lbtDA8yotgGEk7ltllXKIA8Vc3jd_d\">otter behavior</a>\\n  </div>\\n</div>\\n'}, 'groundingChunks': [{'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2pn0bF_6oFB3sdMwWq6AhdM4zuO-Xps0_S1bLBeXmWli7HPTvNASqRFBdloU1Si-pU2Guj-4yGn8t2lY3znWVGeG1ZI8R93cajVmHVmeytR74QRLYVH77UwL_hiPz', 'title': 'wikipedia.org'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEodF4gXW5x0gckfC-61dKg4HrDDzH4Gmg5CYOFmSPmJAXVDu9Im4Hr6kIQCPXhkHas81DGHb9zOGZib_HCmBoR2P0YX2848NHuivTqntd0FcuMOEBWEXvvztxrJNEfH9c2QQ==', 'title': 'britannica.com'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFFQo0zU7gp9wrrxaxTWtv8TcphvYKvxSL0ejmYdfsEAIJ9Dmy5pQwTOxqQbqW-sLKrQoPE4T1yQ9hl4oYgD5pc__Fd-lWZn4bfLFUgMdnRXKNpoaO8ZymBoGLtzTOqJg5lVnwtVKvNTNqCTWwdCI_U5pMgerGQNZqV6MB6U3N8VLVeGho=', 'title': 'study.com'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFRF0VW6hoXzWmAVA2B0Jmxy9-NCSur9G5Hhq8Nw6374Hcs01Keya_v6SCRSw-eSXk74OlD-BD4tvVj1tPGJkGQeCQdvNdc3z51uUssEeYzJquEt4YbWGQEZZvNHjUjow7tpOg=', 'title': 'ukwildottertrust.org'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHFQi65Rnx7jTizXrLDnx9N61o8IEohpWVC-j22j9M5V2gDyfSLclxAvBrmbohJuXvgMiCSSeGF19YqgezmNicOEyDGdXuSN7BtDvqCQ8MeYqQldnPzK_oNNW8ta0tnw2LQmf0brWZHmclXib9JVj7JN6faMg==', 'title': 'crittercontrol.com'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHikH4zJLdkJQg80KmE30onD0iv0fWZ_owkdyLRLD5mJPT1Ntrt_J68utfO2omyUxMfcvbTn02AbqZiqrjWA3rcgIFitCZe9MouiXYAhploBRH3QTX_w__weveMt6jMqIZvCdOdlYADZhwC3UFYNg==', 'title': 'seaworld.org'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_8ckWYXXHCvYLvK4fCKFn7rmuEsGlSG7Nocws9XT3LOHTrXmpE2K-IhEO94yPnji1KaVDmwpCkHoYJDUiJQ7AqU4_GrHw5U4EufghQffGg46OYooNPFlIJikjahDUs8MjwJiOSGbDqRJ4W-Mg_jDluZ8Nl59tTtYhqLolJWFwja8SGDBFJv65hPI3Or-LxPYZiekQmo39QagEzIpwEgIrL19KPwy6wqRZKvZTZ7H3ePZA-rXZAdqU4v7GblD8-QSM46rkqQv5we8oyOXt4w==', 'title': 'bluereefaquarium.co.uk'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFmhaLUT5FXSCe9JLketaUGHHz9D7eR0bXXCWq31gf6RBIw-p6_c9dd8CVsodWOf0e1b01wQQpqFaE8fnN62mNIXtNf-8z0dhhe7FOaRsATeuOsJEPK_UUaHTdgA0zFHRxPDEL13e2JhPnL4EJilGHgCzp56Vxr1Ff7UCfleETlclhbkhCau-LhItpI', 'title': 'woodlandtrust.org.uk'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE3NCd5oA17ZUlfbQ9XbxXkx380QemvE4qYk479kDjOKNHHKzjwcTdVE5zmRx-BUGjgIkgLaXLGU6vKU8ax7F44_dZKMJXlELIeB7iVRvUUjUT2PqTqzB0WJsv0wLScaa-FCsUyu_lhct_UYWUvbJan', 'title': 'seaworld.com'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGJ9ARniOp2Bq8pMJfgt0EQc0NfxbSNyKcKyh2NYu2qUv5-kSk545BuzRV1PZoEIaO7zpEvZ1jWSb39WidJGYGvjHOeNn9kRihYwOsV334tFa7fv7Ne7fTlctHBmrq7RPM9rKWdiwP275qlswCsrNPiovl7y66QBw==', 'title': 'petscare.com'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQETEv5OTvoUF8IiONMTdsfVYwAiOccFc0pvTYJWBRgAebHXihwUBZawAhuRd_F9GIrieqokMtARA3y6uoLjHg4JCCrBaiKw29iTsJm_G5sZqBJh9YC4O0bFPtbKJakXPLAw_oBdWxR-YWxBXnmGFp1PsYfFybS7eBw=', 'title': 'si.edu'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQE_5vdCEqYBtISJo0eWBUEojafrDzqiRq8p5N40Jq2yvJYs1GbAQQhULKc1fOHyAjBYyUYMfMKitLtwyLIWeYE0GK3KW1Mvs6n0K6MPikCeBoUtMKrwayDgC8XB2DWx9rKju8w4rvCWJLPp9PplTFZuxCw=', 'title': 'seaworld.org'}}], 'groundingSupports': [{'segment': {'endIndex': 103, 'text': 'Otters are carnivorous mammals known for their playful behavior and adaptations to a semi-aquatic life.'}, 'groundingChunkIndices': [0, 1]}, {'segment': {'startIndex': 104, 'endIndex': 178, 'text': 'There are 14 known species of otters, which are part of the weasel family.'}, 'groundingChunkIndices': [0]}, {'segment': {'startIndex': 180, 'endIndex': 315, 'text': '**Physical Characteristics:** Otters typically have long, slender bodies with short legs and powerful webbed feet perfect for swimming.'}, 'groundingChunkIndices': [0, 1, 2]}, {'segment': {'startIndex': 316, 'endIndex': 372, 'text': 'They possess dense, waterproof fur that keeps them warm.'}, 'groundingChunkIndices': [0, 3]}, {'segment': {'startIndex': 373, 'endIndex': 482, 'text': 'Their size varies by species, ranging from about 2 to 6 feet in length and weighing between 6 and 100 pounds.'}, 'groundingChunkIndices': [2]}, {'segment': {'startIndex': 484, 'endIndex': 606, 'text': '**Habitat and Diet:** Most otters live in and around freshwater rivers, lakes, and wetlands, while two species are marine.'}, 'groundingChunkIndices': [0, 1]}, {'segment': {'startIndex': 607, 'endIndex': 714, 'text': 'Their diet is primarily carnivorous and consists of fish, crayfish, crabs, and other aquatic invertebrates.'}, 'groundingChunkIndices': [4, 0, 5, 6, 7]}, {'segment': {'startIndex': 715, 'endIndex': 793, 'text': 'Some species are adept at using tools, such as rocks, to break open shellfish.'}, 'groundingChunkIndices': [0, 8]}, {'segment': {'startIndex': 795, 'endIndex': 908, 'text': '**Behavior and Social Structure:** Otters are known for their playful nature, often seen sliding down riverbanks.'}, 'groundingChunkIndices': [0, 9, 10]}, {'segment': {'startIndex': 909, 'endIndex': 1002, 'text': 'Their social structure varies; some species are mostly solitary, while others live in groups.'}, 'groundingChunkIndices': [0, 3, 11]}, {'segment': {'startIndex': 1003, 'endIndex': 1079, 'text': 'They communicate through a variety of sounds, including whistles and chirps.'}, 'groundingChunkIndices': [3]}, {'segment': {'startIndex': 1080, 'endIndex': 1123, 'text': 'Otters can live up to 16 years in the wild.'}, 'groundingChunkIndices': [0]}], 'webSearchQueries': ['otter overview', 'what are the characteristics of otters', 'what do otters eat', 'otter behavior']}], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smsg = mk_msg(\"Search the web and tell me very briefly about otters\")\n",
    "r = c(smsg, web_search_options={\"search_context_size\": \"low\"})  # or 'medium' / 'high'\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1af925d",
   "metadata": {},
   "source": [
    "## Citations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abaeeee1",
   "metadata": {},
   "source": [
    "Next, lets handle Anthropic's search citations.\n",
    "\n",
    "When not using streaming, all citations are placed in a separate key in the response:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45e2a7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['searchEntryPoint', 'groundingChunks', 'groundingSupports', 'webSearchQueries'])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['vertex_ai_grounding_metadata'][0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da64e713",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['otter overview',\n",
       " 'what are the characteristics of otters',\n",
       " 'what do otters eat',\n",
       " 'otter behavior']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['vertex_ai_grounding_metadata'][0]['webSearchQueries']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689cdf9d",
   "metadata": {},
   "source": [
    "Web search results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "030d17a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQF2pn0bF_6oFB3sdMwWq6AhdM4zuO-Xps0_S1bLBeXmWli7HPTvNASqRFBdloU1Si-pU2Guj-4yGn8t2lY3znWVGeG1ZI8R93cajVmHVmeytR74QRLYVH77UwL_hiPz',\n",
       "   'title': 'wikipedia.org'}},\n",
       " {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEodF4gXW5x0gckfC-61dKg4HrDDzH4Gmg5CYOFmSPmJAXVDu9Im4Hr6kIQCPXhkHas81DGHb9zOGZib_HCmBoR2P0YX2848NHuivTqntd0FcuMOEBWEXvvztxrJNEfH9c2QQ==',\n",
       "   'title': 'britannica.com'}},\n",
       " {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFFQo0zU7gp9wrrxaxTWtv8TcphvYKvxSL0ejmYdfsEAIJ9Dmy5pQwTOxqQbqW-sLKrQoPE4T1yQ9hl4oYgD5pc__Fd-lWZn4bfLFUgMdnRXKNpoaO8ZymBoGLtzTOqJg5lVnwtVKvNTNqCTWwdCI_U5pMgerGQNZqV6MB6U3N8VLVeGho=',\n",
       "   'title': 'study.com'}}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['vertex_ai_grounding_metadata'][0]['groundingChunks'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8971dc08",
   "metadata": {},
   "source": [
    "Citations in gemini: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb79aef5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'segment': {'endIndex': 103,\n",
       "   'text': 'Otters are carnivorous mammals known for their playful behavior and adaptations to a semi-aquatic life.'},\n",
       "  'groundingChunkIndices': [0, 1]},\n",
       " {'segment': {'startIndex': 104,\n",
       "   'endIndex': 178,\n",
       "   'text': 'There are 14 known species of otters, which are part of the weasel family.'},\n",
       "  'groundingChunkIndices': [0]},\n",
       " {'segment': {'startIndex': 180,\n",
       "   'endIndex': 315,\n",
       "   'text': '**Physical Characteristics:** Otters typically have long, slender bodies with short legs and powerful webbed feet perfect for swimming.'},\n",
       "  'groundingChunkIndices': [0, 1, 2]}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r['vertex_ai_grounding_metadata'][0]['groundingSupports'][:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f2a8559",
   "metadata": {},
   "source": [
    "However, when streaming the results are not captured this way.\n",
    "Instead, we provide this helper function that adds the citation to the `content` field in markdown format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc341e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cite_footnote(msg):\n",
    "    if not (delta:=nested_idx(msg, 'choices', 0, 'delta')): return\n",
    "    if citation:= nested_idx(delta, 'provider_specific_fields', 'citation'):\n",
    "        title = citation['title'].replace('\"', '\\\\\"')\n",
    "        delta.content = f'[*]({citation[\"url\"]} \"{title}\") '\n",
    "        \n",
    "def cite_footnotes(stream_list):\n",
    "    \"Add markdown footnote citations to stream deltas\"\n",
    "    for msg in stream_list: cite_footnote(msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2150365",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Otters are carnivorous mammals belonging to the subfamily Lutrinae, part of the weasel family (Mustelidae), which also includes badgers, mink, and wolverines. There are 13 extant species of otters, all of which are semi-aquatic, living in both freshwater and marine environments. They are found on every continent except Australia and Antarctica.\n",
       "\n",
       "These mammals are recognized by their long, slim bodies, short limbs, and powerful, webbed feet, which make them excellent swimmers. Most species also possess a long, muscular tail. Otters have incredibly dense, insulated fur, especially sea otters which have the thickest fur of any animal, helping them trap air for warmth and buoyancy in water as they lack a blubber layer. Their diet primarily consists of fish, but can also include frogs, birds, and shellfish. Otters are known for their playful behavior, engaging in activities like sliding and playing with stones. They typically live in dens called \"holts\" near water sources.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=305, prompt_tokens=12, total_tokens=317, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Otters are carnivorous mammals belonging to the subfamily Lutrinae, part of the weasel family (Mustelidae), which also includes badgers, mink, and wolverines. There are 13 extant species of otters, all of which are semi-aquatic, living in both freshwater and marine environments. They are found on every continent except Australia and Antarctica.\\n\\nThese mammals are recognized by their long, slim bodies, short limbs, and powerful, webbed feet, which make them excellent swimmers. Most species also possess a long, muscular tail. Otters have incredibly dense, insulated fur, especially sea otters which have the thickest fur of any animal, helping them trap air for warmth and buoyancy in water as they lack a blubber layer. Their diet primarily consists of fish, but can also include frogs, birds, and shellfish. Otters are known for their playful behavior, engaging in activities like sliding and playing with stones. They typically live in dens called \"holts\" near water sources.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, annotations=[{'type': 'url_citation', 'url_citation': {'start_index': 159, 'end_index': 278, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEikjifWl2Ho0XBpILK2GDuDb_yAvNWrhkvWaMWYUyDvmjDr-vgVI5dbqrVg7m4c1bLSB7UFDU-HwbhaKEz0Btj6xrm00GjjGDZuGro4FxG5v5xAzEFYTBAzvYBLXbX', 'title': 'wikipedia.org'}}, {'type': 'url_citation', 'url_citation': {'start_index': 280, 'end_index': 345, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH3h53jqxHUFanpfBoCWCSFEatHhzUvZTR2jq6wxCsWxLa7DYQTY3FcNoJiHbyjDmfZE4zkpISg00GsT1Hgy5cwXP0SutjGcjnURlTdnR1gcGlI7KmbRyfP_arMsTmdWmQABglBAGRpZQHV-3WZjrd1UKjCg3h81XbQM43c', 'title': 'treehugger.com'}}, {'type': 'url_citation', 'url_citation': {'start_index': 348, 'end_index': 479, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEikjifWl2Ho0XBpILK2GDuDb_yAvNWrhkvWaMWYUyDvmjDr-vgVI5dbqrVg7m4c1bLSB7UFDU-HwbhaKEz0Btj6xrm00GjjGDZuGro4FxG5v5xAzEFYTBAzvYBLXbX', 'title': 'wikipedia.org'}}, {'type': 'url_citation', 'url_citation': {'start_index': 481, 'end_index': 528, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEikjifWl2Ho0XBpILK2GDuDb_yAvNWrhkvWaMWYUyDvmjDr-vgVI5dbqrVg7m4c1bLSB7UFDU-HwbhaKEz0Btj6xrm00GjjGDZuGro4FxG5v5xAzEFYTBAzvYBLXbX', 'title': 'wikipedia.org'}}, {'type': 'url_citation', 'url_citation': {'start_index': 530, 'end_index': 723, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEikjifWl2Ho0XBpILK2GDuDb_yAvNWrhkvWaMWYUyDvmjDr-vgVI5dbqrVg7m4c1bLSB7UFDU-HwbhaKEz0Btj6xrm00GjjGDZuGro4FxG5v5xAzEFYTBAzvYBLXbX', 'title': 'wikipedia.org'}}, {'type': 'url_citation', 'url_citation': {'start_index': 725, 'end_index': 812, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEikjifWl2Ho0XBpILK2GDuDb_yAvNWrhkvWaMWYUyDvmjDr-vgVI5dbqrVg7m4c1bLSB7UFDU-HwbhaKEz0Btj6xrm00GjjGDZuGro4FxG5v5xAzEFYTBAzvYBLXbX', 'title': 'wikipedia.org'}}, {'type': 'url_citation', 'url_citation': {'start_index': 814, 'end_index': 918, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEikjifWl2Ho0XBpILK2GDuDb_yAvNWrhkvWaMWYUyDvmjDr-vgVI5dbqrVg7m4c1bLSB7UFDU-HwbhaKEz0Btj6xrm00GjjGDZuGro4FxG5v5xAzEFYTBAzvYBLXbX', 'title': 'wikipedia.org'}}, {'type': 'url_citation', 'url_citation': {'start_index': 920, 'end_index': 981, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEOcyghAA8GOTgGxMjelz2bAQn5RrgyF6WKnKhjZ2HByNFaOM-WtZN-XtMXWLRls6quJSFyIk-flfzmfNWZYvQ1YacXTlvLHRcQ5E3IwKDrfMkGGh8hVX-Uchje967i3azI3-38CojrrRMry7FNriPmBXwym1572RM=', 'title': 'crittercarewildlife.org'}}]))], usage=Usage(completion_tokens=305, prompt_tokens=12, total_tokens=317, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = list(c(smsg, ms[2], stream=True, web_search_options={\"search_context_size\": \"low\"}))\n",
    "cite_footnotes(r)\n",
    "stream_chunk_builder(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29018310",
   "metadata": {},
   "source": [
    "# Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9223dc",
   "metadata": {},
   "source": [
    "LiteLLM is pretty bare bones. It doesnt keep track of conversation history or what tools have been added in the conversation so far.\n",
    "\n",
    "So lets make a Claudette style wrapper so we can do streaming, toolcalling, and toolloops without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a636d732",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "effort = AttrDict({o[0]:o for o in ('low','medium','high')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715e9a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _mk_prefill(pf): return ModelResponseStream([StreamingChoices(delta=Delta(content=pf,role='assistant'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "987fba85",
   "metadata": {},
   "source": [
    "When the tool uses are about to be exhausted it is important to alert the AI so that it knows to use its final steps for communicating the user current progress and next steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18e226c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "_final_prompt = dict(role=\"user\", content=\"You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Chat:\n",
    "    def __init__(\n",
    "        self,\n",
    "        model:str,                # LiteLLM compatible model name \n",
    "        sp='',                    # System prompt\n",
    "        temp=0,                   # Temperature\n",
    "        search=False,             # Search (l,m,h), if model supports it\n",
    "        tools:list=None,          # Add tools\n",
    "        hist:list=None,           # Chat history\n",
    "        ns:Optional[dict]=None,   # Custom namespace for tool calling \n",
    "        cache=False,              # Anthropic prompt caching\n",
    "        cache_idxs:list=[-1],     # Anthropic cache breakpoint idxs, use `0` for sys prompt if provided\n",
    "        ttl=None,                 # Anthropic prompt caching ttl\n",
    "        api_base=None,            # API base URL for custom providers\n",
    "        api_key=None,             # API key for custom providers\n",
    "    ):\n",
    "        \"LiteLLM chat client.\"\n",
    "        self.model = model\n",
    "        hist,tools = mk_msgs(hist,cache,cache_idxs,ttl),listify(tools)\n",
    "        if ns is None and tools: ns = mk_ns(tools)\n",
    "        elif ns is None: ns = globals()\n",
    "        self.tool_schemas = [lite_mk_func(t) for t in tools] if tools else None\n",
    "        store_attr()\n",
    "    \n",
    "    def _prep_msg(self, msg=None, prefill=None):\n",
    "        \"Prepare the messages list for the API call\"\n",
    "        sp = [{\"role\": \"system\", \"content\": self.sp}] if self.sp else []\n",
    "        if sp:\n",
    "            if 0 in self.cache_idxs: sp[0] = _add_cache_control(sp[0])\n",
    "            cache_idxs = L(self.cache_idxs).filter().map(lambda o: o-1 if o>0 else o)\n",
    "        else:\n",
    "            cache_idxs = self.cache_idxs\n",
    "        if msg: self.hist = mk_msgs(self.hist+[msg], self.cache and 'claude' in self.model, cache_idxs, self.ttl)\n",
    "        pf = [{\"role\":\"assistant\",\"content\":prefill}] if prefill else []\n",
    "        return sp + self.hist + pf\n",
    "\n",
    "    def _call(self, msg=None, prefill=None, temp=None, think=None, search=None, stream=False, max_steps=2, step=1, final_prompt=None, tool_choice=None, **kwargs):\n",
    "        \"Internal method that always yields responses\"\n",
    "        if step>max_steps: return\n",
    "        try:\n",
    "            model_info = get_model_info(self.model)\n",
    "        except Exception:\n",
    "            register_model({self.model: {}})\n",
    "            model_info = get_model_info(self.model)\n",
    "        if not model_info.get(\"supports_assistant_prefill\"): prefill=None\n",
    "        if _has_search(self.model) and (s:=ifnone(search,self.search)): kwargs['web_search_options'] = {\"search_context_size\": effort[s]}\n",
    "        else: _=kwargs.pop('web_search_options',None)\n",
    "        if self.api_base: kwargs['api_base'] = self.api_base\n",
    "        if self.api_key: kwargs['api_key'] = self.api_key\n",
    "        res = completion(model=self.model, messages=self._prep_msg(msg, prefill), stream=stream, \n",
    "                         tools=self.tool_schemas, reasoning_effort = effort.get(think), tool_choice=tool_choice,\n",
    "                         # temperature is not supported when reasoning\n",
    "                         temperature=None if think else ifnone(temp,self.temp),\n",
    "                         caching=self.cache and 'claude' not in self.model,\n",
    "                         **kwargs)\n",
    "        if stream:\n",
    "            if prefill: yield _mk_prefill(prefill)\n",
    "            res = yield from stream_with_complete(res,postproc=cite_footnotes)\n",
    "        m = contents(res)\n",
    "        if prefill: m.content = prefill + m.content\n",
    "        self.hist.append(m)\n",
    "        yield res\n",
    "\n",
    "        if tcs := m.tool_calls:\n",
    "            tool_results=[_lite_call_func(tc, ns=self.ns) for tc in tcs]\n",
    "            self.hist+=tool_results\n",
    "            for r in tool_results: yield r\n",
    "            if step>=max_steps-1: prompt,tool_choice,search = final_prompt,'none',False\n",
    "            else: prompt = None\n",
    "            yield from self._call(\n",
    "                prompt, prefill, temp, think, search, stream, max_steps, step+1,\n",
    "                final_prompt, tool_choice, **kwargs)\n",
    "    \n",
    "    def __call__(self,\n",
    "                 msg=None,          # Message str, or list of multiple message parts\n",
    "                 prefill=None,      # Prefill AI response if model supports it\n",
    "                 temp=None,         # Override temp set on chat initialization\n",
    "                 think=None,        # Thinking (l,m,h)\n",
    "                 search=None,       # Override search set on chat initialization (l,m,h)\n",
    "                 stream=False,      # Stream results\n",
    "                 max_steps=2, # Maximum number of tool calls\n",
    "                 final_prompt=_final_prompt, # Final prompt when tool calls have ran out \n",
    "                 return_all=False,  # Returns all intermediate ModelResponses if not streaming and has tool calls\n",
    "                 **kwargs):\n",
    "        \"Main call method - handles streaming vs non-streaming\"\n",
    "        result_gen = self._call(msg, prefill, temp, think, search, stream, max_steps, 1, final_prompt, **kwargs)     \n",
    "        if stream: return result_gen              # streaming\n",
    "        elif return_all: return list(result_gen)  # toolloop behavior\n",
    "        else: return last(result_gen)             # normal chat behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69419bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(as_prop=True)\n",
    "def cost(self: Chat):\n",
    "    \"Total cost of all responses in conversation history\"\n",
    "    return sum(getattr(r, '_hidden_params', {}).get('response_cost')  or 0\n",
    "               for r in self.h if hasattr(r, 'choices'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9247ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def print_hist(self:Chat):\n",
    "    \"Print each message on a different line\"\n",
    "    for r in self.hist: print(r, end='\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce163563",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf319a",
   "metadata": {},
   "source": [
    "### History tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ccd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Rens! It's nice to meet you.\n",
       "\n",
       "How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=837, prompt_tokens=6, total_tokens=843, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=818, rejected_prediction_tokens=None, text_tokens=19, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=6, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Hi Rens! It's nice to meet you.\\n\\nHow can I help you today?\", role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=837, prompt_tokens=6, total_tokens=843, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=818, rejected_prediction_tokens=None, text_tokens=19, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=6, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(model)\n",
    "res = chat(\"Hey my name is Rens\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978da56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Rens.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=234, prompt_tokens=30, total_tokens=264, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=229, rejected_prediction_tokens=None, text_tokens=5, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=30, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Your name is Rens.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=234, prompt_tokens=30, total_tokens=264, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=229, rejected_prediction_tokens=None, text_tokens=5, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=30, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"Whats my name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb77c7",
   "metadata": {},
   "source": [
    "See now we keep track of history!\n",
    "\n",
    "History is stored in the `hist` attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a3c29d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'Hey my name is Rens'},\n",
       " Message(content=\"Hi Rens! It's nice to meet you.\\n\\nHow can I help you today?\", role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None),\n",
       " {'role': 'user', 'content': 'Whats my name'},\n",
       " Message(content='Your name is Rens.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d010ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'Hey my name is Rens'}\n",
      "\n",
      "Message(content=\"Hi Rens! It's nice to meet you.\\n\\nHow can I help you today?\", role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None)\n",
      "\n",
      "{'role': 'user', 'content': 'Whats my name'}\n",
      "\n",
      "Message(content='Your name is Rens.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chat.print_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f38015b",
   "metadata": {},
   "source": [
    "You can also pass an old chat history into new Chat objects:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f575f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Rens.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=255, prompt_tokens=43, total_tokens=298, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=250, rejected_prediction_tokens=None, text_tokens=5, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=43, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Your name is Rens.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=255, prompt_tokens=43, total_tokens=298, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=250, rejected_prediction_tokens=None, text_tokens=5, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=43, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat2 = Chat(model, hist=chat.hist)\n",
    "chat2(\"What was my name again?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36165660",
   "metadata": {},
   "source": [
    "You can prefix an [OpenAI compatible model](https://docs.litellm.ai/docs/providers/openai_compatible) with 'openai/' and use an `api_base` and `api_key` argument to use models not registered with litellm.\n",
    "\n",
    "```python\n",
    "import os, litellm\n",
    "OPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\n",
    "OPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\n",
    "c = Chat(\"openai/gpt-oss-20b\", api_key=OPENROUTER_API_KEY, api_base=OPENROUTER_BASE_URL)\n",
    "c(\"hi\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26748132",
   "metadata": {},
   "source": [
    "### Synthetic History Creation\n",
    "\n",
    "Lets build chat history step by step. That way we can tweak anything we need to during testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ef8d88",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = \"What is 5 + 7? Use the tool to calculate it.\"\n",
    "c = Chat(model, tools=[simple_add])\n",
    "res = c(pr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde51fc9",
   "metadata": {},
   "source": [
    "Whereas normally without tools we would get one user input and one assistant response. Here we get two extra messages in between.\n",
    "- An assistant message requesting the tools with arguments.\n",
    "- A tool response with the result to the tool call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49792a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n",
      "\n",
      "Message(content=None, role='assistant', tool_calls=[{'index': 0, 'provider_specific_fields': {'thought_signature': 'CoAEAXLI2nxg1BmWgRL8lIYWOo2H7fgtcxWY7BjC28NKgxeXMtS6HhOVRn6Mckj3qOtCNz9GlYYf+BHzfekYJWuR5231U0kBs/1o0WfTs6Hsv3/gWADGyxtL0xSTayXcMRlwZFEekDsuFoI9WRBHUk4n5HfxLYrUhLrkrDR5IMOKQSapoYFlASPWfyvhOsltYU3J9WHyQzKzdUNuW0UQBZ6SA89xwzGoEQVmiAk0232hNU0rPeQ0nGiRrwJnPFr3sEedM3ZPds3SMHCUPgbv2mVJeg1JJwV3/QK8CDqYOJzk11g41YWTgSFSSX+ulzAedWHMlDvCJkDxfMjM6oKckjf8FoQ06jZ+78H+k6t3Qdw7zcwAfPkzRNVJWZK0coY01dnSbJASRn8dPWc/qX2YF7oHtuMFLoOMcD+yu/wpVoTeoz/plaSUxYEx45zFh06WAxQGlQppRABi3Erq+MQBE5XZf8RZReqaF3hj4B/VbuefZdU58f1OClK7u9j4DkLqh07GkLIuFQFAJ7t7FfqzzH4qDWen2BTL1ptsBpU26OTaZM/YsGTWkPb8c6/XnrhKPf9EontbPzcY1Kw++M6E7LdkOeL6/0Dt8F17ELjwFW3RvRNozj9kKPZfIFwRXlFPFw57NVtHRwDpwfRJ+afztSz2Jr9VptkJ6dk/CL4gPfK1PYA='}, 'function': {'arguments': '{\"b\": 7, \"a\": 5}', 'name': 'simple_add'}, 'id': 'call_b293f3251da24184b521f56f0b60__thought__CoAEAXLI2nxg1BmWgRL8lIYWOo2H7fgtcxWY7BjC28NKgxeXMtS6HhOVRn6Mckj3qOtCNz9GlYYf+BHzfekYJWuR5231U0kBs/1o0WfTs6Hsv3/gWADGyxtL0xSTayXcMRlwZFEekDsuFoI9WRBHUk4n5HfxLYrUhLrkrDR5IMOKQSapoYFlASPWfyvhOsltYU3J9WHyQzKzdUNuW0UQBZ6SA89xwzGoEQVmiAk0232hNU0rPeQ0nGiRrwJnPFr3sEedM3ZPds3SMHCUPgbv2mVJeg1JJwV3/QK8CDqYOJzk11g41YWTgSFSSX+ulzAedWHMlDvCJkDxfMjM6oKckjf8FoQ06jZ+78H+k6t3Qdw7zcwAfPkzRNVJWZK0coY01dnSbJASRn8dPWc/qX2YF7oHtuMFLoOMcD+yu/wpVoTeoz/plaSUxYEx45zFh06WAxQGlQppRABi3Erq+MQBE5XZf8RZReqaF3hj4B/VbuefZdU58f1OClK7u9j4DkLqh07GkLIuFQFAJ7t7FfqzzH4qDWen2BTL1ptsBpU26OTaZM/YsGTWkPb8c6/XnrhKPf9EontbPzcY1Kw++M6E7LdkOeL6/0Dt8F17ELjwFW3RvRNozj9kKPZfIFwRXlFPFw57NVtHRwDpwfRJ+afztSz2Jr9VptkJ6dk/CL4gPfK1PYA=', 'type': 'function'}], function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"functionCall\": {\"name\": \"simple_add\", \"args\": {\"b\": 7, \"a\": 5}}}', 'signature': 'CoAEAXLI2nxg1BmWgRL8lIYWOo2H7fgtcxWY7BjC28NKgxeXMtS6HhOVRn6Mckj3qOtCNz9GlYYf+BHzfekYJWuR5231U0kBs/1o0WfTs6Hsv3/gWADGyxtL0xSTayXcMRlwZFEekDsuFoI9WRBHUk4n5HfxLYrUhLrkrDR5IMOKQSapoYFlASPWfyvhOsltYU3J9WHyQzKzdUNuW0UQBZ6SA89xwzGoEQVmiAk0232hNU0rPeQ0nGiRrwJnPFr3sEedM3ZPds3SMHCUPgbv2mVJeg1JJwV3/QK8CDqYOJzk11g41YWTgSFSSX+ulzAedWHMlDvCJkDxfMjM6oKckjf8FoQ06jZ+78H+k6t3Qdw7zcwAfPkzRNVJWZK0coY01dnSbJASRn8dPWc/qX2YF7oHtuMFLoOMcD+yu/wpVoTeoz/plaSUxYEx45zFh06WAxQGlQppRABi3Erq+MQBE5XZf8RZReqaF3hj4B/VbuefZdU58f1OClK7u9j4DkLqh07GkLIuFQFAJ7t7FfqzzH4qDWen2BTL1ptsBpU26OTaZM/YsGTWkPb8c6/XnrhKPf9EontbPzcY1Kw++M6E7LdkOeL6/0Dt8F17ELjwFW3RvRNozj9kKPZfIFwRXlFPFw57NVtHRwDpwfRJ+afztSz2Jr9VptkJ6dk/CL4gPfK1PYA='}], provider_specific_fields=None)\n",
      "\n",
      "{'tool_call_id': 'call_b293f3251da24184b521f56f0b60__thought__CoAEAXLI2nxg1BmWgRL8lIYWOo2H7fgtcxWY7BjC28NKgxeXMtS6HhOVRn6Mckj3qOtCNz9GlYYf+BHzfekYJWuR5231U0kBs/1o0WfTs6Hsv3/gWADGyxtL0xSTayXcMRlwZFEekDsuFoI9WRBHUk4n5HfxLYrUhLrkrDR5IMOKQSapoYFlASPWfyvhOsltYU3J9WHyQzKzdUNuW0UQBZ6SA89xwzGoEQVmiAk0232hNU0rPeQ0nGiRrwJnPFr3sEedM3ZPds3SMHCUPgbv2mVJeg1JJwV3/QK8CDqYOJzk11g41YWTgSFSSX+ulzAedWHMlDvCJkDxfMjM6oKckjf8FoQ06jZ+78H+k6t3Qdw7zcwAfPkzRNVJWZK0coY01dnSbJASRn8dPWc/qX2YF7oHtuMFLoOMcD+yu/wpVoTeoz/plaSUxYEx45zFh06WAxQGlQppRABi3Erq+MQBE5XZf8RZReqaF3hj4B/VbuefZdU58f1OClK7u9j4DkLqh07GkLIuFQFAJ7t7FfqzzH4qDWen2BTL1ptsBpU26OTaZM/YsGTWkPb8c6/XnrhKPf9EontbPzcY1Kw++M6E7LdkOeL6/0Dt8F17ELjwFW3RvRNozj9kKPZfIFwRXlFPFw57NVtHRwDpwfRJ+afztSz2Jr9VptkJ6dk/CL4gPfK1PYA=', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n",
      "\n",
      "{'role': 'user', 'content': 'You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.'}\n",
      "\n",
      "Message(content='Based on my tool use, I have found that the sum of 5 and 7 is 12. My goal of calculating the value is complete.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2eb0a2",
   "metadata": {},
   "source": [
    "Lets try to build this up manually so we have full control over the inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37a77b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def random_tool_id():\n",
    "    \"Generate a random tool ID with 'toolu_' prefix\"\n",
    "    random_part = ''.join(random.choices(string.ascii_letters + string.digits, k=25))\n",
    "    return f'toolu_{random_part}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a0bd16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'toolu_0UAqFzWsDK4FrUMp48Y3tT3QD'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random_tool_id()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d22e52b7",
   "metadata": {},
   "source": [
    "A tool call request can contain one more or more tool calls. Lets make one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00e88b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc(func, args, tcid=None, idx=1):\n",
    "    if not tcid: tcid = random_tool_id()\n",
    "    return {'index': idx, 'function': {'arguments': args, 'name': func}, 'id': tcid, 'type': 'function'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324b9182",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 1,\n",
       " 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'},\n",
       " 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n",
       " 'type': 'function'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc = mk_tc(simple_add.__name__, json.dumps(dict(a=5, b=7)))\n",
    "tc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97da6222",
   "metadata": {},
   "source": [
    "This can then be packged into the full Message object produced by the assitant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436abceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_tc_req(content, tcs): return Message(content=content, role='assistant', tool_calls=tcs, function_call=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94c031e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 5, \"b\": 7}', name='simple_add'), id='toolu_gAL47D1qXIaSyZPaE1pu1lJo7', type='function')], function_call=None, provider_specific_fields=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc_cts = \"I'll use the simple_add tool to calculate 5 + 7 for you.\"\n",
    "tcq = mk_tc_req(tc_cts, [tc])\n",
    "tcq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a1a0364",
   "metadata": {},
   "source": [
    "Notice how Message instantiation creates a list of ChatCompletionMessageToolCalls by default. When the tools are executed this is converted back\n",
    "to a dictionary, for consistency we want to keep these as dictionaries from the beginning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00cebbbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc_req(content, tcs):\n",
    "    msg = Message(content=content, role='assistant', tool_calls=tcs, function_call=None)\n",
    "    msg.tool_calls = [{**dict(tc), 'function': dict(tc['function'])} for tc in msg.tool_calls]\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0d3468d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcq = mk_tc_req(tc_cts, [tc])\n",
    "tcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75dc3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chat(model, tools=[simple_add], hist=[pr, tcq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd673382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n",
      "\n",
      "Message(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c490dcfb",
   "metadata": {},
   "source": [
    "Looks good so far! Now we will want to provide the actual result!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e69d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc_result(tc, result): return {'tool_call_id': tc['id'], 'role': 'tool', 'name': tc['function']['name'], 'content': result}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94067b82",
   "metadata": {},
   "source": [
    "Note we might have more than one tool call if more than one was passed in, here we just will make one result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175b9d78",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': 1,\n",
       " 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'},\n",
       " 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n",
       " 'type': 'function'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcq.tool_calls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f969e27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '12'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_tc_result(tcq.tool_calls[0], '12')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5d8e695",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tc_results(tcq, results): return [mk_tc_result(a,b) for a,b in zip(tcq.tool_calls, results)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90d8c658",
   "metadata": {},
   "source": [
    "Same for here tcq.tool_calls will match the number of results passed in the results list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd6e2307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c2f72e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '12'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcr = mk_tc_results(tcq, ['12'])\n",
    "tcr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "608b90d2",
   "metadata": {},
   "source": [
    "Now we can call it with this synthetic data to see what the response is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efed96b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "OK, 5 + 7 = 12.\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=12, prompt_tokens=134, total_tokens=146, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=134, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='OK, 5 + 7 = 12.\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=12, prompt_tokens=134, total_tokens=146, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=134, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(tcr[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8e06d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n",
      "\n",
      "Message(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n",
      "\n",
      "{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n",
      "\n",
      "Message(content='OK, 5 + 7 = 12.\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b6af73",
   "metadata": {},
   "source": [
    "Lets try this again, but lets give it something that is clearly wrong for fun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01a9049c",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chat(model, tools=[simple_add], hist=[pr, tcq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f546c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '13'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcr = mk_tc_results(tcq, ['13'])\n",
    "tcr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7befdf1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "OK, 5 + 7 is 13.\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=12, prompt_tokens=134, total_tokens=146, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=134, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='OK, 5 + 7 is 13.\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=12, prompt_tokens=134, total_tokens=146, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=134, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(tcr[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84387429",
   "metadata": {},
   "source": [
    "Lets make sure this works with multiple tool calls in the same assistant Message."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "027f9a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcs = [\n",
    "    mk_tc(simple_add.__name__, json.dumps({\"a\": 5, \"b\": 7})), \n",
    "    mk_tc(simple_add.__name__, json.dumps({\"a\": 6, \"b\": 7})), \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44baa92b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content='I will calculate these for you!', role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'type': 'function'}, {'index': 1, 'function': {'arguments': '{\"a\": 6, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'type': 'function'}], function_call=None, provider_specific_fields=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcq = mk_tc_req(\"I will calculate these for you!\", tcs)\n",
    "tcq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2abb6a8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tcr = mk_tc_results(tcq, ['12', '13'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866aa31d",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = Chat(model, tools=[simple_add], hist=[pr, tcq, tcr[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a9d9ecd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "5 + 7 is 12 and 6 + 7 is 13.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=430, prompt_tokens=156, total_tokens=586, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=411, rejected_prediction_tokens=None, text_tokens=19, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=156, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 7 is 12 and 6 + 7 is 13.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"text\": \"5 + 7 is 12 and 6 + 7 is 13.\"}', 'signature': 'CpILAXLI2nx+XPCzUx5WjEhFwJZm6lLA6zNElCJcmjOjCQvssWGtn9ksFQT8t8pD7AUMtniOtq77jwPl6IchSf2t4e6dhlefATiE6k3p4l5HtUIyeo5FdBBAYmGt0CTY5uhTPw3eskyuCGA0svLxdoHOkYk4bwEioM5wDZkPg6jm8v55Soq0NfiO8YKsumvVoPcMk/eBP4KC9WT6BXlYYO3A0cpy1RAGkQo/g/Avg0Vsv5SMnBEE7XUCBHE9FmIz+ceeU7N6Ee/nmOx05I6CBrMlNlhKHAyIfyEc38zFE/TrMlO3MkG4C1BGv0cCK3XMFnDuQXrVH49q8TTf0BbrBKYI4wzQp4IndZhC9Y8PgJEloXYtOzrD8JO+F3q13SHhkB7XuRugnY8q38K5PSq567ixyz4y6r2OJ/VQwclBRKA8tlSkYlkV8VNXTHxkGFxLSvhwe7KyYF8n2YYvfdK3BwuA3TFfnPgHRzkkMqFeK0e1jaSdVNE0WoQCP1cddbZ2tRu2/OjRtTykQpVUYFigY5diLEQDFhtbfo2hgCu8St0gr9MJnPtozFep0IFMdc2otGq0cvV+LLXOD4K3O3lfSYRY1mpfjYoaRQUzPrU129LpBwy3jlgCAZy//pYv4LfVncr8vqy3CcvYoPVxfkFMUt6K4aaDfAnyf/UXSL4m0S8KYvlYrcGPRu/3KQzwVdHpHNsOph8v4kmcp2fowOCOWoSOhFLyiTBsxWcKn38q/EK4ytWFqfhCoj/arafxsyVSttBi2wbwt6Cb3WyxWM6e76rnAG8cXzZcyq57omTVBhyHOUgjO6J6OG/08a196dJ9gWweUxXkFpBqgrqmxd/XWEfkyMo88OIbLIg42LOfjl3F8QTIByCWX4D29Sm8Dzx8t/XxtycRwxFRBlzRbNioz2FVcpZDsQQJINVjWy4Z5vKW3VW82I9MaK4GzZXKbwhzCxXucqucTH7LtSTP2or3IuJNBdGk3QllqKJqzpay4HTtHFJLWFst7Vmh8mB8DyRgqEKEskAo15Jr0gemw6Je3YnzHeJ1cBkfbZ2IWa3OGOMTnsEeIe4vBlNX/qF2Jf33xnDk1KYC80iFZcQ0TXqu4dcPJ4q2SxTqlE/WZlejPUC61uBAkBnUWY7Ohyspi+SZNTG9KSWlbkDILW5SPkwN+yGv/3xokal0I60MasYgznZbdem8jwQgJnaekP8uqBqqbKA/DKMMv0QGyv24liRCiSyQgTuHjzAhRsrLhBKwTq9aQglTb3d5yDSYQzBf1c6enoouGVITpv0cb/hvJGlJ/ac0F2E7frv+o6+HcMMqr+iLOxKcXeck28ukYz1+ysvO20RHqtGevqTOA+r4ec1JbVmUivnQpGHIT7fDfnnLnrAfW+Pb/MB+Yt0Ubdw9wLvHXdM5G0iICIkf5Jbizo4q5pl11u++AHZivgrQm90yDl9bO/J+ICK3l1yLXS09ulnRvSxVXTZefGVyis31qjgXjtZ8e809ysi1++LPd0GUP/DdSwQLV/b1OUdVZ62lApGa5KzUdnO1sgiurIpjlEB1EDrWtNV5UDpW7CySMUs7cCLOOBlv1uBt5K63UBfk61EqaS1kqS4Uxq7cl79FQca8iSg6p1E+hKaUi45C/AL3wivdDZEfQjnxLq4itC4nL/JBB9p1dAEVC76h42J8IBMMiLRgWNPRUT9kYPndcGBPuQSOAqGWXYSzGDrHE68LyBpD1hEI2bTt+dykyn6E6eoctijvCL20+AusfcBVrAWeHg5PXj3cSeH64z7tnP8/y9ky101opct5y6W2IQraqM+vtd2dyrRRE/xQsGKU9cQ26++drXc6ZLcLt3avR/XThqE3XhMkJ2bL/LiIZ1wfHGYTfypUVFMY0GPu05atsVgLN7h8JH21YA=='}], provider_specific_fields=None))], usage=Usage(completion_tokens=430, prompt_tokens=156, total_tokens=586, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=411, rejected_prediction_tokens=None, text_tokens=19, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=156, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/markdown": [
       "I have calculated the result of 6 + 7, which is 13. How can I help you further?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=56, prompt_tokens=209, total_tokens=265, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=31, rejected_prediction_tokens=None, text_tokens=25, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=209, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='I have calculated the result of 6 + 7, which is 13. How can I help you further?', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"text\": \"I have calculated the result of 6 + 7, which is 13. How can I help you further?\"}', 'signature': 'CqgBAXLI2nyTID3PEoOa3UuJAlJijfkTlp6ruZpGgKPckCtaHUSV1kFi9tRhTen3cxwspgfGdZRFQCoIn+fBTu/rqUe/P6H4Dt+4DZLXAZ/DmRfetCLB5TlOMaemuB5cTTamPVIRGvdqpQ/tDk/nKd3LNyrMHzXul01n3G6s4gB9znrtrZoN86RK/pU1RV01kPFCMahSVZz6kSUlmKsmHCGotXaNVsZW2mDF'}], provider_specific_fields=None))], usage=Usage(completion_tokens=56, prompt_tokens=209, total_tokens=265, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=31, rejected_prediction_tokens=None, text_tokens=25, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=209, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/markdown": [
       "I have the result of the calculation. What would you like me to do with it?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=49, prompt_tokens=274, total_tokens=323, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=31, rejected_prediction_tokens=None, text_tokens=18, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=274, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='I have the result of the calculation. What would you like me to do with it?', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"text\": \"I have the result of the calculation. What would you like me to do with it?\"}', 'signature': 'CqgBAXLI2nxmNvbE3dC1BtN7is+ok4FZgEv8yGdXRHtMi3U4PBOk1wIm6lRvPUCRFUFmw460P0uFoAYhA/FR5ki0+EipA9hVCcpPpEudINbUI2Yyb9kNWKf60xuqLMIgEpeYPJHthKHUJJAfTvPpuRBHa0at1TTPi3A/mLtRJGcZs609g92Dk8EjKBvXf+VOd4l2LEZKL1ULqeva3Bt2AwdjnkIBBSJbVNl4'}], provider_specific_fields=None))], usage=Usage(completion_tokens=49, prompt_tokens=274, total_tokens=323, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=31, rejected_prediction_tokens=None, text_tokens=18, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=274, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c(tcr[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee111193",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n",
      "\n",
      "Message(content='I will calculate these for you!', role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'type': 'function'}, {'index': 1, 'function': {'arguments': '{\"a\": 6, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n",
      "\n",
      "{'tool_call_id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n",
      "\n",
      "{'tool_call_id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'role': 'tool', 'name': 'simple_add', 'content': '13'}\n",
      "\n",
      "Message(content='5 + 7 is 12 and 6 + 7 is 13.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"text\": \"5 + 7 is 12 and 6 + 7 is 13.\"}', 'signature': 'CpILAXLI2nx+XPCzUx5WjEhFwJZm6lLA6zNElCJcmjOjCQvssWGtn9ksFQT8t8pD7AUMtniOtq77jwPl6IchSf2t4e6dhlefATiE6k3p4l5HtUIyeo5FdBBAYmGt0CTY5uhTPw3eskyuCGA0svLxdoHOkYk4bwEioM5wDZkPg6jm8v55Soq0NfiO8YKsumvVoPcMk/eBP4KC9WT6BXlYYO3A0cpy1RAGkQo/g/Avg0Vsv5SMnBEE7XUCBHE9FmIz+ceeU7N6Ee/nmOx05I6CBrMlNlhKHAyIfyEc38zFE/TrMlO3MkG4C1BGv0cCK3XMFnDuQXrVH49q8TTf0BbrBKYI4wzQp4IndZhC9Y8PgJEloXYtOzrD8JO+F3q13SHhkB7XuRugnY8q38K5PSq567ixyz4y6r2OJ/VQwclBRKA8tlSkYlkV8VNXTHxkGFxLSvhwe7KyYF8n2YYvfdK3BwuA3TFfnPgHRzkkMqFeK0e1jaSdVNE0WoQCP1cddbZ2tRu2/OjRtTykQpVUYFigY5diLEQDFhtbfo2hgCu8St0gr9MJnPtozFep0IFMdc2otGq0cvV+LLXOD4K3O3lfSYRY1mpfjYoaRQUzPrU129LpBwy3jlgCAZy//pYv4LfVncr8vqy3CcvYoPVxfkFMUt6K4aaDfAnyf/UXSL4m0S8KYvlYrcGPRu/3KQzwVdHpHNsOph8v4kmcp2fowOCOWoSOhFLyiTBsxWcKn38q/EK4ytWFqfhCoj/arafxsyVSttBi2wbwt6Cb3WyxWM6e76rnAG8cXzZcyq57omTVBhyHOUgjO6J6OG/08a196dJ9gWweUxXkFpBqgrqmxd/XWEfkyMo88OIbLIg42LOfjl3F8QTIByCWX4D29Sm8Dzx8t/XxtycRwxFRBlzRbNioz2FVcpZDsQQJINVjWy4Z5vKW3VW82I9MaK4GzZXKbwhzCxXucqucTH7LtSTP2or3IuJNBdGk3QllqKJqzpay4HTtHFJLWFst7Vmh8mB8DyRgqEKEskAo15Jr0gemw6Je3YnzHeJ1cBkfbZ2IWa3OGOMTnsEeIe4vBlNX/qF2Jf33xnDk1KYC80iFZcQ0TXqu4dcPJ4q2SxTqlE/WZlejPUC61uBAkBnUWY7Ohyspi+SZNTG9KSWlbkDILW5SPkwN+yGv/3xokal0I60MasYgznZbdem8jwQgJnaekP8uqBqqbKA/DKMMv0QGyv24liRCiSyQgTuHjzAhRsrLhBKwTq9aQglTb3d5yDSYQzBf1c6enoouGVITpv0cb/hvJGlJ/ac0F2E7frv+o6+HcMMqr+iLOxKcXeck28ukYz1+ysvO20RHqtGevqTOA+r4ec1JbVmUivnQpGHIT7fDfnnLnrAfW+Pb/MB+Yt0Ubdw9wLvHXdM5G0iICIkf5Jbizo4q5pl11u++AHZivgrQm90yDl9bO/J+ICK3l1yLXS09ulnRvSxVXTZefGVyis31qjgXjtZ8e809ysi1++LPd0GUP/DdSwQLV/b1OUdVZ62lApGa5KzUdnO1sgiurIpjlEB1EDrWtNV5UDpW7CySMUs7cCLOOBlv1uBt5K63UBfk61EqaS1kqS4Uxq7cl79FQca8iSg6p1E+hKaUi45C/AL3wivdDZEfQjnxLq4itC4nL/JBB9p1dAEVC76h42J8IBMMiLRgWNPRUT9kYPndcGBPuQSOAqGWXYSzGDrHE68LyBpD1hEI2bTt+dykyn6E6eoctijvCL20+AusfcBVrAWeHg5PXj3cSeH64z7tnP8/y9ky101opct5y6W2IQraqM+vtd2dyrRRE/xQsGKU9cQ26++drXc6ZLcLt3avR/XThqE3XhMkJ2bL/LiIZ1wfHGYTfypUVFMY0GPu05atsVgLN7h8JH21YA=='}], provider_specific_fields=None)\n",
      "\n",
      "{'tool_call_id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'role': 'tool', 'name': 'simple_add', 'content': '13'}\n",
      "\n",
      "Message(content='I have calculated the result of 6 + 7, which is 13. How can I help you further?', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"text\": \"I have calculated the result of 6 + 7, which is 13. How can I help you further?\"}', 'signature': 'CqgBAXLI2nyTID3PEoOa3UuJAlJijfkTlp6ruZpGgKPckCtaHUSV1kFi9tRhTen3cxwspgfGdZRFQCoIn+fBTu/rqUe/P6H4Dt+4DZLXAZ/DmRfetCLB5TlOMaemuB5cTTamPVIRGvdqpQ/tDk/nKd3LNyrMHzXul01n3G6s4gB9znrtrZoN86RK/pU1RV01kPFCMahSVZz6kSUlmKsmHCGotXaNVsZW2mDF'}], provider_specific_fields=None)\n",
      "\n",
      "{'tool_call_id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'role': 'tool', 'name': 'simple_add', 'content': '13'}\n",
      "\n",
      "Message(content='I have the result of the calculation. What would you like me to do with it?', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"text\": \"I have the result of the calculation. What would you like me to do with it?\"}', 'signature': 'CqgBAXLI2nxmNvbE3dC1BtN7is+ok4FZgEv8yGdXRHtMi3U4PBOk1wIm6lRvPUCRFUFmw460P0uFoAYhA/FR5ki0+EipA9hVCcpPpEudINbUI2Yyb9kNWKf60xuqLMIgEpeYPJHthKHUJJAfTvPpuRBHa0at1TTPi3A/mLtRJGcZs609g92Dk8EjKBvXf+VOd4l2LEZKL1ULqeva3Bt2AwdjnkIBBSJbVNl4'}], provider_specific_fields=None)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "c.print_hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e5b97b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on my previous action, I used the `simple_add` tool to calculate the sum of 5 and 3. The tool returned a result of 8.\n",
       "\n",
       "Therefore, my finding is that 5 + 3 = 8. The goal has been completed.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=554, prompt_tokens=157, total_tokens=711, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=497, rejected_prediction_tokens=None, text_tokens=57, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=157, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Based on my previous action, I used the `simple_add` tool to calculate the sum of 5 and 3. The tool returned a result of 8.\\n\\nTherefore, my finding is that 5 + 3 = 8. The goal has been completed.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=554, prompt_tokens=157, total_tokens=711, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=497, rejected_prediction_tokens=None, text_tokens=57, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=157, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(ms[1], tools=[simple_add])\n",
    "res = chat(\"What's 5 + 3? Use the `simple_add` tool.\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c84d6ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Of course!\n",
       "\n",
       "What did the number 0 say to the number 8?\n",
       "\n",
       "\"Nice belt!\"\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=595, prompt_tokens=227, total_tokens=822, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=573, rejected_prediction_tokens=None, text_tokens=22, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=227, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Of course!\\n\\nWhat did the number 0 say to the number 8?\\n\\n\"Nice belt!\"', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"text\": \"Of course!\\\\n\\\\nWhat did the number 0 say to the number 8?\\\\n\\\\n\\\\\"Nice belt!\\\\\"\"}', 'signature': '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'}], provider_specific_fields=None))], usage=Usage(completion_tokens=595, prompt_tokens=227, total_tokens=822, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=573, rejected_prediction_tokens=None, text_tokens=22, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=227, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = chat(\"Now, tell me a joke based on that result.\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a8bec1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': \"What's 5 + 3? Use the `simple_add` tool.\"},\n",
       " Message(content=None, role='assistant', tool_calls=[{'index': 0, 'provider_specific_fields': {'thought_signature': 'CoIEAXLI2nzHj0pxAc8HKjiv8Pqf6WgSo++njY2Nu/s+nHhnNk3oMxhGCKWAf5FYN0VqiqD12CjdktEPLNoYQRdlmaP4LQW7gH2Rip0CGm5tgq1zXJBDgO6LVAgFdEnmpdYwQwOswHes8uB8n4VSVisXD5Rqn8rYcdRXFCsMqQeolD/dLmLmAl0bLB9fmBBzgXXgrm2YCk0ogfpYfLKRSxk7UTWKWRemWyk51/wygh6MZ3EWj7EvVeIKqWAqNWZgknxQwkb4zOKN+1uVVkFr4maRXcuUaPsvdtaZWTEWZsSpKbnlXm9x64sj0I0uiSAe97qilf2ck5RUtk8ms8oVeRhs0OjuSywRKZnlh9b0WSKhfL9n3uxholmahmybPs2DfS2a9ve84xmGkHcHMzqueo+Br6kOXDGSRVCo8AWBy11HM0o6sDrSxnuIjfaDrzMgEQ08S+KeDNFf0Wn2xtwUidFhiYLAlIYmEP3489P+DIaIKzHByV0Yl3DoNb9L3ij0M+3Q+/Yr08nDYi99SiNPtt+Hd02Cc7oY/Ee+b8Engp+ODFmNvJnXdXxAyCEtaLN861yFPs8vb3aYp+V51P2dk9vqCnPRdbpRFa9zBWMtm3RJzjy7LDrzofzThFPg5lme90hwNm1n/SqqE+ixWg5kFRjen3PhlFVp4nF40zIIwX3O3DgBeg=='}, 'function': {'arguments': '{\"a\": 5, \"b\": 3}', 'name': 'simple_add'}, 'id': 'call_d759a870fbb5440f87fa7c3a4f88__thought__CoIEAXLI2nzHj0pxAc8HKjiv8Pqf6WgSo++njY2Nu/s+nHhnNk3oMxhGCKWAf5FYN0VqiqD12CjdktEPLNoYQRdlmaP4LQW7gH2Rip0CGm5tgq1zXJBDgO6LVAgFdEnmpdYwQwOswHes8uB8n4VSVisXD5Rqn8rYcdRXFCsMqQeolD/dLmLmAl0bLB9fmBBzgXXgrm2YCk0ogfpYfLKRSxk7UTWKWRemWyk51/wygh6MZ3EWj7EvVeIKqWAqNWZgknxQwkb4zOKN+1uVVkFr4maRXcuUaPsvdtaZWTEWZsSpKbnlXm9x64sj0I0uiSAe97qilf2ck5RUtk8ms8oVeRhs0OjuSywRKZnlh9b0WSKhfL9n3uxholmahmybPs2DfS2a9ve84xmGkHcHMzqueo+Br6kOXDGSRVCo8AWBy11HM0o6sDrSxnuIjfaDrzMgEQ08S+KeDNFf0Wn2xtwUidFhiYLAlIYmEP3489P+DIaIKzHByV0Yl3DoNb9L3ij0M+3Q+/Yr08nDYi99SiNPtt+Hd02Cc7oY/Ee+b8Engp+ODFmNvJnXdXxAyCEtaLN861yFPs8vb3aYp+V51P2dk9vqCnPRdbpRFa9zBWMtm3RJzjy7LDrzofzThFPg5lme90hwNm1n/SqqE+ixWg5kFRjen3PhlFVp4nF40zIIwX3O3DgBeg==', 'type': 'function'}], function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"functionCall\": {\"name\": \"simple_add\", \"args\": {\"a\": 5, \"b\": 3}}}', 'signature': 'CoIEAXLI2nzHj0pxAc8HKjiv8Pqf6WgSo++njY2Nu/s+nHhnNk3oMxhGCKWAf5FYN0VqiqD12CjdktEPLNoYQRdlmaP4LQW7gH2Rip0CGm5tgq1zXJBDgO6LVAgFdEnmpdYwQwOswHes8uB8n4VSVisXD5Rqn8rYcdRXFCsMqQeolD/dLmLmAl0bLB9fmBBzgXXgrm2YCk0ogfpYfLKRSxk7UTWKWRemWyk51/wygh6MZ3EWj7EvVeIKqWAqNWZgknxQwkb4zOKN+1uVVkFr4maRXcuUaPsvdtaZWTEWZsSpKbnlXm9x64sj0I0uiSAe97qilf2ck5RUtk8ms8oVeRhs0OjuSywRKZnlh9b0WSKhfL9n3uxholmahmybPs2DfS2a9ve84xmGkHcHMzqueo+Br6kOXDGSRVCo8AWBy11HM0o6sDrSxnuIjfaDrzMgEQ08S+KeDNFf0Wn2xtwUidFhiYLAlIYmEP3489P+DIaIKzHByV0Yl3DoNb9L3ij0M+3Q+/Yr08nDYi99SiNPtt+Hd02Cc7oY/Ee+b8Engp+ODFmNvJnXdXxAyCEtaLN861yFPs8vb3aYp+V51P2dk9vqCnPRdbpRFa9zBWMtm3RJzjy7LDrzofzThFPg5lme90hwNm1n/SqqE+ixWg5kFRjen3PhlFVp4nF40zIIwX3O3DgBeg=='}], provider_specific_fields=None),\n",
       " {'tool_call_id': 'call_d759a870fbb5440f87fa7c3a4f88__thought__CoIEAXLI2nzHj0pxAc8HKjiv8Pqf6WgSo++njY2Nu/s+nHhnNk3oMxhGCKWAf5FYN0VqiqD12CjdktEPLNoYQRdlmaP4LQW7gH2Rip0CGm5tgq1zXJBDgO6LVAgFdEnmpdYwQwOswHes8uB8n4VSVisXD5Rqn8rYcdRXFCsMqQeolD/dLmLmAl0bLB9fmBBzgXXgrm2YCk0ogfpYfLKRSxk7UTWKWRemWyk51/wygh6MZ3EWj7EvVeIKqWAqNWZgknxQwkb4zOKN+1uVVkFr4maRXcuUaPsvdtaZWTEWZsSpKbnlXm9x64sj0I0uiSAe97qilf2ck5RUtk8ms8oVeRhs0OjuSywRKZnlh9b0WSKhfL9n3uxholmahmybPs2DfS2a9ve84xmGkHcHMzqueo+Br6kOXDGSRVCo8AWBy11HM0o6sDrSxnuIjfaDrzMgEQ08S+KeDNFf0Wn2xtwUidFhiYLAlIYmEP3489P+DIaIKzHByV0Yl3DoNb9L3ij0M+3Q+/Yr08nDYi99SiNPtt+Hd02Cc7oY/Ee+b8Engp+ODFmNvJnXdXxAyCEtaLN861yFPs8vb3aYp+V51P2dk9vqCnPRdbpRFa9zBWMtm3RJzjy7LDrzofzThFPg5lme90hwNm1n/SqqE+ixWg5kFRjen3PhlFVp4nF40zIIwX3O3DgBeg==',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '8'},\n",
       " {'role': 'user',\n",
       "  'content': 'You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.'},\n",
       " Message(content='Based on my previous action, I used the `simple_add` tool to calculate the sum of 5 and 3. The tool returned a result of 8.\\n\\nTherefore, my finding is that 5 + 3 = 8. The goal has been completed.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None),\n",
       " {'role': 'user', 'content': 'Now, tell me a joke based on that result.'},\n",
       " Message(content='Of course!\\n\\nWhat did the number 0 say to the number 8?\\n\\n\"Nice belt!\"', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"text\": \"Of course!\\\\n\\\\nWhat did the number 0 say to the number 8?\\\\n\\\\n\\\\\"Nice belt!\\\\\"\"}', 'signature': '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'}], provider_specific_fields=None)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0678268a",
   "metadata": {},
   "source": [
    "### Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60942eeb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This image features an adorable **puppy** peeking out from behind a bush of **purple flowers**.\n",
       "\n",
       "Here's a more detailed breakdown:\n",
       "\n",
       "*   **Main Subject:** A small, fluffy puppy, likely a Cavalier King Charles Spaniel or similar breed, with white fur and large patches of reddish-brown/tan on its floppy ears and around its eyes. It's lying down on green grass, looking directly at the viewer with big, dark, expressive eyes.\n",
       "*   **Foreground/Left:** A lush green bush covered in numerous small, delicate light purple or lavender flowers. The puppy is positioned as if it's emerging or hiding behind this bush.\n",
       "*   **Ground:** The puppy is resting on vibrant green grass.\n",
       "*   **Background (Right):** The background is softly blurred, showing hints of darker, possibly wooden, structures or furniture, suggesting an outdoor garden or patio setting.\n",
       "\n",
       "The overall impression is one of cuteness and natural beauty.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=998, prompt_tokens=264, total_tokens=1262, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=797, rejected_prediction_tokens=None, text_tokens=201, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=6, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"This image features an adorable **puppy** peeking out from behind a bush of **purple flowers**.\\n\\nHere's a more detailed breakdown:\\n\\n*   **Main Subject:** A small, fluffy puppy, likely a Cavalier King Charles Spaniel or similar breed, with white fur and large patches of reddish-brown/tan on its floppy ears and around its eyes. It's lying down on green grass, looking directly at the viewer with big, dark, expressive eyes.\\n*   **Foreground/Left:** A lush green bush covered in numerous small, delicate light purple or lavender flowers. The puppy is positioned as if it's emerging or hiding behind this bush.\\n*   **Ground:** The puppy is resting on vibrant green grass.\\n*   **Background (Right):** The background is softly blurred, showing hints of darker, possibly wooden, structures or furniture, suggesting an outdoor garden or patio setting.\\n\\nThe overall impression is one of cuteness and natural beauty.\", role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=998, prompt_tokens=264, total_tokens=1262, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=797, rejected_prediction_tokens=None, text_tokens=201, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=6, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(ms[2])\n",
    "chat(['Whats in this img?',img_fn.read_bytes()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43244a6",
   "metadata": {},
   "source": [
    "### Prefill"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb4fe330",
   "metadata": {},
   "source": [
    "Prefill works as expected:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c034582f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I can't spell your name because I don't know what it is!\n",
       "\n",
       "If you tell me your name, I'd be happy to spell it for you.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=323, prompt_tokens=4, total_tokens=327, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=287, rejected_prediction_tokens=None, text_tokens=36, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I can't spell your name because I don't know what it is!\\n\\nIf you tell me your name, I'd be happy to spell it for you.\", role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=323, prompt_tokens=4, total_tokens=327, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=287, rejected_prediction_tokens=None, text_tokens=36, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(ms[2])\n",
    "chat(\"Spell my name\",prefill=\"Your name is R E\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac64b334",
   "metadata": {},
   "source": [
    "And the entire message is stored in the history, not just the generated part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbf54ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chat.hist[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46716033",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02c7ab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe74496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1, 2, 3, 4, 5"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1, 2, 3, 4, 5\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=39, prompt_tokens=5, total_tokens=44, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='1, 2, 3, 4, 5', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=39, prompt_tokens=5, total_tokens=44, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Chat(ms[2])\n",
    "stream_gen = chat(\"Count to 5\", stream=True)\n",
    "for chunk in stream_gen:\n",
    "    if isinstance(chunk, ModelResponse): display(chunk)\n",
    "    else: print(delta_text(chunk) or '',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558e55c0",
   "metadata": {},
   "source": [
    "Lets try prefill with streaming too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "834c058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stream_gen = chat(\"Continue counting to 10\",\"Okay! 6, 7\",stream=True)\n",
    "# for chunk in stream_gen:\n",
    "#     if isinstance(chunk, ModelResponse): display(chunk)\n",
    "#     else: print(delta_text(chunk) or '',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c3666",
   "metadata": {},
   "source": [
    "### Tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17377a",
   "metadata": {},
   "source": [
    "Ok now lets test tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e9b6d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemini/gemini-3-pro-preview',\n",
       " 'gemini/gemini-2.5-pro',\n",
       " 'gemini/gemini-2.5-flash',\n",
       " 'claude-sonnet-4-5',\n",
       " 'openai/gpt-4.1']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4cf429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**gemini/gemini-2.5-pro:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I have successfully completed the goal.\n",
       "\n",
       "To find the sum of 5 + 3, I used the `simple_add` tool. I provided the inputs `a=5` and `b=3`. The tool processed these numbers and returned the result, which is 8.\n",
       "\n",
       "Therefore, 5 + 3 = 8.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=787, prompt_tokens=214, total_tokens=1001, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=716, rejected_prediction_tokens=None, text_tokens=71, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=214, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='I have successfully completed the goal.\\n\\nTo find the sum of 5 + 3, I used the `simple_add` tool. I provided the inputs `a=5` and `b=3`. The tool processed these numbers and returned the result, which is 8.\\n\\nTherefore, 5 + 3 = 8.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=787, prompt_tokens=214, total_tokens=1001, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=716, rejected_prediction_tokens=None, text_tokens=71, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=214, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**gemini/gemini-2.5-flash:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I used the `simple_add` tool to calculate 5 + 3.\n",
       "The tool returned the result 8.\n",
       "\n",
       "Therefore, 5 + 3 = 8.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=133, prompt_tokens=160, total_tokens=293, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=95, rejected_prediction_tokens=None, text_tokens=38, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=160, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='I used the `simple_add` tool to calculate 5 + 3.\\nThe tool returned the result 8.\\n\\nTherefore, 5 + 3 = 8.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=133, prompt_tokens=160, total_tokens=293, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=95, rejected_prediction_tokens=None, text_tokens=38, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=160, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**claude-sonnet-4-5:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "## Summary\n",
       "\n",
       "I successfully completed the calculation using the `simple_add` tool.\n",
       "\n",
       "**Result: 5 + 3 = 8**\n",
       "\n",
       "**Explanation:**\n",
       "The `simple_add` function took two parameters:\n",
       "- `a = 5` (the first operand)\n",
       "- `b = 3` (the second operand)\n",
       "\n",
       "The function performed the addition operation and returned **8** as the result.\n",
       "\n",
       "The goal has been fully completed - no further work is needed.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=110, prompt_tokens=770, total_tokens=880, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='## Summary\\n\\nI successfully completed the calculation using the `simple_add` tool.\\n\\n**Result: 5 + 3 = 8**\\n\\n**Explanation:**\\nThe `simple_add` function took two parameters:\\n- `a = 5` (the first operand)\\n- `b = 3` (the second operand)\\n\\nThe function performed the addition operation and returned **8** as the result.\\n\\nThe goal has been fully completed - no further work is needed.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=110, prompt_tokens=770, total_tokens=880, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "**openai/gpt-4.1:**"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "I used the simple_add tool to calculate 5 + 3, and the result is 8.\n",
       "\n",
       "Summary:\n",
       "- I successfully completed the calculation using the tool.\n",
       "- 5 + 3 = 8.\n",
       "\n",
       "No further work is needed for this task. If you have more calculations or questions, please let me know!\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=65, prompt_tokens=156, total_tokens=221, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_09249d7c7b', choices=[Choices(finish_reason='stop', index=0, message=Message(content='I used the simple_add tool to calculate 5 + 3, and the result is 8.\\n\\nSummary:\\n- I successfully completed the calculation using the tool.\\n- 5 + 3 = 8.\\n\\nNo further work is needed for this task. If you have more calculations or questions, please let me know!', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=65, prompt_tokens=156, total_tokens=221, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for m in ms[1:]:\n",
    "    display(Markdown(f'**{m}:**'))\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's 5 + 3? Use  the `simple_add` tool. Explain.\")\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f62a8",
   "metadata": {},
   "source": [
    "### Thinking w tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fe375b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"b\": 3, \"a\": 5})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=161, prompt_tokens=74, total_tokens=235, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=141, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=74, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'index': 0, 'provider_specific_fields': {'thought_signature': 'CuADAXLI2nyWO0CQDly8P3vsYo3Yc3eyV4gDjYb9yvQq7t//BS2bq6LWPXkVwXQTIju4kxHkgd4HdjaBPmAY5qxePsaO+31uqUH2jPAQBjn2pQxigo91xFkIyHXWgquKKmLCWRmr3ozZ33OHRgc82C4/KxoIFqL2VQHh9k5Gz92yk4OvQbIj+BCw0F88lG/t/Azh1VmLBJfai1tgdSP2AeXOmxqvW7do7hwqXu/THDwajeHSk4t2pb2s92QVhumfcXTEuDvcjZh25BeTDVmtxRKSSGS4rFzxJIN5e+J1F9Gw28b658P7XXjU/JAOzv397F3vePR5bdLH0m0MEZZcZBog61E3P/HW2dUUnVbRemKfFvWxg4GIzXTmEZ2PlvIEhvJyApQwbED3BakRNOznb34qwBODtT3Sni642obxsLym3+SPyzwh8qapiZmjtxfIVZJ7Y6+TTQkw3c9lCjiksxqnmaA6XTtNpShsBx8qjGLHSduJkpzoeLHJWE9jZZNeKonvuH5m78EvCu8BeDi0z/LbaERbs/XCpvOfaqtOSfrvnrCzsF1ZnTgDDIv5V/+dbPxiQZaf8hqqjEVuhPlJ/jSL98qjDdBZzogunT2It7WuB+1wxVMESmE0OjZveJt/+eMD'}, 'function': {'arguments': '{\"b\": 3, \"a\": 5}', 'name': 'simple_add'}, 'id': 'call_b7ae92a91f8840e4b3f45a685dae__thought__CuADAXLI2nyWO0CQDly8P3vsYo3Yc3eyV4gDjYb9yvQq7t//BS2bq6LWPXkVwXQTIju4kxHkgd4HdjaBPmAY5qxePsaO+31uqUH2jPAQBjn2pQxigo91xFkIyHXWgquKKmLCWRmr3ozZ33OHRgc82C4/KxoIFqL2VQHh9k5Gz92yk4OvQbIj+BCw0F88lG/t/Azh1VmLBJfai1tgdSP2AeXOmxqvW7do7hwqXu/THDwajeHSk4t2pb2s92QVhumfcXTEuDvcjZh25BeTDVmtxRKSSGS4rFzxJIN5e+J1F9Gw28b658P7XXjU/JAOzv397F3vePR5bdLH0m0MEZZcZBog61E3P/HW2dUUnVbRemKfFvWxg4GIzXTmEZ2PlvIEhvJyApQwbED3BakRNOznb34qwBODtT3Sni642obxsLym3+SPyzwh8qapiZmjtxfIVZJ7Y6+TTQkw3c9lCjiksxqnmaA6XTtNpShsBx8qjGLHSduJkpzoeLHJWE9jZZNeKonvuH5m78EvCu8BeDi0z/LbaERbs/XCpvOfaqtOSfrvnrCzsF1ZnTgDDIv5V/+dbPxiQZaf8hqqjEVuhPlJ/jSL98qjDdBZzogunT2It7WuB+1wxVMESmE0OjZveJt/+eMD', 'type': 'function'}], function_call=None, images=[], reasoning_content='**My Reasoning on this Simple Addition Problem**\\n\\nOkay, so I\\'m looking at this problem and it\\'s pretty straightforward.  First, the user\\'s asking about addition, which immediately points me towards the `simple_add` tool. Seems like a good fit for this task.\\n\\nNext, I need to understand the inputs. The user gives me two numbers: 5 and 3.  I quickly see that `simple_add` takes two parameters, conveniently named `a` and `b`.  I can easily map the user\\'s \"5\" to the parameter `a` and the user\\'s \"3\" to the parameter `b`.\\n\\nFinally, to get the answer, I need to execute the addition.  I know I can call the function with the assigned parameters.  So I\\'ll just formulate the correct function call: `print(simple_add(a=5, b=3))` should do the trick.  That\\'ll give me the sum.  Easy peasy.\\n', thinking_blocks=[{'type': 'thinking', 'thinking': '{\"functionCall\": {\"name\": \"simple_add\", \"args\": {\"b\": 3, \"a\": 5}}}', 'signature': 'CuADAXLI2nyWO0CQDly8P3vsYo3Yc3eyV4gDjYb9yvQq7t//BS2bq6LWPXkVwXQTIju4kxHkgd4HdjaBPmAY5qxePsaO+31uqUH2jPAQBjn2pQxigo91xFkIyHXWgquKKmLCWRmr3ozZ33OHRgc82C4/KxoIFqL2VQHh9k5Gz92yk4OvQbIj+BCw0F88lG/t/Azh1VmLBJfai1tgdSP2AeXOmxqvW7do7hwqXu/THDwajeHSk4t2pb2s92QVhumfcXTEuDvcjZh25BeTDVmtxRKSSGS4rFzxJIN5e+J1F9Gw28b658P7XXjU/JAOzv397F3vePR5bdLH0m0MEZZcZBog61E3P/HW2dUUnVbRemKfFvWxg4GIzXTmEZ2PlvIEhvJyApQwbED3BakRNOznb34qwBODtT3Sni642obxsLym3+SPyzwh8qapiZmjtxfIVZJ7Y6+TTQkw3c9lCjiksxqnmaA6XTtNpShsBx8qjGLHSduJkpzoeLHJWE9jZZNeKonvuH5m78EvCu8BeDi0z/LbaERbs/XCpvOfaqtOSfrvnrCzsF1ZnTgDDIv5V/+dbPxiQZaf8hqqjEVuhPlJ/jSL98qjDdBZzogunT2It7WuB+1wxVMESmE0OjZveJt/+eMD'}], provider_specific_fields=None))], usage=Usage(completion_tokens=161, prompt_tokens=74, total_tokens=235, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=141, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=74, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_b7ae92a91f8840e4b3f45a685dae__thought__CuADAXLI2nyWO0CQDly8P3vsYo3Yc3eyV4gDjYb9yvQq7t//BS2bq6LWPXkVwXQTIju4kxHkgd4HdjaBPmAY5qxePsaO+31uqUH2jPAQBjn2pQxigo91xFkIyHXWgquKKmLCWRmr3ozZ33OHRgc82C4/KxoIFqL2VQHh9k5Gz92yk4OvQbIj+BCw0F88lG/t/Azh1VmLBJfai1tgdSP2AeXOmxqvW7do7hwqXu/THDwajeHSk4t2pb2s92QVhumfcXTEuDvcjZh25BeTDVmtxRKSSGS4rFzxJIN5e+J1F9Gw28b658P7XXjU/JAOzv397F3vePR5bdLH0m0MEZZcZBog61E3P/HW2dUUnVbRemKfFvWxg4GIzXTmEZ2PlvIEhvJyApQwbED3BakRNOznb34qwBODtT3Sni642obxsLym3+SPyzwh8qapiZmjtxfIVZJ7Y6+TTQkw3c9lCjiksxqnmaA6XTtNpShsBx8qjGLHSduJkpzoeLHJWE9jZZNeKonvuH5m78EvCu8BeDi0z/LbaERbs/XCpvOfaqtOSfrvnrCzsF1ZnTgDDIv5V/+dbPxiQZaf8hqqjEVuhPlJ/jSL98qjDdBZzogunT2It7WuB+1wxVMESmE0OjZveJt/+eMD',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '8'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Based on my tool use, the sum of 5 and 3 is 8.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=476, prompt_tokens=361, total_tokens=837, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=458, rejected_prediction_tokens=None, text_tokens=18, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=361, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Based on my tool use, the sum of 5 and 3 is 8.', role='assistant', tool_calls=None, function_call=None, images=[], reasoning_content='**Summarizing My Reasoning**\\n\\nOkay, so the user wants a summary of what just happened. Let\\'s see... first, I need to recognize the user\\'s implicit request for a recap. They want a concise explanation of how I arrived at the answer. Let\\'s dig in. I see the user asked \"What\\'s 5 + 3?\" My call to the `simple_add` tool was quite straightforward: `simple_add(a=5, b=3)`, and the tool spit back `8`.\\n\\nMy analysis is simple: the user presented an elementary math problem. I leveraged a dedicated tool, the `simple_add` function, to do the heavy lifting of the computation. That function then took the input `5` and `3`, and the output was `8`, which is the correct sum.\\n\\nNow to formulate the summary.  Since this is an expert level interaction, I\\'m thinking the user wants a no-nonsense, technically correct summary.  The core information to convey is the question, the tool used, and the result. However, the tool use isn\\'t strictly necessary here.  Given the brevity of the process, a concise statement like \"The sum of 5 and 3 is 8\" is the ideal answer. This is direct, accurate, and completely fulfills the objective. Plus, the goal was achieved, so no further explanation is necessary.\\n', thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=476, prompt_tokens=361, total_tokens=837, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=458, rejected_prediction_tokens=None, text_tokens=18, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=361, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Chat(ms[1], tools=[simple_add])\n",
    "res = chat(\"What's 5 + 3?\",think='l',return_all=True)\n",
    "display(*res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21fcaf1d",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9a499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Otters are carnivorous mammals known for their playful behavior and adaptations to a semi-aquatic life. A member of the weasel family, there are 13 different species of otters found in various aquatic habitats around the world.\n",
      "\n",
      "Key characteristics of otters include their long, slender bodies, short legs with powerful webbed feet for swimming, and a strong tail that helps them move through the water. They are also distinguished by their very dense fur, which traps air to keep them warm and buoyant, as they lack a layer of blubber for insulation like other marine mammals. In fact, sea otters have the thickest fur of any animal.\n",
      "\n",
      "Their diet consists mainly of fish, but can also include frogs, crayfish, and crabs. Otters are a keystone species, meaning they play a critical role in their ecosystem, such as controlling sea urchin populations which in turn protects kelp forests.\n",
      "\n",
      "Otters can be found in a variety of environments, including freshwater rivers, lakes, and marshes, as well as coastal marine habitats. They build dens, known as holts, in riverbanks or under tree roots. While some otter populations have faced declines, reintroduction programs have been successful in some areas."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Otters are carnivorous mammals known for their playful behavior and adaptations to a semi-aquatic life. A member of the weasel family, there are 13 different species of otters found in various aquatic habitats around the world.\n",
       "\n",
       "Key characteristics of otters include their long, slender bodies, short legs with powerful webbed feet for swimming, and a strong tail that helps them move through the water. They are also distinguished by their very dense fur, which traps air to keep them warm and buoyant, as they lack a layer of blubber for insulation like other marine mammals. In fact, sea otters have the thickest fur of any animal.\n",
       "\n",
       "Their diet consists mainly of fish, but can also include frogs, crayfish, and crabs. Otters are a keystone species, meaning they play a critical role in their ecosystem, such as controlling sea urchin populations which in turn protects kelp forests.\n",
       "\n",
       "Otters can be found in a variety of environments, including freshwater rivers, lakes, and marshes, as well as coastal marine habitats. They build dens, known as holts, in riverbanks or under tree roots. While some otter populations have faced declines, reintroduction programs have been successful in some areas.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=411, prompt_tokens=12, total_tokens=423, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Otters are carnivorous mammals known for their playful behavior and adaptations to a semi-aquatic life. A member of the weasel family, there are 13 different species of otters found in various aquatic habitats around the world.\\n\\nKey characteristics of otters include their long, slender bodies, short legs with powerful webbed feet for swimming, and a strong tail that helps them move through the water. They are also distinguished by their very dense fur, which traps air to keep them warm and buoyant, as they lack a layer of blubber for insulation like other marine mammals. In fact, sea otters have the thickest fur of any animal.\\n\\nTheir diet consists mainly of fish, but can also include frogs, crayfish, and crabs. Otters are a keystone species, meaning they play a critical role in their ecosystem, such as controlling sea urchin populations which in turn protects kelp forests.\\n\\nOtters can be found in a variety of environments, including freshwater rivers, lakes, and marshes, as well as coastal marine habitats. They build dens, known as holts, in riverbanks or under tree roots. While some otter populations have faced declines, reintroduction programs have been successful in some areas.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, annotations=[{'type': 'url_citation', 'url_citation': {'start_index': 104, 'end_index': 227, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5_gxxY7d1_3GK3lPurE3-AI0487RmhHqt3hzDN1NnYk2b3DScF7_6Wi3_-O4c_HGoczctHH_VwM7_iyUjTRA7B3Dtslpi5Vz7aqB5S_M3sPStnh--hqj_dH9SL7Q8JWOgYr0MtHkSjYqxmu1NfN1tWB1PDK8vnS3doYEVah_PKao=', 'title': 'doi.gov'}}, {'type': 'url_citation', 'url_citation': {'start_index': 229, 'end_index': 403, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGkX41Wyp8epNwOW65lhQ-7iOla1ORY5hBQSIxt0oY6IpBM9omX0XKJTLzkuEHJBvQBjlsXLISjX7m-dKXwpMC1VaYUmawxMP6fJ7RucjDEhRvxEzOvWkK9DIYMvws=', 'title': 'wikipedia.org'}}, {'type': 'url_citation', 'url_citation': {'start_index': 404, 'end_index': 577, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGkX41Wyp8epNwOW65lhQ-7iOla1ORY5hBQSIxt0oY6IpBM9omX0XKJTLzkuEHJBvQBjlsXLISjX7m-dKXwpMC1VaYUmawxMP6fJ7RucjDEhRvxEzOvWkK9DIYMvws=', 'title': 'wikipedia.org'}}, {'type': 'url_citation', 'url_citation': {'start_index': 578, 'end_index': 634, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5_gxxY7d1_3GK3lPurE3-AI0487RmhHqt3hzDN1NnYk2b3DScF7_6Wi3_-O4c_HGoczctHH_VwM7_iyUjTRA7B3Dtslpi5Vz7aqB5S_M3sPStnh--hqj_dH9SL7Q8JWOgYr0MtHkSjYqxmu1NfN1tWB1PDK8vnS3doYEVah_PKao=', 'title': 'doi.gov'}}, {'type': 'url_citation', 'url_citation': {'start_index': 636, 'end_index': 720, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGkX41Wyp8epNwOW65lhQ-7iOla1ORY5hBQSIxt0oY6IpBM9omX0XKJTLzkuEHJBvQBjlsXLISjX7m-dKXwpMC1VaYUmawxMP6fJ7RucjDEhRvxEzOvWkK9DIYMvws=', 'title': 'wikipedia.org'}}, {'type': 'url_citation', 'url_citation': {'start_index': 721, 'end_index': 885, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQH5_gxxY7d1_3GK3lPurE3-AI0487RmhHqt3hzDN1NnYk2b3DScF7_6Wi3_-O4c_HGoczctHH_VwM7_iyUjTRA7B3Dtslpi5Vz7aqB5S_M3sPStnh--hqj_dH9SL7Q8JWOgYr0MtHkSjYqxmu1NfN1tWB1PDK8vnS3doYEVah_PKao=', 'title': 'doi.gov'}}, {'type': 'url_citation', 'url_citation': {'start_index': 887, 'end_index': 1021, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqQTGheYeURsOfADjIFXqIczhpXYC1zvv5OnfQYNGtQlwXHxG1eHC3zdc5CybwdYz5lu9uJ-jbgbZ0kf1oxxSTqO2It1mHso5uAWtOwtv5wyvyXGtAj7bp2k8gfNQh99jypyPDAsqgdTTxErPVZ-9yZ_p3PTqY7Q==', 'title': 'crittercarewildlife.org'}}, {'type': 'url_citation', 'url_citation': {'start_index': 1022, 'end_index': 1089, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqQTGheYeURsOfADjIFXqIczhpXYC1zvv5OnfQYNGtQlwXHxG1eHC3zdc5CybwdYz5lu9uJ-jbgbZ0kf1oxxSTqO2It1mHso5uAWtOwtv5wyvyXGtAj7bp2k8gfNQh99jypyPDAsqgdTTxErPVZ-9yZ_p3PTqY7Q==', 'title': 'crittercarewildlife.org'}}, {'type': 'url_citation', 'url_citation': {'start_index': 1090, 'end_index': 1199, 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHqQTGheYeURsOfADjIFXqIczhpXYC1zvv5OnfQYNGtQlwXHxG1eHC3zdc5CybwdYz5lu9uJ-jbgbZ0kf1oxxSTqO2It1mHso5uAWtOwtv5wyvyXGtAj7bp2k8gfNQh99jypyPDAsqgdTTxErPVZ-9yZ_p3PTqY7Q==', 'title': 'crittercarewildlife.org'}}]))], usage=Usage(completion_tokens=411, prompt_tokens=12, total_tokens=423, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Chat(ms[1])\n",
    "res = chat(\"Search the web and tell me very briefly about otters\", search='l', stream=True)\n",
    "for o in res:\n",
    "    if isinstance(o, ModelResponse): sleep(0.01); display(o)\n",
    "    else: print(delta_text(o) or '',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cce2d9",
   "metadata": {},
   "source": [
    "### Multi tool calling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb42380",
   "metadata": {},
   "source": [
    "We can let the model call multiple tools in sequence using the `max_steps` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f7aa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"b\": 3, \"a\": 5})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=421, prompt_tokens=83, total_tokens=504, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=401, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=83, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'index': 0, 'provider_specific_fields': {'thought_signature': 'Cq8JAXLI2nyI1OmWr9kdtN0PS13ZhCWAfCRxf4MjaoInzCR/bwzqgYAwIMWbSpvvcpsFOeCp0BVKgD612WkQS5UYEsLGebguKY7QxSm/sKX+hVaRfySu/INlef29rks6OJ6Jl7KyH4ux+v0h0RoRtv5mPARXfQArRpO4Z41Mp51KSqJY7f4HcYdfVt3ORp1IZPBFfZ5YTvtyT194V1FWhzRxc5fagOwt8LP22c+6ppmE8tNQMojPgX3KbaMIEisu5FRXd09A43JEUOz2hdLTijvbwvGtDtVLcujOSgJ88sIZr7q8cQBYNXV+jygwYH60XakR+zifT6vdzzYF0QQTXT4fCtriZdxRLh63alrvbVQHTTBHwhrQjz5pgMZk3IvgoFpsZbjhfBadJIUC0k5mvFCkQ/ehQdUgnNlzIy57+PchqAnpeoZ/L3ZpGdi3C3wwrcjwj5nxu9lktSWlv7A4XzC7SWMMQIfGVbMuCEOLcOaJlri2ZWqVBepG+eY5qYZ6wfiyx/XF4VyWPHhnmmqX3X97zBulVf7wRWG0yeiTHdPAbPH6s8H3qrbVdJvX1TEZdt5nMDgCxa7hXiMc9uLfE1UDMQS/h4I01Pe8gCn9feHZXizrk3GP2FMfkdO1v0vp3xjzDroGzbWe75ecqrH+dD1zpwC3dBL3kgi3e2wJsiKars+Vjnd1VRhMyylgh+q1bDo1q82NzkeXmrnRpjEE99IYxlAL8id89BVv2t2fMuBxu740uFjNW/pNlg47Yv78XM0oscqfgbj2jtm2ulBRqb7gKtoaDMeyhhzxms1medlJJ2TLSptzJuNEdrPpumKOlE0lDnYfU58QPp0fM7tGp/cApMC1nLkSsLKyra9Y76G4nTA8nTlYt4+C1a1i1rsZ3BLorfEhl0+jo/3yWxxlB2gisk5W5kwZSPZgkkFjYARadruhc8spE+VV8BOw7A548HDxvlm/kFe9AXfIx+sckdYu+r112QcJq77KL5fZAQlCGOT1h33jIW+Z0HmIlSMyIrn0eCQUwZH3s3e7unFIyKKI2DwF5huUAhFgWf8NdCqNxxuMs3wFPWw1oI6h8iMwbSUyejMc/V5gswDUU1I7Q+9LMRq5C2mrPjAQX8PdcngApiSzbODXmIPSKeXjctC0mlDZ5v622LRsgA++UDQ5L+MPpqJRzaVMXL0VsK8OQzpBpZ81dhV2QxdsHyd1vqjQ772sqF/AFmW0FBZyWooXEKMK063PBLBlTMTYT56b9C7FiA6JhqjgZkSiF+7UIuBDuAot7r/eayENYEBbI8IiDbqmvHBLqDAcnV9Wdw4iLlZzcqj/c/h4/R9NoSxOrAhaZuTwpGrZsbCaUCou7KTST4ASdvLeB1jk2h96dNLQw5ns1IHkmOVVf0kC0kVzcvIzKB3M7UpbOOCFDnsjODLNQqL3RLSwj/b+l03X6U55AViorZe9KipS59/X8YKMKQygPEY6a1d4y5EP0yLHJWPARA4Qc/iWYWA1gJAvprYchS1OoiF+2gPCFUmmTL30tbVJ1dsf7SGaGgGiPAFWvWP4/nRGG2Xu9XlwW1ZhSpU8XRMkKOVLRp30jsObCZXMkkc8a9U='}, 'function': {'arguments': '{\"b\": 3, \"a\": 5}', 'name': 'simple_add'}, 'id': 'call_47199466713f4457b9226d953235__thought__Cq8JAXLI2nyI1OmWr9kdtN0PS13ZhCWAfCRxf4MjaoInzCR/bwzqgYAwIMWbSpvvcpsFOeCp0BVKgD612WkQS5UYEsLGebguKY7QxSm/sKX+hVaRfySu/INlef29rks6OJ6Jl7KyH4ux+v0h0RoRtv5mPARXfQArRpO4Z41Mp51KSqJY7f4HcYdfVt3ORp1IZPBFfZ5YTvtyT194V1FWhzRxc5fagOwt8LP22c+6ppmE8tNQMojPgX3KbaMIEisu5FRXd09A43JEUOz2hdLTijvbwvGtDtVLcujOSgJ88sIZr7q8cQBYNXV+jygwYH60XakR+zifT6vdzzYF0QQTXT4fCtriZdxRLh63alrvbVQHTTBHwhrQjz5pgMZk3IvgoFpsZbjhfBadJIUC0k5mvFCkQ/ehQdUgnNlzIy57+PchqAnpeoZ/L3ZpGdi3C3wwrcjwj5nxu9lktSWlv7A4XzC7SWMMQIfGVbMuCEOLcOaJlri2ZWqVBepG+eY5qYZ6wfiyx/XF4VyWPHhnmmqX3X97zBulVf7wRWG0yeiTHdPAbPH6s8H3qrbVdJvX1TEZdt5nMDgCxa7hXiMc9uLfE1UDMQS/h4I01Pe8gCn9feHZXizrk3GP2FMfkdO1v0vp3xjzDroGzbWe75ecqrH+dD1zpwC3dBL3kgi3e2wJsiKars+Vjnd1VRhMyylgh+q1bDo1q82NzkeXmrnRpjEE99IYxlAL8id89BVv2t2fMuBxu740uFjNW/pNlg47Yv78XM0oscqfgbj2jtm2ulBRqb7gKtoaDMeyhhzxms1medlJJ2TLSptzJuNEdrPpumKOlE0lDnYfU58QPp0fM7tGp/cApMC1nLkSsLKyra9Y76G4nTA8nTlYt4+C1a1i1rsZ3BLorfEhl0+jo/3yWxxlB2gisk5W5kwZSPZgkkFjYARadruhc8spE+VV8BOw7A548HDxvlm/kFe9AXfIx+sckdYu+r112QcJq77KL5fZAQlCGOT1h33jIW+Z0HmIlSMyIrn0eCQUwZH3s3e7unFIyKKI2DwF5huUAhFgWf8NdCqNxxuMs3wFPWw1oI6h8iMwbSUyejMc/V5gswDUU1I7Q+9LMRq5C2mrPjAQX8PdcngApiSzbODXmIPSKeXjctC0mlDZ5v622LRsgA++UDQ5L+MPpqJRzaVMXL0VsK8OQzpBpZ81dhV2QxdsHyd1vqjQ772sqF/AFmW0FBZyWooXEKMK063PBLBlTMTYT56b9C7FiA6JhqjgZkSiF+7UIuBDuAot7r/eayENYEBbI8IiDbqmvHBLqDAcnV9Wdw4iLlZzcqj/c/h4/R9NoSxOrAhaZuTwpGrZsbCaUCou7KTST4ASdvLeB1jk2h96dNLQw5ns1IHkmOVVf0kC0kVzcvIzKB3M7UpbOOCFDnsjODLNQqL3RLSwj/b+l03X6U55AViorZe9KipS59/X8YKMKQygPEY6a1d4y5EP0yLHJWPARA4Qc/iWYWA1gJAvprYchS1OoiF+2gPCFUmmTL30tbVJ1dsf7SGaGgGiPAFWvWP4/nRGG2Xu9XlwW1ZhSpU8XRMkKOVLRp30jsObCZXMkkc8a9U=', 'type': 'function'}], function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"functionCall\": {\"name\": \"simple_add\", \"args\": {\"b\": 3, \"a\": 5}}}', 'signature': 'Cq8JAXLI2nyI1OmWr9kdtN0PS13ZhCWAfCRxf4MjaoInzCR/bwzqgYAwIMWbSpvvcpsFOeCp0BVKgD612WkQS5UYEsLGebguKY7QxSm/sKX+hVaRfySu/INlef29rks6OJ6Jl7KyH4ux+v0h0RoRtv5mPARXfQArRpO4Z41Mp51KSqJY7f4HcYdfVt3ORp1IZPBFfZ5YTvtyT194V1FWhzRxc5fagOwt8LP22c+6ppmE8tNQMojPgX3KbaMIEisu5FRXd09A43JEUOz2hdLTijvbwvGtDtVLcujOSgJ88sIZr7q8cQBYNXV+jygwYH60XakR+zifT6vdzzYF0QQTXT4fCtriZdxRLh63alrvbVQHTTBHwhrQjz5pgMZk3IvgoFpsZbjhfBadJIUC0k5mvFCkQ/ehQdUgnNlzIy57+PchqAnpeoZ/L3ZpGdi3C3wwrcjwj5nxu9lktSWlv7A4XzC7SWMMQIfGVbMuCEOLcOaJlri2ZWqVBepG+eY5qYZ6wfiyx/XF4VyWPHhnmmqX3X97zBulVf7wRWG0yeiTHdPAbPH6s8H3qrbVdJvX1TEZdt5nMDgCxa7hXiMc9uLfE1UDMQS/h4I01Pe8gCn9feHZXizrk3GP2FMfkdO1v0vp3xjzDroGzbWe75ecqrH+dD1zpwC3dBL3kgi3e2wJsiKars+Vjnd1VRhMyylgh+q1bDo1q82NzkeXmrnRpjEE99IYxlAL8id89BVv2t2fMuBxu740uFjNW/pNlg47Yv78XM0oscqfgbj2jtm2ulBRqb7gKtoaDMeyhhzxms1medlJJ2TLSptzJuNEdrPpumKOlE0lDnYfU58QPp0fM7tGp/cApMC1nLkSsLKyra9Y76G4nTA8nTlYt4+C1a1i1rsZ3BLorfEhl0+jo/3yWxxlB2gisk5W5kwZSPZgkkFjYARadruhc8spE+VV8BOw7A548HDxvlm/kFe9AXfIx+sckdYu+r112QcJq77KL5fZAQlCGOT1h33jIW+Z0HmIlSMyIrn0eCQUwZH3s3e7unFIyKKI2DwF5huUAhFgWf8NdCqNxxuMs3wFPWw1oI6h8iMwbSUyejMc/V5gswDUU1I7Q+9LMRq5C2mrPjAQX8PdcngApiSzbODXmIPSKeXjctC0mlDZ5v622LRsgA++UDQ5L+MPpqJRzaVMXL0VsK8OQzpBpZ81dhV2QxdsHyd1vqjQ772sqF/AFmW0FBZyWooXEKMK063PBLBlTMTYT56b9C7FiA6JhqjgZkSiF+7UIuBDuAot7r/eayENYEBbI8IiDbqmvHBLqDAcnV9Wdw4iLlZzcqj/c/h4/R9NoSxOrAhaZuTwpGrZsbCaUCou7KTST4ASdvLeB1jk2h96dNLQw5ns1IHkmOVVf0kC0kVzcvIzKB3M7UpbOOCFDnsjODLNQqL3RLSwj/b+l03X6U55AViorZe9KipS59/X8YKMKQygPEY6a1d4y5EP0yLHJWPARA4Qc/iWYWA1gJAvprYchS1OoiF+2gPCFUmmTL30tbVJ1dsf7SGaGgGiPAFWvWP4/nRGG2Xu9XlwW1ZhSpU8XRMkKOVLRp30jsObCZXMkkc8a9U='}], provider_specific_fields=None))], usage=Usage(completion_tokens=421, prompt_tokens=83, total_tokens=504, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=401, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=83, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_47199466713f4457b9226d953235__thought__Cq8JAXLI2nyI1OmWr9kdtN0PS13ZhCWAfCRxf4MjaoInzCR/bwzqgYAwIMWbSpvvcpsFOeCp0BVKgD612WkQS5UYEsLGebguKY7QxSm/sKX+hVaRfySu/INlef29rks6OJ6Jl7KyH4ux+v0h0RoRtv5mPARXfQArRpO4Z41Mp51KSqJY7f4HcYdfVt3ORp1IZPBFfZ5YTvtyT194V1FWhzRxc5fagOwt8LP22c+6ppmE8tNQMojPgX3KbaMIEisu5FRXd09A43JEUOz2hdLTijvbwvGtDtVLcujOSgJ88sIZr7q8cQBYNXV+jygwYH60XakR+zifT6vdzzYF0QQTXT4fCtriZdxRLh63alrvbVQHTTBHwhrQjz5pgMZk3IvgoFpsZbjhfBadJIUC0k5mvFCkQ/ehQdUgnNlzIy57+PchqAnpeoZ/L3ZpGdi3C3wwrcjwj5nxu9lktSWlv7A4XzC7SWMMQIfGVbMuCEOLcOaJlri2ZWqVBepG+eY5qYZ6wfiyx/XF4VyWPHhnmmqX3X97zBulVf7wRWG0yeiTHdPAbPH6s8H3qrbVdJvX1TEZdt5nMDgCxa7hXiMc9uLfE1UDMQS/h4I01Pe8gCn9feHZXizrk3GP2FMfkdO1v0vp3xjzDroGzbWe75ecqrH+dD1zpwC3dBL3kgi3e2wJsiKars+Vjnd1VRhMyylgh+q1bDo1q82NzkeXmrnRpjEE99IYxlAL8id89BVv2t2fMuBxu740uFjNW/pNlg47Yv78XM0oscqfgbj2jtm2ulBRqb7gKtoaDMeyhhzxms1medlJJ2TLSptzJuNEdrPpumKOlE0lDnYfU58QPp0fM7tGp/cApMC1nLkSsLKyra9Y76G4nTA8nTlYt4+C1a1i1rsZ3BLorfEhl0+jo/3yWxxlB2gisk5W5kwZSPZgkkFjYARadruhc8spE+VV8BOw7A548HDxvlm/kFe9AXfIx+sckdYu+r112QcJq77KL5fZAQlCGOT1h33jIW+Z0HmIlSMyIrn0eCQUwZH3s3e7unFIyKKI2DwF5huUAhFgWf8NdCqNxxuMs3wFPWw1oI6h8iMwbSUyejMc/V5gswDUU1I7Q+9LMRq5C2mrPjAQX8PdcngApiSzbODXmIPSKeXjctC0mlDZ5v622LRsgA++UDQ5L+MPpqJRzaVMXL0VsK8OQzpBpZ81dhV2QxdsHyd1vqjQ772sqF/AFmW0FBZyWooXEKMK063PBLBlTMTYT56b9C7FiA6JhqjgZkSiF+7UIuBDuAot7r/eayENYEBbI8IiDbqmvHBLqDAcnV9Wdw4iLlZzcqj/c/h4/R9NoSxOrAhaZuTwpGrZsbCaUCou7KTST4ASdvLeB1jk2h96dNLQw5ns1IHkmOVVf0kC0kVzcvIzKB3M7UpbOOCFDnsjODLNQqL3RLSwj/b+l03X6U55AViorZe9KipS59/X8YKMKQygPEY6a1d4y5EP0yLHJWPARA4Qc/iWYWA1gJAvprYchS1OoiF+2gPCFUmmTL30tbVJ1dsf7SGaGgGiPAFWvWP4/nRGG2Xu9XlwW1ZhSpU8XRMkKOVLRp30jsObCZXMkkc8a9U=',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '8'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"b\": 7, \"a\": 8})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=20, prompt_tokens=117, total_tokens=137, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=117, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'index': 0, 'function': {'arguments': '{\"b\": 7, \"a\": 8}', 'name': 'simple_add'}, 'id': 'call_51f19735530f4d9aaffe98c302d4', 'type': 'function'}], function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=20, prompt_tokens=117, total_tokens=137, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=117, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_51f19735530f4d9aaffe98c302d4',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '15'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"b\": 11, \"a\": 15})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=22, prompt_tokens=152, total_tokens=174, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=152, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'index': 0, 'function': {'arguments': '{\"b\": 11, \"a\": 15}', 'name': 'simple_add'}, 'id': 'call_1721fdfe888e4e4fa1d7ea82157b', 'type': 'function'}], function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=22, prompt_tokens=152, total_tokens=174, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=152, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_1721fdfe888e4e4fa1d7ea82157b',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '26'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "OK, let's break it down.\n",
       "\n",
       "First, we add 5 and 3, which equals 8.\n",
       "Then, we add 7 to that, which equals 15.\n",
       "Finally, we add 11 to that, which equals 26.\n",
       "\n",
       "Therefore, ((5 + 3)+7)+11 = 26.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=76, prompt_tokens=189, total_tokens=265, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=189, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"OK, let's break it down.\\n\\nFirst, we add 5 and 3, which equals 8.\\nThen, we add 7 to that, which equals 15.\\nFinally, we add 11 to that, which equals 26.\\n\\nTherefore, ((5 + 3)+7)+11 = 26.\", role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=76, prompt_tokens=189, total_tokens=265, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=189, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Chat(model, tools=[simple_add])\n",
    "res = chat(\"What's ((5 + 3)+7)+11? Work step by step\", return_all=True, max_steps=5)\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f66101",
   "metadata": {},
   "source": [
    "Some models support parallel tool calling. I.e. sending multiple tool call requests in one conversation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec77539",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 5, \"b\": 3})\n",
       "\n",
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 7, \"b\": 2})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=52, prompt_tokens=110, total_tokens=162, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_09249d7c7b', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"a\": 5, \"b\": 3}', 'name': 'simple_add'}, 'id': 'call_ZkjKhtlfgQlLS7sYaBogidfb', 'type': 'function'}, {'function': {'arguments': '{\"a\": 7, \"b\": 2}', 'name': 'simple_add'}, 'id': 'call_tno0OSF22ZQI3ShG1xFdGDxT', 'type': 'function'}], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=52, prompt_tokens=110, total_tokens=162, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_ZkjKhtlfgQlLS7sYaBogidfb',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '8'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_tno0OSF22ZQI3ShG1xFdGDxT',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '9'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ multiply({\"a\":8,\"b\":9})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=178, total_tokens=195, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_09249d7c7b', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'function': {'arguments': '{\"a\":8,\"b\":9}', 'name': 'multiply'}, 'id': 'call_osF7ih5fUnuMrWC0PAae8ik4', 'type': 'function'}], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=17, prompt_tokens=178, total_tokens=195, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_osF7ih5fUnuMrWC0PAae8ik4',\n",
       " 'role': 'tool',\n",
       " 'name': 'multiply',\n",
       " 'content': '72'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "(5 + 3) = 8 and (7 + 2) = 9. Multiplying them gives 8 Ã— 9 = 72. \n",
       "\n",
       "So, (5 + 3) * (7 + 2) = 72.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=54, prompt_tokens=203, total_tokens=257, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_09249d7c7b', choices=[Choices(finish_reason='stop', index=0, message=Message(content='(5 + 3) = 8 and (7 + 2) = 9. Multiplying them gives 8 Ã— 9 = 72. \\n\\nSo, (5 + 3) * (7 + 2) = 72.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=54, prompt_tokens=203, total_tokens=257, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"Multiply two numbers\"\n",
    "    return a * b\n",
    "\n",
    "chat = Chat('openai/gpt-4.1', tools=[simple_add, multiply])\n",
    "res = chat(\"Calculate (5 + 3) * (7 + 2)\", max_steps=5, return_all=True)\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3d9d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 5, \"b\": 3})\n",
       "\n",
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 7, \"b\": 2})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=437, prompt_tokens=133, total_tokens=570, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=397, rejected_prediction_tokens=None, text_tokens=40, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=133, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'index': 0, 'provider_specific_fields': {'thought_signature': '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'}, 'function': {'arguments': '{\"a\": 5, \"b\": 3}', 'name': 'simple_add'}, 'id': 'call_941ecf74405a412b9e9c7c2be60b__thought__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', 'type': 'function'}, {'index': 1, 'function': {'arguments': '{\"a\": 7, \"b\": 2}', 'name': 'simple_add'}, 'id': 'call_ffca4698c1684d52a29d0b429e2c', 'type': 'function'}], function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"functionCall\": {\"name\": \"simple_add\", \"args\": {\"a\": 5, \"b\": 3}}}', 'signature': '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'}], provider_specific_fields=None))], usage=Usage(completion_tokens=437, prompt_tokens=133, total_tokens=570, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=397, rejected_prediction_tokens=None, text_tokens=40, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=133, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_941ecf74405a412b9e9c7c2be60b__thought__CsoJAXLI2nyi+SWD/EIPkeNQgWjtEr0fmsi/pcJlk7Oyg+2sPpofUjK097Mg5fGTgPmllZru8StllpOcJVLLPPvWzpKkkeaGVj4ZDnRFvSfJSlVo7GMyRTWimJNRASCEAWKTUCQ5A/vyGqhYDRQQKAEnBtdcqfG/+QvhHvUr77XMLAWJTR4kgaczZV1igOnGjrhddo5d5Ti/bl2CrwGIem5FQXR1R0SOjrtsXDDD3KJFX386MecRiCNHmmBLnTL8C3hJ24jds5IxqDvKvXcxYxdTFVdF5vwN2IslkxaBrrHToNTlCj508WC0a8mRNqtkWGV6GQAfnSubqhhCMp9Et7djmDQgfl+F1fw7YfIyaPDIJgVgatLZhOE4Xyceku0fGY3Aqxmj27i702bMaLgVvOkB4BTgsmtKMPeYXxPaOapAYdgAw9Y64y2sjd/3yBvt+rX0V/rCX5z4WISavzvu0Bv5Bi0XZ91HzeqhFdR7vj/X24HBZ7peszVXdx+9V0rscIFmJGWulSVFtL1LFz0T60ig44ms1yECZQCsbn1ChJ+Eev5H6XhCbnRBvqht920GGVSo/XwHEzCg59b1vhFi59Ndt5PSwVmxIWydxSsUYT6Tfv2cFX2HhA1tyPB2Fch/Fbb2vx3pFX/CfHAWmub/SBCxhgG6zZqJ4CiXDTbKtbf636fTO8OtnC+SAbVBKjaylzx/6iRtx0nnz4wdAI9j3qcX6duXQWhTtlghHU/xBzmT6HovR/O60Oc4v2tzPig/qtWIFjKdLLorB0F5ElQQNqtvpsctZJIEJX9e1+1SXH0LdBERe2DdpamnSxX1U9yKCcCcZfwQkjNpvKDVbwCZ6gsASe2LzKhoRKzaZ+hYw1loksQdFto1Y2QHVLB+cxhhAZ+ADnBL5cdufwmpVi2kFDzM/rcUxYXPKll0f85RxNNT3uXvtpfmQoiGLvq9rY7KKVf6N0/AS/6RvIj50PSXWoPax2DzRRFp7etN1ehTF041iXAxTNwZJD7uWXkZIQ0eeBWTm3IeWyw72jhDfnxmssWzKDRUUYIFaN9sONIJP8lRvUikVvsmmh9POZNuycugYMHKM1Z/jJgOuissZGY/wwthpjTdLs9aX9PSOY8ClfjOMwDq7TpbYVNZRpej0dD9r8BJl1TVn0jM+Qsz6x+tuLtDsJnIPlx8m2MKYx9qYrDY4AimTcJ1IIPGe7xAeC494r5LtTZ7GKdD2eS+JWsn7prstD3/hxrYt/Y1VDc3L6wW11XgSZugWCK21nW800AR19aIqVw9xqycZ29qzkXXVcAlKzyIz3xW6HeL1ZSm1VehnunPIbDdj9PYrYc+aWe4op2trDesPLwd9u7Wj3azkIgNLwjhiDbbYFe6iTziEgQW4C42aYLlPOlsDO0QIb3wBWSh2hWkb65JblhQwlpNEOhFSWb4oVYR4XHNXlgX98yg13mrUx9pvSV5V0K5W52qyOg1vv8HRatQ1pERP0oPJwao0Mu5GLQTEB3V+zCKVr/uUgE/zfwIwhYsarRwVCqX9wFy+y6QxgImjBgiPkfS1BrblO+tTR2p/XSDzr/5glVy6t8nIfgvu9bTf/mrvNzS/y4ZHXIj7xvbahWcNYCK0cbVX2Fnk+6tH0HeO88KrgEBcsjafFlbendVA5x2dMVfBPV89SuXazNAD1AnvHXSrSYEPbpQMfXVqZj8es5lsjrecWk3hfUOOJCPYyCehpC6oI+l1xXC/XU/b5e53Avdtqg4+NvwIJa1FZkf2wm9jAFamvQZsOchRSAHHF/KwM4AOw+RCBwCOODRm+XESbhvA5m6p7mwclV/ell/gdl98cGaapFFUGue2mHMezPIUe2Gf7i068X1bQjm2COqmRoKXwFyyNp8Fc2fQ6ide/xeMpRt1JChE7n15+5qkf/OxwiBcmsrCELf1vqqy0ZAXTphc5al6hKSN1gz0Bc6fEvwn2BxYxEgXVjvFEP35GCKwcnef65ak7NqbfhuKdQVzb6o',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '8'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_ffca4698c1684d52a29d0b429e2c',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '9'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ multiply({\"b\": 9, \"a\": 8})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=18, prompt_tokens=200, total_tokens=218, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=200, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'index': 0, 'function': {'arguments': '{\"b\": 9, \"a\": 8}', 'name': 'multiply'}, 'id': 'call_e1c360b4b5d84764914d6fc4b824', 'type': 'function'}], function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=18, prompt_tokens=200, total_tokens=218, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=200, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_e1c360b4b5d84764914d6fc4b824',\n",
       " 'role': 'tool',\n",
       " 'name': 'multiply',\n",
       " 'content': '72'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "(5 + 3) * (7 + 2) = 72\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=231, total_tokens=248, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=231, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='(5 + 3) * (7 + 2) = 72\\n', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=17, prompt_tokens=231, total_tokens=248, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=231, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Chat(model, tools=[simple_add, multiply])\n",
    "res = chat(\"Calculate (5 + 3) * (7 + 2)\", max_steps=5, return_all=True)\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4958f",
   "metadata": {},
   "source": [
    "See it did the additions in one go!l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b17e71",
   "metadata": {},
   "source": [
    "We don't want the model to keep running tools indefinitely. Lets showcase how we can force the model to stop after our specified number of toolcall rounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7298c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"b\": 5, \"a\": 10})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=282, prompt_tokens=196, total_tokens=478, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=261, rejected_prediction_tokens=None, text_tokens=21, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=196, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'index': 0, 'provider_specific_fields': {'thought_signature': 'CvsFAXLI2ny8YLvBawTd8D7gBw9VmF8EuI2U6Cjg78oPBdDYqQ6SJ9VeYHkwte8J9GJUOSj0XxmWCDSnIUCE4k1ywaBXjKvCBEULv91iDTIlWtLzfAxqoVce6byGsTL6JRCyeLWtLn8pjbvQ2dCfMEc+JbwZqmjgkA5kkRVXbMSS+clU8gJ4eZzDEM8ybvBIbbK0SpHlcKqwnpADIGlRiAH/3DXaZlEnIqSlaVZKFtQIbx4K0t3R1+BuOt2nIuss1i0Vwxa68ywc+UEOfl8eBmVcozr2VDTDb6ce5cW13yQ6gpwf3VDUMb5C7R5qFRgtUijGac4QncRTnQMreakSctMA/wdL6DxhDs82I67+9EP0edctTO3DCv/VjwIAejTUa67rH9O6tmteIcNC0Re2ylsDXPH0rZYHH+FQcNIYGBJQkXL3oDXe/AcT4YWGjCMQ1TNNPiOgyYiLwejgzCi5/mgddl75RNKf+j8m4tx9yJSi97ATkj4h6NBR5XjiMDXxZwwVCpKF89csyiP6zSqddweWcIStJ+VLZEy4PEgGjKeC+YGEqMQmr2GCYAZtsaRsixvDpZnvqy4vBbWz7h31DyTZIs7KhQPBdp1B2EklwCoUhPTId2nD3xfTTC+BEsCWylp7tpvvzDL31A5D3mxUMnxnNYhA3RW9W8Vru1LICx8ChdHFkjhkZHO3PFcAShL4SiGGMbltDUKqVVAks1m+sFheO1kX5yc5GLtA39CSel6ZWsgyIwDecufseaCRXDkW5KnoakQmA+UMHos8yNi8tsIFITWEerc5oOWnmpcZxuL2z5YnJ+ntOnFREvY6z77lzELTjkOFPmGOnqBoECiFatj2ZC1o0B8tnWtCIkXMYKDYYnK2PlhCRfwlTa7lub9qtVo+4SblW+lcyTPifPy7w7uhT2Wkv1sFwykB9LUfnpcqExS6AO8lkng3JOjBGa5w2l2uoBa+232aI0ukPI6XdyStCffI4FRp1QoDVzLj0EZtvJc+iPSeHgAeJYMsCQ=='}, 'function': {'arguments': '{\"b\": 5, \"a\": 10}', 'name': 'simple_add'}, 'id': 'call_646fcae09bfe4ce88109dab44d6a__thought__CvsFAXLI2ny8YLvBawTd8D7gBw9VmF8EuI2U6Cjg78oPBdDYqQ6SJ9VeYHkwte8J9GJUOSj0XxmWCDSnIUCE4k1ywaBXjKvCBEULv91iDTIlWtLzfAxqoVce6byGsTL6JRCyeLWtLn8pjbvQ2dCfMEc+JbwZqmjgkA5kkRVXbMSS+clU8gJ4eZzDEM8ybvBIbbK0SpHlcKqwnpADIGlRiAH/3DXaZlEnIqSlaVZKFtQIbx4K0t3R1+BuOt2nIuss1i0Vwxa68ywc+UEOfl8eBmVcozr2VDTDb6ce5cW13yQ6gpwf3VDUMb5C7R5qFRgtUijGac4QncRTnQMreakSctMA/wdL6DxhDs82I67+9EP0edctTO3DCv/VjwIAejTUa67rH9O6tmteIcNC0Re2ylsDXPH0rZYHH+FQcNIYGBJQkXL3oDXe/AcT4YWGjCMQ1TNNPiOgyYiLwejgzCi5/mgddl75RNKf+j8m4tx9yJSi97ATkj4h6NBR5XjiMDXxZwwVCpKF89csyiP6zSqddweWcIStJ+VLZEy4PEgGjKeC+YGEqMQmr2GCYAZtsaRsixvDpZnvqy4vBbWz7h31DyTZIs7KhQPBdp1B2EklwCoUhPTId2nD3xfTTC+BEsCWylp7tpvvzDL31A5D3mxUMnxnNYhA3RW9W8Vru1LICx8ChdHFkjhkZHO3PFcAShL4SiGGMbltDUKqVVAks1m+sFheO1kX5yc5GLtA39CSel6ZWsgyIwDecufseaCRXDkW5KnoakQmA+UMHos8yNi8tsIFITWEerc5oOWnmpcZxuL2z5YnJ+ntOnFREvY6z77lzELTjkOFPmGOnqBoECiFatj2ZC1o0B8tnWtCIkXMYKDYYnK2PlhCRfwlTa7lub9qtVo+4SblW+lcyTPifPy7w7uhT2Wkv1sFwykB9LUfnpcqExS6AO8lkng3JOjBGa5w2l2uoBa+232aI0ukPI6XdyStCffI4FRp1QoDVzLj0EZtvJc+iPSeHgAeJYMsCQ==', 'type': 'function'}], function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"functionCall\": {\"name\": \"simple_add\", \"args\": {\"b\": 5, \"a\": 10}}}', 'signature': 'CvsFAXLI2ny8YLvBawTd8D7gBw9VmF8EuI2U6Cjg78oPBdDYqQ6SJ9VeYHkwte8J9GJUOSj0XxmWCDSnIUCE4k1ywaBXjKvCBEULv91iDTIlWtLzfAxqoVce6byGsTL6JRCyeLWtLn8pjbvQ2dCfMEc+JbwZqmjgkA5kkRVXbMSS+clU8gJ4eZzDEM8ybvBIbbK0SpHlcKqwnpADIGlRiAH/3DXaZlEnIqSlaVZKFtQIbx4K0t3R1+BuOt2nIuss1i0Vwxa68ywc+UEOfl8eBmVcozr2VDTDb6ce5cW13yQ6gpwf3VDUMb5C7R5qFRgtUijGac4QncRTnQMreakSctMA/wdL6DxhDs82I67+9EP0edctTO3DCv/VjwIAejTUa67rH9O6tmteIcNC0Re2ylsDXPH0rZYHH+FQcNIYGBJQkXL3oDXe/AcT4YWGjCMQ1TNNPiOgyYiLwejgzCi5/mgddl75RNKf+j8m4tx9yJSi97ATkj4h6NBR5XjiMDXxZwwVCpKF89csyiP6zSqddweWcIStJ+VLZEy4PEgGjKeC+YGEqMQmr2GCYAZtsaRsixvDpZnvqy4vBbWz7h31DyTZIs7KhQPBdp1B2EklwCoUhPTId2nD3xfTTC+BEsCWylp7tpvvzDL31A5D3mxUMnxnNYhA3RW9W8Vru1LICx8ChdHFkjhkZHO3PFcAShL4SiGGMbltDUKqVVAks1m+sFheO1kX5yc5GLtA39CSel6ZWsgyIwDecufseaCRXDkW5KnoakQmA+UMHos8yNi8tsIFITWEerc5oOWnmpcZxuL2z5YnJ+ntOnFREvY6z77lzELTjkOFPmGOnqBoECiFatj2ZC1o0B8tnWtCIkXMYKDYYnK2PlhCRfwlTa7lub9qtVo+4SblW+lcyTPifPy7w7uhT2Wkv1sFwykB9LUfnpcqExS6AO8lkng3JOjBGa5w2l2uoBa+232aI0ukPI6XdyStCffI4FRp1QoDVzLj0EZtvJc+iPSeHgAeJYMsCQ=='}], provider_specific_fields=None))], usage=Usage(completion_tokens=282, prompt_tokens=196, total_tokens=478, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=261, rejected_prediction_tokens=None, text_tokens=21, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=196, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_646fcae09bfe4ce88109dab44d6a__thought__CvsFAXLI2ny8YLvBawTd8D7gBw9VmF8EuI2U6Cjg78oPBdDYqQ6SJ9VeYHkwte8J9GJUOSj0XxmWCDSnIUCE4k1ywaBXjKvCBEULv91iDTIlWtLzfAxqoVce6byGsTL6JRCyeLWtLn8pjbvQ2dCfMEc+JbwZqmjgkA5kkRVXbMSS+clU8gJ4eZzDEM8ybvBIbbK0SpHlcKqwnpADIGlRiAH/3DXaZlEnIqSlaVZKFtQIbx4K0t3R1+BuOt2nIuss1i0Vwxa68ywc+UEOfl8eBmVcozr2VDTDb6ce5cW13yQ6gpwf3VDUMb5C7R5qFRgtUijGac4QncRTnQMreakSctMA/wdL6DxhDs82I67+9EP0edctTO3DCv/VjwIAejTUa67rH9O6tmteIcNC0Re2ylsDXPH0rZYHH+FQcNIYGBJQkXL3oDXe/AcT4YWGjCMQ1TNNPiOgyYiLwejgzCi5/mgddl75RNKf+j8m4tx9yJSi97ATkj4h6NBR5XjiMDXxZwwVCpKF89csyiP6zSqddweWcIStJ+VLZEy4PEgGjKeC+YGEqMQmr2GCYAZtsaRsixvDpZnvqy4vBbWz7h31DyTZIs7KhQPBdp1B2EklwCoUhPTId2nD3xfTTC+BEsCWylp7tpvvzDL31A5D3mxUMnxnNYhA3RW9W8Vru1LICx8ChdHFkjhkZHO3PFcAShL4SiGGMbltDUKqVVAks1m+sFheO1kX5yc5GLtA39CSel6ZWsgyIwDecufseaCRXDkW5KnoakQmA+UMHos8yNi8tsIFITWEerc5oOWnmpcZxuL2z5YnJ+ntOnFREvY6z77lzELTjkOFPmGOnqBoECiFatj2ZC1o0B8tnWtCIkXMYKDYYnK2PlhCRfwlTa7lub9qtVo+4SblW+lcyTPifPy7w7uhT2Wkv1sFwykB9LUfnpcqExS6AO8lkng3JOjBGa5w2l2uoBa+232aI0ukPI6XdyStCffI4FRp1QoDVzLj0EZtvJc+iPSeHgAeJYMsCQ==',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '15'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ simple_add({\"b\": 1, \"a\": 2})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=98, prompt_tokens=232, total_tokens=330, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=78, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=232, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[{'index': 0, 'provider_specific_fields': {'thought_signature': 'CoQCAXLI2nyE+/Pk8/Z5JYl9tai+rE0NrCA+TEcxB3NU/pknbVuQttrpWaN+hjPEAbEHo1+q4wCLeIkIto5Al8x2UCrssbg0b2b03Wh0MOpIM2Y3Xf13adEbXQN8FvL5ORxt5oxWrnGq19kzf9Ln9/SRQK1JsfK4TB61yp+XI8wIWHIPpNCAhGf/+XYfxL7FmAsMMtt5F7FQAkXUgzLZwhPGeXZ1yMQCyAU1RfHvc9lBswHldffGPvdL/C2juLH0IZy85aBSyjXJFkawjWZ50qUnQP2HiYedyKXJoz1irGU45jUM5QphiLSxXW/1Jan/s+HLsuYuGhy4Wa/BwUhepxjTyu0oby4='}, 'function': {'arguments': '{\"b\": 1, \"a\": 2}', 'name': 'simple_add'}, 'id': 'call_fff6a33e5bf14c9e996cfb461b8d__thought__CoQCAXLI2nyE+/Pk8/Z5JYl9tai+rE0NrCA+TEcxB3NU/pknbVuQttrpWaN+hjPEAbEHo1+q4wCLeIkIto5Al8x2UCrssbg0b2b03Wh0MOpIM2Y3Xf13adEbXQN8FvL5ORxt5oxWrnGq19kzf9Ln9/SRQK1JsfK4TB61yp+XI8wIWHIPpNCAhGf/+XYfxL7FmAsMMtt5F7FQAkXUgzLZwhPGeXZ1yMQCyAU1RfHvc9lBswHldffGPvdL/C2juLH0IZy85aBSyjXJFkawjWZ50qUnQP2HiYedyKXJoz1irGU45jUM5QphiLSxXW/1Jan/s+HLsuYuGhy4Wa/BwUhepxjTyu0oby4=', 'type': 'function'}], function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"functionCall\": {\"name\": \"simple_add\", \"args\": {\"b\": 1, \"a\": 2}}}', 'signature': 'CoQCAXLI2nyE+/Pk8/Z5JYl9tai+rE0NrCA+TEcxB3NU/pknbVuQttrpWaN+hjPEAbEHo1+q4wCLeIkIto5Al8x2UCrssbg0b2b03Wh0MOpIM2Y3Xf13adEbXQN8FvL5ORxt5oxWrnGq19kzf9Ln9/SRQK1JsfK4TB61yp+XI8wIWHIPpNCAhGf/+XYfxL7FmAsMMtt5F7FQAkXUgzLZwhPGeXZ1yMQCyAU1RfHvc9lBswHldffGPvdL/C2juLH0IZy85aBSyjXJFkawjWZ50qUnQP2HiYedyKXJoz1irGU45jUM5QphiLSxXW/1Jan/s+HLsuYuGhy4Wa/BwUhepxjTyu0oby4='}], provider_specific_fields=None))], usage=Usage(completion_tokens=98, prompt_tokens=232, total_tokens=330, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=78, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=232, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_fff6a33e5bf14c9e996cfb461b8d__thought__CoQCAXLI2nyE+/Pk8/Z5JYl9tai+rE0NrCA+TEcxB3NU/pknbVuQttrpWaN+hjPEAbEHo1+q4wCLeIkIto5Al8x2UCrssbg0b2b03Wh0MOpIM2Y3Xf13adEbXQN8FvL5ORxt5oxWrnGq19kzf9Ln9/SRQK1JsfK4TB61yp+XI8wIWHIPpNCAhGf/+XYfxL7FmAsMMtt5F7FQAkXUgzLZwhPGeXZ1yMQCyAU1RfHvc9lBswHldffGPvdL/C2juLH0IZy85aBSyjXJFkawjWZ50qUnQP2HiYedyKXJoz1irGU45jUM5QphiLSxXW/1Jan/s+HLsuYuGhy4Wa/BwUhepxjTyu0oby4=',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '3'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "Of course, let's calculate the final answer step by step.\n",
       "\n",
       "**Step 1: Solve the first part in parentheses.**\n",
       "*   10 + 5 = 15\n",
       "\n",
       "**Step 2: Solve the second part in parentheses.**\n",
       "*   2 + 1 = 3\n",
       "\n",
       "**Step 3: Perform the multiplication.**\n",
       "*   Now we take the result from Step 1 and multiply it by 3:\n",
       "*   15 * 3 = 45\n",
       "\n",
       "**Step 4: Perform the division.**\n",
       "*   Finally, we take the result from Step 3 and divide it by the result from Step 2:\n",
       "*   45 / 3 = 15\n",
       "\n",
       "**Final Answer:** The result of the calculation is **15**.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=303, prompt_tokens=280, total_tokens=583, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=139, rejected_prediction_tokens=None, text_tokens=164, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=280, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"\\nOf course, let's calculate the final answer step by step.\\n\\n**Step 1: Solve the first part in parentheses.**\\n*   10 + 5 = 15\\n\\n**Step 2: Solve the second part in parentheses.**\\n*   2 + 1 = 3\\n\\n**Step 3: Perform the multiplication.**\\n*   Now we take the result from Step 1 and multiply it by 3:\\n*   15 * 3 = 45\\n\\n**Step 4: Perform the division.**\\n*   Finally, we take the result from Step 3 and divide it by the result from Step 2:\\n*   45 / 3 = 15\\n\\n**Final Answer:** The result of the calculation is **15**.\", role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=303, prompt_tokens=280, total_tokens=583, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=139, rejected_prediction_tokens=None, text_tokens=164, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=280, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def divide(a: int, b: int) -> float:\n",
    "    \"Divide two numbers\"\n",
    "    return a / b\n",
    "\n",
    "chat = Chat(model, tools=[simple_add, multiply, divide])\n",
    "res = chat(\"Calculate ((10 + 5) * 3) / (2 + 1) step by step.\", \n",
    "           max_steps=3, return_all=True,\n",
    "           final_prompt=\"Please wrap-up for now and summarize how far we got.\")\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cffeeebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "test_eq(len([o for o in res if isinstance(o,ModelResponse)]),3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58dd00b",
   "metadata": {},
   "source": [
    "### Tool call exhaustion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d02210",
   "metadata": {},
   "outputs": [],
   "source": [
    "pr = \"What is 1+2, and then the result of adding +2, and then +3 to it? Use tools to make the calculations!\"\n",
    "c = Chat(model, tools=[simple_add])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e82a43f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on my initial calculation, I found that:\n",
       "\n",
       "*   1 + 2 = 3\n",
       "\n",
       "However, I did not complete the full goal you requested. I was unable to perform the subsequent additions.\n",
       "\n",
       "To complete the problem, the following steps still need to be done:\n",
       "1.  Take the result of 3 and add 2 to it.\n",
       "2.  Take that new result and add 3 to it.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=659, prompt_tokens=169, total_tokens=828, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=570, rejected_prediction_tokens=None, text_tokens=89, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=169, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Based on my initial calculation, I found that:\\n\\n*   1 + 2 = 3\\n\\nHowever, I did not complete the full goal you requested. I was unable to perform the subsequent additions.\\n\\nTo complete the problem, the following steps still need to be done:\\n1.  Take the result of 3 and add 2 to it.\\n2.  Take that new result and add 3 to it.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=659, prompt_tokens=169, total_tokens=828, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=570, rejected_prediction_tokens=None, text_tokens=89, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=169, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = c(pr, max_steps=2)\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b60eaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert c.hist[-2] == _final_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca77d9d",
   "metadata": {},
   "source": [
    "## Async"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494116db",
   "metadata": {},
   "source": [
    "### AsyncChat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25921ffa",
   "metadata": {},
   "source": [
    "If you want to use LiteLLM in a webapp you probably want to use their async function `acompletion`.\n",
    "To make that easier we will implement our version of `AsyncChat` to complement it. It follows the same implementation as Chat as much as possible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d934ac41",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _alite_call_func(tc, ns, raise_on_err=True):\n",
    "    try: fargs = json.loads(tc.function.arguments)\n",
    "    except Exception as e: raise ValueError(f\"Failed to parse function arguments: {tc.function.arguments}\") from e\n",
    "    res = await call_func_async(tc.function.name, fargs, ns=ns)\n",
    "    if isinstance(res, ToolResponse): res = res.content\n",
    "    else: res = str(res)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": res}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13cf1122",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@asave_iter\n",
    "async def astream_with_complete(self, agen, postproc=noop):\n",
    "    chunks = []\n",
    "    async for chunk in agen:\n",
    "        chunks.append(chunk)\n",
    "        postproc(chunk)\n",
    "        yield chunk\n",
    "    self.value = stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc01816",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AsyncChat(Chat):\n",
    "    async def _call(self, msg=None, prefill=None, temp=None, think=None, search=None, stream=False, max_steps=2, step=1, final_prompt=None, tool_choice=None, **kwargs):\n",
    "        if step>max_steps+1: return\n",
    "        if not get_model_info(self.model).get(\"supports_assistant_prefill\"): prefill=None\n",
    "        if _has_search(self.model) and (s:=ifnone(search,self.search)): kwargs['web_search_options'] = {\"search_context_size\": effort[s]}\n",
    "        else: _=kwargs.pop('web_search_options',None)\n",
    "        res = await acompletion(model=self.model, messages=self._prep_msg(msg, prefill), stream=stream,\n",
    "                         tools=self.tool_schemas, reasoning_effort=effort.get(think), tool_choice=tool_choice,\n",
    "                         # temperature is not supported when reasoning\n",
    "                         temperature=None if think else ifnone(temp,self.temp), \n",
    "                         caching=self.cache and 'claude' not in self.model,\n",
    "                         **kwargs)\n",
    "        if stream:\n",
    "            if prefill: yield _mk_prefill(prefill)\n",
    "            res = astream_with_complete(res,postproc=cite_footnote)\n",
    "            async for chunk in res: yield chunk\n",
    "            res = res.value\n",
    "        m=contents(res)\n",
    "        if prefill: m.content = prefill + m.content\n",
    "        yield res\n",
    "        self.hist.append(m)\n",
    "\n",
    "        if tcs := m.tool_calls:\n",
    "            tool_results = []\n",
    "            for tc in tcs:\n",
    "                result = await _alite_call_func(tc, ns=self.ns)\n",
    "                tool_results.append(result)\n",
    "                yield result\n",
    "            self.hist+=tool_results\n",
    "            if step>=max_steps-1: prompt,tool_choice,search = final_prompt,'none',False\n",
    "            else: prompt = None\n",
    "            async for result in self._call(\n",
    "                prompt, prefill, temp, think, search, stream, max_steps, step+1,\n",
    "                final_prompt, tool_choice=tool_choice, **kwargs):\n",
    "                    yield result\n",
    "    \n",
    "    async def __call__(self,\n",
    "                       msg=None,          # Message str, or list of multiple message parts\n",
    "                       prefill=None,      # Prefill AI response if model supports it\n",
    "                       temp=None,         # Override temp set on chat initialization\n",
    "                       think=None,        # Thinking (l,m,h)\n",
    "                       search=None,       # Override search set on chat initialization (l,m,h)\n",
    "                       stream=False,      # Stream results\n",
    "                       max_steps=2, # Maximum number of tool calls\n",
    "                       final_prompt=_final_prompt, # Final prompt when tool calls have ran out \n",
    "                       return_all=False,  # Returns all intermediate ModelResponses if not streaming and has tool calls\n",
    "                       **kwargs):\n",
    "        result_gen = self._call(msg, prefill, temp, think, search, stream, max_steps, 1, final_prompt, **kwargs)\n",
    "        if stream or return_all: return result_gen\n",
    "        async for res in result_gen: pass\n",
    "        return res # normal chat behavior only return last msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffa22b71",
   "metadata": {},
   "source": [
    "### Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5955051",
   "metadata": {},
   "source": [
    "Basic example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c142f088",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2 + 2 = 4\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=217, prompt_tokens=8, total_tokens=225, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=210, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=8, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='2 + 2 = 4', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=217, prompt_tokens=8, total_tokens=225, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=210, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=8, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = AsyncChat(model)\n",
    "await chat(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597b5855",
   "metadata": {},
   "source": [
    "With tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e178ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_add(a: int, b: int) -> int:\n",
    "    \"Add two numbers asynchronously\"\n",
    "    await asyncio.sleep(0.1)\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14f99c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "ðŸ”§ async_add({\"b\": 7, \"a\": 5})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=236, prompt_tokens=73, total_tokens=309, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=216, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=73, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, provider_specific_fields={'thought_signature': 'CsEGAXLI2nziimeXQJFA6Ji4jJ4nO1RDNAAKHUFLT55IihZXAWXS9ZGJT/nD3XibbDf0+MDbIhpJ+RZOLRZSvHPcdWNEscg8mINDnZu2jaWAbd3MLM63mQTLwaKIGQ0mM/JzqNwCqhPFtdldHEFz+zYXM23dr/9Epon0d3/fbFtA3cErty8xC/Y9j7sNf4ATlRYn6rm1Geb6sZajuSjkqwAsguOwdDm0r1jwHprKK1ucco1tte6w1OyB0nYZPm24847ob7haFd5hc9MTvonmBMzrywALc1h2xrOEedOO4wsd/DkeYWWkOuKBxDBwvUMbBCumZMTcA7VzxG2o2rbmprntAMPB4wO1+oWFvvQ4HehLJ03Vv19M8FOUhc2GR/ZuIvYgx4Oz+DzwXkqLyHFZkn9WwX8ZMqTGkEUTaPoyUWS2g+qCx8kRSF5NQ2zcUV/qvC45jaf34+R3WLEgk8GXdOAGsYpkjUijiio5l1PVzqsCmk8cakl4qrKHFUHSIX6LnicSxduU7K8Iqmdlpo0sqVSuPoLgh2I414nLcxGqznni5bm1sMun+UVxx4F9PVJOMq3xmD6TbGMK5Kxco9MYnkO9rSx6bipyjNElc+6h1YIahP9qumV9mbmVZdPVA5gjRbCyRycuhRPC5pqIgy07MfHXMyljz16PCzonWtYg7qeRFBxdgx/+rH3rjJZ+O/n/afwlWpKhQ78I0ozH8x/TpsTxM8qhTgOFa0KHrj3HLE9o67Ru/NMhgKH39Rz/lAculny8QsNT0NijLF1pLyiHvm9hN5Qz505sUSPnpJAoUG40YqGB0z0u0UlXoKv+VhgGWEAZmnmL4+i5b0dGEJjHpNDAFK0/SPSc4B+Q4P6Dad0fFneUJQsR8VvumUxQJR8ioRyIENa+qPQuLLCoA/uN8wanuicCJv2iNeLwvS1Y62rQNPvFokQBL2yiZMWmJFzvLk83AYTZS7t6Gi+r6qeR//rTC1fvqIdrX3nVLwxGRiFEvVMuvKh03qXOXJJ6mOd6AzVU1m0F3yV5QSEZjnT2K7vseS9Ci8b+U0QPxuYOKfWg1R5NYj0uGjHF6eqoeKHwiNiWBJTJzMbtBRhK/0l5LlzXYgA='}, function=Function(arguments='{\"b\": 7, \"a\": 5}', name='async_add'), id='call_eb61b4cd1c9146bd8e8b787a100b__thought__CsEGAXLI2nziimeXQJFA6Ji4jJ4nO1RDNAAKHUFLT55IihZXAWXS9ZGJT/nD3XibbDf0+MDbIhpJ+RZOLRZSvHPcdWNEscg8mINDnZu2jaWAbd3MLM63mQTLwaKIGQ0mM/JzqNwCqhPFtdldHEFz+zYXM23dr/9Epon0d3/fbFtA3cErty8xC/Y9j7sNf4ATlRYn6rm1Geb6sZajuSjkqwAsguOwdDm0r1jwHprKK1ucco1tte6w1OyB0nYZPm24847ob7haFd5hc9MTvonmBMzrywALc1h2xrOEedOO4wsd/DkeYWWkOuKBxDBwvUMbBCumZMTcA7VzxG2o2rbmprntAMPB4wO1+oWFvvQ4HehLJ03Vv19M8FOUhc2GR/ZuIvYgx4Oz+DzwXkqLyHFZkn9WwX8ZMqTGkEUTaPoyUWS2g+qCx8kRSF5NQ2zcUV/qvC45jaf34+R3WLEgk8GXdOAGsYpkjUijiio5l1PVzqsCmk8cakl4qrKHFUHSIX6LnicSxduU7K8Iqmdlpo0sqVSuPoLgh2I414nLcxGqznni5bm1sMun+UVxx4F9PVJOMq3xmD6TbGMK5Kxco9MYnkO9rSx6bipyjNElc+6h1YIahP9qumV9mbmVZdPVA5gjRbCyRycuhRPC5pqIgy07MfHXMyljz16PCzonWtYg7qeRFBxdgx/+rH3rjJZ+O/n/afwlWpKhQ78I0ozH8x/TpsTxM8qhTgOFa0KHrj3HLE9o67Ru/NMhgKH39Rz/lAculny8QsNT0NijLF1pLyiHvm9hN5Qz505sUSPnpJAoUG40YqGB0z0u0UlXoKv+VhgGWEAZmnmL4+i5b0dGEJjHpNDAFK0/SPSc4B+Q4P6Dad0fFneUJQsR8VvumUxQJR8ioRyIENa+qPQuLLCoA/uN8wanuicCJv2iNeLwvS1Y62rQNPvFokQBL2yiZMWmJFzvLk83AYTZS7t6Gi+r6qeR//rTC1fvqIdrX3nVLwxGRiFEvVMuvKh03qXOXJJ6mOd6AzVU1m0F3yV5QSEZjnT2K7vseS9Ci8b+U0QPxuYOKfWg1R5NYj0uGjHF6eqoeKHwiNiWBJTJzMbtBRhK/0l5LlzXYgA=', type='function')], function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"functionCall\": {\"name\": \"async_add\", \"args\": {\"b\": 7, \"a\": 5}}}', 'signature': 'CsEGAXLI2nziimeXQJFA6Ji4jJ4nO1RDNAAKHUFLT55IihZXAWXS9ZGJT/nD3XibbDf0+MDbIhpJ+RZOLRZSvHPcdWNEscg8mINDnZu2jaWAbd3MLM63mQTLwaKIGQ0mM/JzqNwCqhPFtdldHEFz+zYXM23dr/9Epon0d3/fbFtA3cErty8xC/Y9j7sNf4ATlRYn6rm1Geb6sZajuSjkqwAsguOwdDm0r1jwHprKK1ucco1tte6w1OyB0nYZPm24847ob7haFd5hc9MTvonmBMzrywALc1h2xrOEedOO4wsd/DkeYWWkOuKBxDBwvUMbBCumZMTcA7VzxG2o2rbmprntAMPB4wO1+oWFvvQ4HehLJ03Vv19M8FOUhc2GR/ZuIvYgx4Oz+DzwXkqLyHFZkn9WwX8ZMqTGkEUTaPoyUWS2g+qCx8kRSF5NQ2zcUV/qvC45jaf34+R3WLEgk8GXdOAGsYpkjUijiio5l1PVzqsCmk8cakl4qrKHFUHSIX6LnicSxduU7K8Iqmdlpo0sqVSuPoLgh2I414nLcxGqznni5bm1sMun+UVxx4F9PVJOMq3xmD6TbGMK5Kxco9MYnkO9rSx6bipyjNElc+6h1YIahP9qumV9mbmVZdPVA5gjRbCyRycuhRPC5pqIgy07MfHXMyljz16PCzonWtYg7qeRFBxdgx/+rH3rjJZ+O/n/afwlWpKhQ78I0ozH8x/TpsTxM8qhTgOFa0KHrj3HLE9o67Ru/NMhgKH39Rz/lAculny8QsNT0NijLF1pLyiHvm9hN5Qz505sUSPnpJAoUG40YqGB0z0u0UlXoKv+VhgGWEAZmnmL4+i5b0dGEJjHpNDAFK0/SPSc4B+Q4P6Dad0fFneUJQsR8VvumUxQJR8ioRyIENa+qPQuLLCoA/uN8wanuicCJv2iNeLwvS1Y62rQNPvFokQBL2yiZMWmJFzvLk83AYTZS7t6Gi+r6qeR//rTC1fvqIdrX3nVLwxGRiFEvVMuvKh03qXOXJJ6mOd6AzVU1m0F3yV5QSEZjnT2K7vseS9Ci8b+U0QPxuYOKfWg1R5NYj0uGjHF6eqoeKHwiNiWBJTJzMbtBRhK/0l5LlzXYgA='}], provider_specific_fields=None))], usage=Usage(completion_tokens=236, prompt_tokens=73, total_tokens=309, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=216, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=73, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_eb61b4cd1c9146bd8e8b787a100b__thought__CsEGAXLI2nziimeXQJFA6Ji4jJ4nO1RDNAAKHUFLT55IihZXAWXS9ZGJT/nD3XibbDf0+MDbIhpJ+RZOLRZSvHPcdWNEscg8mINDnZu2jaWAbd3MLM63mQTLwaKIGQ0mM/JzqNwCqhPFtdldHEFz+zYXM23dr/9Epon0d3/fbFtA3cErty8xC/Y9j7sNf4ATlRYn6rm1Geb6sZajuSjkqwAsguOwdDm0r1jwHprKK1ucco1tte6w1OyB0nYZPm24847ob7haFd5hc9MTvonmBMzrywALc1h2xrOEedOO4wsd/DkeYWWkOuKBxDBwvUMbBCumZMTcA7VzxG2o2rbmprntAMPB4wO1+oWFvvQ4HehLJ03Vv19M8FOUhc2GR/ZuIvYgx4Oz+DzwXkqLyHFZkn9WwX8ZMqTGkEUTaPoyUWS2g+qCx8kRSF5NQ2zcUV/qvC45jaf34+R3WLEgk8GXdOAGsYpkjUijiio5l1PVzqsCmk8cakl4qrKHFUHSIX6LnicSxduU7K8Iqmdlpo0sqVSuPoLgh2I414nLcxGqznni5bm1sMun+UVxx4F9PVJOMq3xmD6TbGMK5Kxco9MYnkO9rSx6bipyjNElc+6h1YIahP9qumV9mbmVZdPVA5gjRbCyRycuhRPC5pqIgy07MfHXMyljz16PCzonWtYg7qeRFBxdgx/+rH3rjJZ+O/n/afwlWpKhQ78I0ozH8x/TpsTxM8qhTgOFa0KHrj3HLE9o67Ru/NMhgKH39Rz/lAculny8QsNT0NijLF1pLyiHvm9hN5Qz505sUSPnpJAoUG40YqGB0z0u0UlXoKv+VhgGWEAZmnmL4+i5b0dGEJjHpNDAFK0/SPSc4B+Q4P6Dad0fFneUJQsR8VvumUxQJR8ioRyIENa+qPQuLLCoA/uN8wanuicCJv2iNeLwvS1Y62rQNPvFokQBL2yiZMWmJFzvLk83AYTZS7t6Gi+r6qeR//rTC1fvqIdrX3nVLwxGRiFEvVMuvKh03qXOXJJ6mOd6AzVU1m0F3yV5QSEZjnT2K7vseS9Ci8b+U0QPxuYOKfWg1R5NYj0uGjHF6eqoeKHwiNiWBJTJzMbtBRhK/0l5LlzXYgA=',\n",
       " 'role': 'tool',\n",
       " 'name': 'async_add',\n",
       " 'content': '12'}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Based on the tool usage, I was able to complete the requested calculation.\n",
       "\n",
       "**Finding:** The sum of 5 and 7 is 12.\n",
       "\n",
       "The goal was successfully completed, and no further work is needed.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=624, prompt_tokens=148, total_tokens=772, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=578, rejected_prediction_tokens=None, text_tokens=46, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=148, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Based on the tool usage, I was able to complete the requested calculation.\\n\\n**Finding:** The sum of 5 and 7 is 12.\\n\\nThe goal was successfully completed, and no further work is needed.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=624, prompt_tokens=148, total_tokens=772, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=578, rejected_prediction_tokens=None, text_tokens=46, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=148, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_with_tools = AsyncChat(model, tools=[async_add])\n",
    "res = await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\", return_all=True)\n",
    "async for r in res: display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd68aff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'user', 'content': 'What is 2+2?'},\n",
       " Message(content='2 + 2 = 4', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a93874ba",
   "metadata": {},
   "source": [
    "## Async Streaming Display"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2150a316",
   "metadata": {},
   "source": [
    "This is what our outputs look like with streaming results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57698c63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ”§ async_add\n",
      "{'tool_call_id': 'call_e00a72b6a74c4f91baf57f6f4aa9__thought__CiIBcsjafHuwU4cK6Kpus2x9D3E4lG1q4SHrHScUPituJtHYClsBcsjafCAY92mvr+WKuite3oiaiuarKxrYI03XN5hhTV8QXGhOS5MH/nvmUfDScFnQBs2ckrofs8I+7s9HhcMMdctW4itYOTunnVDqzKdhTtkfQPcyWs0TuXgaCowBAXLI2nzyTD5Pj7Z6rZ+CFlKdgzEb4SK5ERJhIV1sh09swtVg71QOPl4GzyHNZiQeWHEawqHUgNQwijzLD6qli+I5H/lpq4vEF1DPQKhUlInKPI11YDCiNkTmTDfE2ShrlORJ94YSegbYD1aCscWugkgof3ac2CbATn06x4Fw80QLSzlxV8/+26b4C4kKhgEBcsjafLrN16P5zwelYxLqLIb+/FCcK8cFbvGNzeNSgo4wSBRmMGAR6B1Jy77dzpazmebiOiE6iOwZNQBi3tm5QIfSY7kBHY9b0ofh56GvRT+FM/FHP6k4dy+ZSM26Urc43MhsX2U8mk+owqirwIq34qVssvjd7xy3VD71vYsiXd4OKSlHQgp8AXLI2nwAdib+UJhos55OM9qf29ALav9EsDuRD9vif9+BnxgBqvCz/mIdNlpRHqAEFTBcdWkBlhseou2ToQXHReQPgJg977puIj5h5khgaiBuRfwxgMhmI5KT8Dc20KdcdsHLZ1D+fdrPyG0L2ZwQz635MX/EXe5fAvqOtgp5AXLI2nz5jn+zGrAwt7POCRxIzyapHU7HUSi7esMjbl9tf1KZwJqOLiDSEn1d/1TmFkfkXN7sQU9c3qP31CXaNQ/GPUv5mgBQZhIYUfTKO8lICMwOAoOMfHZ9a8TPtlsTMRz+dco1ZUg1AIDg5DHyPF3w4UzsK/SWtQqJAQFyyNp8l1jexvzmwnzASjp7S2QTFuWf/KEaVpqevoy4vR8wn9hVrB5YtuydA/kKEq7mdS+Icc8Yr5ADr2d+W6IV6ZFj+TS4GiGUAfrD2SZYGXp858qElYbCCE27Ixcxzlnot3UaWKNu0Bnfpi5LFChmcHkrrrEsXyrO2rckYr6YSva1ZqOqDaTlCosBAXLI2nw26y1xMFVZguIZfNAyJROO5xY7QSA0tp0DThmbgjb+D2r9mPTdHC8/1z+bje8qt4EiL0EMfF1UL6R0GoOndCoD3ovtLVAcIPJN/ZsaDfMUzp06ivdvAg34bnCBgKqEpK3pmxStU+JereMlYbJvydsWOTRpXTRF2J3LRh/T7cCIUafAJVU0rgqWAQFyyNp87zPqN7CCA5TF4CJJt97AM2pzgmKC5d3l1fTu2eHFIheLTc4IjoiuQy4yzerfrrhUJn2c9vZsX5PtdxY1bmj6Yv3c8e17MciPBs+c/Z6yjwUYdqkgMpR3RK4aqirE5GNW6D7rHKuXeBo6tAnVS8sPE0rrJ2DSlA2jHoDvxqqdn56oRYmaqPZLwQc2RTp/KsvQagpFAXLI2nynO6b2A2yWL7jxVhvDjcc/EdfciKpd2mNi+ur13yqGUeMjpIej6ah05MWCBPMdSbVrxatl83/Mz8V7GZzxr+vb', 'role': 'tool', 'name': 'async_add', 'content': '12'}\n",
      "Based on the previous turn, I was asked to calculate 5 + 7.\n",
      "\n",
      "**Findings:**\n",
      "I used the `add` tool with the inputs `a=5` and `b=7`. The tool returned the result **12**.\n",
      "\n",
      "My finding is that 5 + 7 = 12.\n",
      "\n",
      "The goal was successfully completed, and no further work is needed."
     ]
    }
   ],
   "source": [
    "chat_with_tools = AsyncChat(model, tools=[async_add])\n",
    "res = await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\n",
    "async for o in res:\n",
    "    if isinstance(o,ModelResponseStream): print(delta_text(o) or '',end='')\n",
    "    elif isinstance(o,dict): print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9280710",
   "metadata": {},
   "source": [
    "We use this one quite a bit so we want to provide some utilities to better format these outputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99789ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _trunc_str(s, mx=2000, replace=\"<TRUNCATED>\"):\n",
    "    \"Truncate `s` to `mx` chars max, adding `replace` if truncated\"\n",
    "    s = str(s).strip()\n",
    "    if len(s)<=mx: return s\n",
    "    s = s[:mx]\n",
    "    ss = s.split(' ')\n",
    "    if len(ss[-1])>50: ss[-1] = ss[-1][:5]\n",
    "    s = ' '.join(ss)\n",
    "    return s+replace"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f309f775",
   "metadata": {},
   "source": [
    "Here's a complete `ModelResponse` taken from the response stream:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5203c123",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01CWqrNQvoRjf1Q1GLpTUgQR', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=228, prompt_tokens=794, total_tokens=1022, completion_tokens_details=None, prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "resp = ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01CWqrNQvoRjf1Q1GLpTUgQR', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=228, prompt_tokens=794, total_tokens=1022, prompt_tokens_details=None))\n",
    "print(repr(resp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc69f062",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tc=resp.choices[0].message.tool_calls[0]\n",
    "tc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c0add",
   "metadata": {},
   "outputs": [],
   "source": [
    "tr={'tool_call_id': 'toolu_018BGyenjiRkDQFU1jWP6qRo', 'role': 'tool','name': 'simple_add',\n",
    "    'content': '15 is the answer! ' +'.'*2000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7476563",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mk_tr_details(tr, tc, mx=2000):\n",
    "    \"Create <details> block for tool call as JSON\"\n",
    "    args = {k:_trunc_str(v, mx=mx) for k,v in json.loads(tc.function.arguments).items()}\n",
    "    res = {'id':tr['tool_call_id'], \n",
    "           'call':{'function': tc.function.name, 'arguments': args},\n",
    "           'result':_trunc_str(tr.get('content'), mx=mx),}\n",
    "    return f\"\\n\\n{detls_tag}\\n\\n```json\\n{dumps(res, indent=2)}\\n```\\n\\n</details>\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1973a7b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n<details class=\\'tool-usage-details\\'>\\n\\n```json\\n{\\n  \"id\": \"toolu_018BGyenjiRkDQFU1jWP6qRo\",\\n  \"call\": {\\n    \"function\": \"simple_add\",\\n    \"arguments\": {\\n      \"a\": \"10\",\\n      \"b\": \"5\"\\n    }\\n  },\\n  \"result\": \"15 is the answer! .....<TRUNCATED>\"\\n}\\n```\\n\\n</details>\\n\\n'"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mk_tr_details(tr,tc,mx=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7120a308",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AsyncStreamFormatter:\n",
    "    def __init__(self, include_usage=False, mx=2000):\n",
    "        self.outp,self.tcs,self.include_usage,self.mx = '',{},include_usage,mx\n",
    "    \n",
    "    def format_item(self, o):\n",
    "        \"Format a single item from the response stream.\"\n",
    "        res = ''\n",
    "        if isinstance(o, ModelResponseStream):\n",
    "            d = o.choices[0].delta\n",
    "            if nested_idx(d, 'reasoning_content') and d['reasoning_content']!='{\"text\": \"\"}':\n",
    "                res+= 'ðŸ§ ' if not self.outp or self.outp[-1]=='ðŸ§ ' else '\\n\\nðŸ§ ' # gemini can interleave reasoning\n",
    "            elif self.outp and self.outp[-1] == 'ðŸ§ ': res+= '\\n\\n'\n",
    "            if c:=d.content: # gemini has text content in last reasoning chunk\n",
    "                res+=f\"\\n\\n{c}\" if res and res[-1] == 'ðŸ§ ' else c\n",
    "        elif isinstance(o, ModelResponse):\n",
    "            if self.include_usage: res += f\"\\nUsage: {o.usage}\"\n",
    "            if c:=getattr(contents(o),'tool_calls',None):\n",
    "                self.tcs = {tc.id:tc for tc in c}\n",
    "        elif isinstance(o, dict) and 'tool_call_id' in o:\n",
    "            res += mk_tr_details(o, self.tcs.pop(o['tool_call_id']), mx=self.mx)\n",
    "        self.outp+=res\n",
    "        return res\n",
    "    \n",
    "    async def format_stream(self, rs):\n",
    "        \"Format the response stream for markdown display.\"\n",
    "        async for o in rs: yield self.format_item(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ecbcd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'Hello world!'\n"
     ]
    }
   ],
   "source": [
    "stream_msg = ModelResponseStream([StreamingChoices(delta=Delta(content=\"Hello world!\"))])\n",
    "print(repr(AsyncStreamFormatter().format_item(stream_msg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fafd181",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'ðŸ§ '\n"
     ]
    }
   ],
   "source": [
    "reasoning_msg = ModelResponseStream([StreamingChoices(delta=Delta(reasoning_content=\"thinking...\"))])\n",
    "print(repr(AsyncStreamFormatter().format_item(reasoning_msg)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f627276d",
   "metadata": {},
   "outputs": [],
   "source": [
    "mock_tool_call = ChatCompletionMessageToolCall(\n",
    "    id=\"toolu_123abc456def\", type=\"function\", \n",
    "    function=Function( name=\"simple_add\", arguments='{\"a\": 5, \"b\": 3}' )\n",
    ")\n",
    "\n",
    "mock_response = ModelResponse()\n",
    "mock_response.choices = [type('Choice', (), {\n",
    "    'message': type('Message', (), {\n",
    "        'tool_calls': [mock_tool_call]\n",
    "    })()\n",
    "})()]\n",
    "\n",
    "mock_tool_result = {\n",
    "    'tool_call_id': 'toolu_123abc456def', 'role': 'tool', \n",
    "    'name': 'simple_add', 'content': '8'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9362bb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "<details class='tool-usage-details'>\n",
      "\n",
      "```json\n",
      "{\n",
      "  \"id\": \"toolu_123abc456def\",\n",
      "  \"call\": {\n",
      "    \"function\": \"simple_add\",\n",
      "    \"arguments\": {\n",
      "      \"a\": \"5\",\n",
      "      \"b\": \"3\"\n",
      "    }\n",
      "  },\n",
      "  \"result\": \"8\"\n",
      "}\n",
      "```\n",
      "\n",
      "</details>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "fmt = AsyncStreamFormatter()\n",
    "fmt.format_item(mock_response)\n",
    "print(fmt.format_item(mock_tool_result))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ea8c05",
   "metadata": {},
   "source": [
    "In jupyter it's nice to use this `AsyncStreamFormatter` in combination with the `Markdown` `display`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ee8bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def adisplay_stream(rs):\n",
    "    \"Use IPython.display to markdown display the response stream.\"\n",
    "    try: from IPython.display import display, Markdown\n",
    "    except ModuleNotFoundError: raise ModuleNotFoundError(\"This function requires ipython. Please run `pip install ipython` to use.\")\n",
    "    fmt = AsyncStreamFormatter()\n",
    "    md = ''\n",
    "    async for o in fmt.format_stream(rs): \n",
    "        md+=o\n",
    "        display(Markdown(md),clear=True)\n",
    "    return fmt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac772065",
   "metadata": {},
   "source": [
    "## Streaming examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93c2ce8",
   "metadata": {},
   "source": [
    "Now we can demonstrate `AsyncChat` with `stream=True`!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fae17b5b",
   "metadata": {},
   "source": [
    "### Tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375953eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ðŸ§ \n",
       "\n",
       "\n",
       "\n",
       "<details class='tool-usage-details'>\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"id\": \"call_9cbbe217b84447699a26f3fd9b57__thought__CiIBcsjafHuwU4cK6Kpus2x9D3E4lG1q4SHrHScUPituJtHYClsBcsjafCAY92mvr+WKuite3oiaiuarKxrYI03XN5hhTV8QXGhOS5MH/nvmUfDScFnQBs2ckrofs8I+7s9HhcMMdctW4itYOTunnVDqzKdhTtkfQPcyWs0TuXgaCowBAXLI2nzyTD5Pj7Z6rZ+CFlKdgzEb4SK5ERJhIV1sh09swtVg71QOPl4GzyHNZiQeWHEawqHUgNQwijzLD6qli+I5H/lpq4vEF1DPQKhUlInKPI11YDCiNkTmTDfE2ShrlORJ94YSegbYD1aCscWugkgof3ac2CbATn06x4Fw80QLSzlxV8/+26b4C4kKhgEBcsjafLrN16P5zwelYxLqLIb+/FCcK8cFbvGNzeNSgo4wSBRmMGAR6B1Jy77dzpazmebiOiE6iOwZNQBi3tm5QIfSY7kBHY9b0ofh56GvRT+FM/FHP6k4dy+ZSM26Urc43MhsX2U8mk+owqirwIq34qVssvjd7xy3VD71vYsiXd4OKSlHQgp8AXLI2nwAdib+UJhos55OM9qf29ALav9EsDuRD9vif9+BnxgBqvCz/mIdNlpRHqAEFTBcdWkBlhseou2ToQXHReQPgJg977puIj5h5khgaiBuRfwxgMhmI5KT8Dc20KdcdsHLZ1D+fdrPyG0L2ZwQz635MX/EXe5fAvqOtgp5AXLI2nz5jn+zGrAwt7POCRxIzyapHU7HUSi7esMjbl9tf1KZwJqOLiDSEn1d/1TmFkfkXN7sQU9c3qP31CXaNQ/GPUv5mgBQZhIYUfTKO8lICMwOAoOMfHZ9a8TPtlsTMRz+dco1ZUg1AIDg5DHyPF3w4UzsK/SWtQqJAQFyyNp8l1jexvzmwnzASjp7S2QTFuWf/KEaVpqevoy4vR8wn9hVrB5YtuydA/kKEq7mdS+Icc8Yr5ADr2d+W6IV6ZFj+TS4GiGUAfrD2SZYGXp858qElYbCCE27Ixcxzlnot3UaWKNu0Bnfpi5LFChmcHkrrrEsXyrO2rckYr6YSva1ZqOqDaTlCosBAXLI2nw26y1xMFVZguIZfNAyJROO5xY7QSA0tp0DThmbgjb+D2r9mPTdHC8/1z+bje8qt4EiL0EMfF1UL6R0GoOndCoD3ovtLVAcIPJN/ZsaDfMUzp06ivdvAg34bnCBgKqEpK3pmxStU+JereMlYbJvydsWOTRpXTRF2J3LRh/T7cCIUafAJVU0rgqWAQFyyNp87zPqN7CCA5TF4CJJt97AM2pzgmKC5d3l1fTu2eHFIheLTc4IjoiuQy4yzerfrrhUJn2c9vZsX5PtdxY1bmj6Yv3c8e17MciPBs+c/Z6yjwUYdqkgMpR3RK4aqirE5GNW6D7rHKuXeBo6tAnVS8sPE0rrJ2DSlA2jHoDvxqqdn56oRYmaqPZLwQc2RTp/KsvQagpFAXLI2nynO6b2A2yWL7jxVhvDjcc/EdfciKpd2mNi+ur13yqGUeMjpIej6ah05MWCBPMdSbVrxatl83/Mz8V7GZzxr+vb\",\n",
       "  \"call\": {\n",
       "    \"function\": \"async_add\",\n",
       "    \"arguments\": {\n",
       "      \"b\": \"7\",\n",
       "      \"a\": \"5\"\n",
       "    }\n",
       "  },\n",
       "  \"result\": \"12\"\n",
       "}\n",
       "```\n",
       "\n",
       "</details>\n",
       "\n",
       "Based on the previous turn, I was asked to calculate 5 + 7.\n",
       "\n",
       "**Findings:**\n",
       "I used the `add` tool with the inputs `a=5` and `b=7`. The tool returned the result **12**.\n",
       "\n",
       "My finding is that 5 + 7 = 12.\n",
       "\n",
       "The goal was successfully completed, and no further work is needed."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = AsyncChat(model, tools=[async_add])\n",
    "res = await chat(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\n",
    "fmt = await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39cdab76",
   "metadata": {},
   "source": [
    "### Thinking tool call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d97da43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ðŸ§ ðŸ§ ðŸ§ ðŸ§ \n",
       "\n",
       "For a list of 1000 random integers, the most efficient way is to use the **built-in sorting function** provided by your programming language.\n",
       "\n",
       "*   **Python:** `my_list.sort()` or `sorted(my_list)`\n",
       "*   **Java:** `Arrays.sort()`\n",
       "*   **JavaScript:** `my_array.sort((a, b) => a - b)`\n",
       "*   **C++:** `std::sort()`\n",
       "\n",
       "These functions are highly optimized, often using a hybrid algorithm like **Introsort** (a mix of Quicksort, Heapsort, and Insertion Sort). For a small size like 1000, they are practically unbeatable in terms of both implementation speed and performance."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = AsyncChat(model)\n",
    "res = await chat(\"Briefly, what's the most efficient way to sort a list of 1000 random integers?\", think='l',stream=True)\n",
    "_ = await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5580e7f",
   "metadata": {},
   "source": [
    "### Multiple tool calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc5b196",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "ðŸ§ \n",
       "\n",
       "Of course, let's break this down. First, we'll evaluate the expressions in the parentheses, `(10 + 5)` and `(2 + 1)`, in parallel.\n",
       "\n",
       "<details class='tool-usage-details'>\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"id\": \"call_cd436947eca241d99da7b86d94c7\",\n",
       "  \"call\": {\n",
       "    \"function\": \"simple_add\",\n",
       "    \"arguments\": {\n",
       "      \"a\": \"10\",\n",
       "      \"b\": \"5\"\n",
       "    }\n",
       "  },\n",
       "  \"result\": \"15\"\n",
       "}\n",
       "```\n",
       "\n",
       "</details>\n",
       "\n",
       "\n",
       "\n",
       "<details class='tool-usage-details'>\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"id\": \"call_b649e28ff8104d1d8462a43e8d9b\",\n",
       "  \"call\": {\n",
       "    \"function\": \"simple_add\",\n",
       "    \"arguments\": {\n",
       "      \"a\": \"2\",\n",
       "      \"b\": \"1\"\n",
       "    }\n",
       "  },\n",
       "  \"result\": \"3\"\n",
       "}\n",
       "```\n",
       "\n",
       "</details>\n",
       "\n",
       "Now that we have the results for the expressions in the parentheses, we can proceed. We have simplified the expression to `15 * 3 / 3`. Next, we will perform the multiplication and division in order from left to right. So, we'll first multiply 15 by 3.\n",
       "\n",
       "<details class='tool-usage-details'>\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"id\": \"call_4647729b389544e6a534e11030fc\",\n",
       "  \"call\": {\n",
       "    \"function\": \"multiply\",\n",
       "    \"arguments\": {\n",
       "      \"b\": \"3\",\n",
       "      \"a\": \"15\"\n",
       "    }\n",
       "  },\n",
       "  \"result\": \"45\"\n",
       "}\n",
       "```\n",
       "\n",
       "</details>\n",
       "\n",
       "\n",
       "We have now calculated `15 * 3` and have the result `45`. The expression is now `45 / 3`. The final step is to perform the division.\n",
       "\n",
       "<details class='tool-usage-details'>\n",
       "\n",
       "```json\n",
       "{\n",
       "  \"id\": \"call_246e3c7be0e64087b8d3a23f32d2\",\n",
       "  \"call\": {\n",
       "    \"function\": \"divide\",\n",
       "    \"arguments\": {\n",
       "      \"b\": \"3\",\n",
       "      \"a\": \"45\"\n",
       "    }\n",
       "  },\n",
       "  \"result\": \"15.0\"\n",
       "}\n",
       "```\n",
       "\n",
       "</details>\n",
       "\n",
       "\n",
       "We have now completed the final step of the calculation. The expression `45 / 3` evaluates to `15.0`. Therefore, the final answer is 15.0.\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#| hide\n",
    "chat = AsyncChat(model, tools=[simple_add, multiply, divide])\n",
    "res = await chat(\"Calculate ((10 + 5) * 3) / (2 + 1) Use parallel tool calls, but explain where we are after each batch.\", \n",
    "           max_steps=3, stream=True,\n",
    "           final_prompt=\"Please wrap-up for now and summarize how far we got.\")\n",
    "fmt = await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba4a72c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content=\"Of course, let's break this down. First, we'll evaluate the expressions in the parentheses, `(10 + 5)` and `(2 + 1)`, in parallel.\", role='assistant', tool_calls=[{'function': {'arguments': '{\"a\": 10, \"b\": 5}', 'name': 'simple_add'}, 'id': 'call_cd436947eca241d99da7b86d94c7', 'type': 'function'}, {'function': {'arguments': '{\"a\": 2, \"b\": 1}', 'name': 'simple_add'}, 'id': 'call_b649e28ff8104d1d8462a43e8d9b', 'type': 'function'}], function_call=None, provider_specific_fields=None, reasoning_content='{\"text\": \"Of course, let\\'s break this down. First, we\\'ll evaluate the expressions in the parentheses, `(10\"}')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ea0f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_cd436947eca241d99da7b86d94c7',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '15'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c81c0f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_b649e28ff8104d1d8462a43e8d9b',\n",
       " 'role': 'tool',\n",
       " 'name': 'simple_add',\n",
       " 'content': '3'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456910e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Message(content=\"Now that we have the results for the expressions in the parentheses, we can proceed. We have simplified the expression to `15 * 3 / 3`. Next, we will perform the multiplication and division in order from left to right. So, we'll first multiply 15 by 3.\", role='assistant', tool_calls=[{'function': {'arguments': '{\"b\": 3, \"a\": 15}', 'name': 'multiply'}, 'id': 'call_4647729b389544e6a534e11030fc', 'type': 'function'}], function_call=None, provider_specific_fields=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf490ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tool_call_id': 'call_4647729b389544e6a534e11030fc',\n",
       " 'role': 'tool',\n",
       " 'name': 'multiply',\n",
       " 'content': '45'}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat.hist[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "364a8bbe",
   "metadata": {},
   "source": [
    "Now to demonstrate that we can load back the formatted output back into a new `Chat` object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55653287",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "We just solved the mathematical expression `(10 + 5) * (2 + 1) / 3` step-by-step using the available tools.\n",
       "\n",
       "Here's a breakdown of the process:\n",
       "\n",
       "1.  **Parentheses First:** We first calculated the expressions inside the parentheses:\n",
       "    *   `10 + 5` was solved using `simple_add(a=10, b=5)`, which gave us `15`.\n",
       "    *   `2 + 1` was solved using `simple_add(a=2, b=1)`, which gave us `3`.\n",
       "\n",
       "2.  **Multiplication:** Next, we multiplied the results from the first step:\n",
       "    *   `15 * 3` was solved using `multiply(a=15, b=3)`, resulting in `45`.\n",
       "\n",
       "3.  **Division:** Finally, we performed the division:\n",
       "    *   `45 / 3` was solved using `divide(a=45, b=3)`, which gave us the final answer.\n",
       "\n",
       "The final result of the entire calculation was **15.0**.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-pro`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=712, prompt_tokens=501, total_tokens=1213, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=469, rejected_prediction_tokens=None, text_tokens=243, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=501, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-pro', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"We just solved the mathematical expression `(10 + 5) * (2 + 1) / 3` step-by-step using the available tools.\\n\\nHere's a breakdown of the process:\\n\\n1.  **Parentheses First:** We first calculated the expressions inside the parentheses:\\n    *   `10 + 5` was solved using `simple_add(a=10, b=5)`, which gave us `15`.\\n    *   `2 + 1` was solved using `simple_add(a=2, b=1)`, which gave us `3`.\\n\\n2.  **Multiplication:** Next, we multiplied the results from the first step:\\n    *   `15 * 3` was solved using `multiply(a=15, b=3)`, resulting in `45`.\\n\\n3.  **Division:** Finally, we performed the division:\\n    *   `45 / 3` was solved using `divide(a=45, b=3)`, which gave us the final answer.\\n\\nThe final result of the entire calculation was **15.0**.\", role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[{'type': 'thinking', 'thinking': '{\"text\": \"We just solved the mathematical expression `(10 + 5) * (2 + 1) / 3` step-by-step using the available tools.\\\\n\\\\nHere\\'s a breakdown of the process:\\\\n\\\\n1.  **Parentheses First:** We first calculated the expressions inside the parentheses:\\\\n    *   `10 + 5` was solved using `simple_add(a=10, b=5)`, which gave us `15`.\\\\n    *   `2 + 1` was solved using `simple_add(a=2, b=1)`, which gave us `3`.\\\\n\\\\n2.  **Multiplication:** Next, we multiplied the results from the first step:\\\\n    *   `15 * 3` was solved using `multiply(a=15, b=3)`, resulting in `45`.\\\\n\\\\n3.  **Division:** Finally, we performed the division:\\\\n    *   `45 / 3` was solved using `divide(a=45, b=3)`, which gave us the final answer.\\\\n\\\\nThe final result of the entire calculation was **15.0**.\"}', 'signature': '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'}], provider_specific_fields=None))], usage=Usage(completion_tokens=712, prompt_tokens=501, total_tokens=1213, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=469, rejected_prediction_tokens=None, text_tokens=243, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=501, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat5 = Chat(model,hist=fmt2hist(fmt.outp),tools=[simple_add, multiply, divide])\n",
    "chat5('what did we just do?')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bc0df26",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124da784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Weather in New York City: Chilly with a Chance of Snow Showers\n",
       "\n",
       "**New York, NY** - This morning in New York City is currently cloudy with a temperature of 38Â°F (3Â°C), though it feels more like 33Â°F (1Â°C). There is a very low chance of snow. The humidity is currently at 54%.\n",
       "\n",
       "Today's forecast shows a high of around 41Â°F (5Â°C) and a low of 21Â°F (-6Â°C). Skies will be mostly cloudy, gradually becoming sunny.\n",
       "\n",
       "Looking ahead, Friday is expected to bring snow showers with temperatures ranging from 30Â°F to 31Â°F (-1Â°C). The weekend forecast shows partly sunny skies with a high of 41Â°F (5Â°C) on Saturday and 40Â°F (4Â°C) on Sunday.\n",
       "\n",
       "The extended forecast indicates a mix of sun and clouds with a potential for light rain early next week and a chance of a rain and snow mix by next Thursday. A heavy snowstorm is possible next Sunday."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat_stream_tools = AsyncChat(model, search='l')\n",
    "res = await chat_stream_tools(\"Search the weather in NYC\", stream=True)\n",
    "_=await adisplay_stream(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1fb99",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e071882",
   "metadata": {},
   "source": [
    "#### Anthropic\n",
    "\n",
    "We use explicit caching via cache control checkpoints. Anthropic requires exact match with cached tokens and even a small change results in cache invalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_cachy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32340dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "a,b = random.randint(0,100), random.randint(0,100)\n",
    "hist = [[f\"What is {a}+{b}?\\n\" * 250], f\"It's {a+b}\", ['hi'], \"Hello\"]\n",
    "msgs = mk_msgs(hist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c6f1c",
   "metadata": {},
   "source": [
    "In this first api call we will see cache creation until the last user msg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6160c8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage(completion_tokens=13, prompt_tokens=2026, total_tokens=2039, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=2023), cache_creation_input_tokens=2023, cache_read_input_tokens=0)\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "sleep(5)\n",
    "chat = AsyncChat(ms[3], cache=True, hist=hist)\n",
    "rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs: \n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cache_read_toks = o.usage.cache_creation_input_tokens\n",
    "# test_eq(cache_read_toks > 1000, True)\n",
    "# test_eq(o.usage.cache_read_input_tokens, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a63ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage(completion_tokens=17, prompt_tokens=2040, total_tokens=2057, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=2023, text_tokens=None, image_tokens=None, cache_creation_tokens=14), cache_creation_input_tokens=14, cache_read_input_tokens=2023)\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "hist.extend([['hi again'], 'how may i help you?'])\n",
    "chat = AsyncChat(ms[3], cache=True, hist=hist)\n",
    "rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs:\n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528cf7b4",
   "metadata": {},
   "source": [
    "The subsequent call should re-use the existing cache:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa997bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_eq(o.usage.cache_read_input_tokens, cache_read_toks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118670c9",
   "metadata": {},
   "source": [
    "#### Gemini\n",
    "\n",
    "Gemini implicit caching supports partial token matches. The usage metadata only shows cache hits with the `cached_tokens` field. So, to view them we need to run completions at least twice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdad46",
   "metadata": {},
   "source": [
    "Testing with `gemini-2.5-flash` until `gemini-3-pro-preview` is more reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6215649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage(completion_tokens=63, prompt_tokens=2525, total_tokens=2588, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=53, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=2011, text_tokens=514, image_tokens=None))\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs: \n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd85cb9",
   "metadata": {},
   "source": [
    "Running the same completion again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa84445",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage(completion_tokens=63, prompt_tokens=2525, total_tokens=2588, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=53, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=2011, text_tokens=514, image_tokens=None))\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "sleep(5) # it takes a while for cached tokens to be avail.\n",
    "chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs: \n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # flaky test\n",
    "# test_eq(o.usage.prompt_tokens_details.cached_tokens > 1800, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a1d56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage(completion_tokens=34, prompt_tokens=2535, total_tokens=2569, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=27, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=2005, text_tokens=530, image_tokens=None))\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "hist.extend([['hi again'], 'how may i help you?'])\n",
    "chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs:\n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # flaky test\n",
    "# test_eq(o.usage.prompt_tokens_details.cached_tokens > 2000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806d459",
   "metadata": {},
   "source": [
    "Let's modify the cached content and see that partial matching works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88eb842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Usage(completion_tokens=36, prompt_tokens=1920, total_tokens=1956, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=29, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=991, text_tokens=929, image_tokens=None))\n"
     ]
    }
   ],
   "source": [
    "#| notest\n",
    "c = hist[0][0]\n",
    "hist[0][0] = c[:int(len(c)*0.75)] + \" Some extra text\"\n",
    "hist.extend([['hi again'], 'how may i help you?'])\n",
    "chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n",
    "async for o in rs:\n",
    "    if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f82575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # flaky test\n",
    "# test_eq(o.usage.prompt_tokens_details.cached_tokens > 900, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9764dc",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e677e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
