{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "798b1171",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> lisette core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ebe320a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82380377",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import litellm, json\n",
    "from litellm import completion, stream_chunk_builder\n",
    "from litellm.types.utils import ModelResponseStream,ModelResponse\n",
    "from litellm.utils import function_to_dict\n",
    "from toolslm.funccall import mk_ns, call_func\n",
    "from typing import Optional\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f20759",
   "metadata": {},
   "source": [
    "## LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38c4ea2",
   "metadata": {},
   "source": [
    "Litellm provides an easy wrapper for most big LLM providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6a85dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = [\"gemini/gemini-2.5-flash\", \"claude-sonnet-4-20250514\", \"openai/gpt-4.1\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fa8988a",
   "metadata": {},
   "source": [
    "TODO: test mixed content/tool calls message (and mixed images too)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d61cf441",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _repr_markdown_(self: litellm.ModelResponse):\n",
    "    message = self.choices[0].message\n",
    "    content = ''\n",
    "    if message.content: content += message.content\n",
    "    if message.tool_calls:\n",
    "        tool_calls = [f\"\\n\\n🔧 {tc.function.name}({tc.function.arguments})\\n\" for tc in message.tool_calls]\n",
    "        content += \"\\n\".join(tool_calls)\n",
    "    if not content: content = str(message)\n",
    "    details = [\n",
    "        f\"id: `{self.id}`\",\n",
    "        f\"model: `{self.model}`\",\n",
    "        f\"finish_reason: `{self.choices[0].finish_reason}`\"\n",
    "    ]\n",
    "    if hasattr(self, 'usage') and self.usage: details.append(f\"usage: `{self.usage}`\")\n",
    "    det_str = '\\n- '.join(details)\n",
    "    \n",
    "    return f\"\"\"{content}\n",
    "\n",
    "<details>\n",
    "\n",
    "- {det_str}\n",
    "\n",
    "</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920d0288",
   "metadata": {},
   "outputs": [],
   "source": [
    "msg = [{'role':'user','content':'Hey there!'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124c866d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'=== gemini/gemini-2.5-flash ==='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hey there! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `std9aJiaG4-cz7IP26rhiQk`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=39, prompt_tokens=4, total_tokens=43, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=29, rejected_prediction_tokens=None, text_tokens=10), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='std9aJiaG4-cz7IP26rhiQk', created=1753077681, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hey there! How can I help you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=39, prompt_tokens=4, total_tokens=43, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=29, rejected_prediction_tokens=None, text_tokens=10), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=== claude-sonnet-4-20250514 ==='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello! Nice to meet you. How are you doing today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-832e778e-20f2-491d-a6c3-e3c36257f7d3`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=16, prompt_tokens=10, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-832e778e-20f2-491d-a6c3-e3c36257f7d3', created=1753077684, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! Nice to meet you. How are you doing today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=16, prompt_tokens=10, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=== openai/gpt-4.1 ==='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I help you today? 😊\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BvdsTGjaiUXsVSh9i6FDb2SMU2vmr`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=10, prompt_tokens=10, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BvdsTGjaiUXsVSh9i6FDb2SMU2vmr', created=1753077685, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I help you today? 😊', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=10, prompt_tokens=10, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for m in ms:\n",
    "    display(f'=== {m} ===')\n",
    "    display(completion(m,msg))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bad470e4",
   "metadata": {},
   "source": [
    "### Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad6fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stream_with_complete(gen):\n",
    "    \"Extend streaming response chunks with the complete response\"\n",
    "    chunks = []\n",
    "    for chunk in gen:\n",
    "        chunks.append(chunk)\n",
    "        yield chunk\n",
    "    return stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc4b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastcore.xtras import SaveReturn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63ce765",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ms[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f16571",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = completion(messages=msg, model=model, stream=True)\n",
    "r2 = SaveReturn(stream_with_complete(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9797136b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello! How are you doing today? Is there anything I can help you with?"
     ]
    }
   ],
   "source": [
    "for o in r2:\n",
    "    cts = o.choices[0].delta.content\n",
    "    if cts: print(cts, end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "897ec073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hello! How are you doing today? Is there anything I can help you with?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-846a2bef-2a87-44de-941a-2091f929fd30`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=20, prompt_tokens=0, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-846a2bef-2a87-44de-941a-2091f929fd30', created=1753077688, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How are you doing today? Is there anything I can help you with?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=20, prompt_tokens=0, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd8e11c0",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61393b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "from toolslm.funccall import get_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4301402e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _lite_mk_func(f):\n",
    "    if isinstance(f, dict): return f\n",
    "    return {'type':'function', 'function':get_schema(f, pname='parameters')}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b103600",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_add(\n",
    "    a: int,   # first operand\n",
    "    b: int=0  # second operand\n",
    ") -> int:\n",
    "    \"Add two numbers together\"\n",
    "    print(f\"TOOL CALLED {a=} + {b=}\")\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fc27b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'function': {'name': 'simple_add',\n",
       "  'description': 'Add two numbers together\\n\\nReturns:\\n- type: integer',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'a': {'type': 'integer', 'description': 'first operand'},\n",
       "    'b': {'type': 'integer', 'description': 'second operand', 'default': 0}},\n",
       "   'required': ['a']}}}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "toolsc = _lite_mk_func(simple_add)\n",
    "toolsc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32754b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_user(s): return {\"role\": \"user\", \"content\": s}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c2af29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmsg = mk_user(\"What is 5478954793+547982745? How about 5479749754+9875438979? Always use tools for calculations, and describe what you'll do before using a tool. Where multiple tool calls are required, do them in a single response where possible.\")\n",
    "r = completion(model, [tmsg], tools=[toolsc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdb1817",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'll help you calculate both of those additions using the simple_add tool. Let me perform both calculations for you:\n",
       "\n",
       "🔧 simple_add({\"a\": 5478954793, \"b\": 547982745})\n",
       "\n",
       "\n",
       "\n",
       "🔧 simple_add({\"a\": 5479749754, \"b\": 9875438979})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-30828398-009f-4eca-ab9f-e96d05b53ca0`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=159, prompt_tokens=475, total_tokens=634, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-30828398-009f-4eca-ab9f-e96d05b53ca0', created=1753077691, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll help you calculate both of those additions using the simple_add tool. Let me perform both calculations for you:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 5478954793, \"b\": 547982745}', name='simple_add'), id='toolu_01J2C6Q72JqHzY33qzfarZeF', type='function'), ChatCompletionMessageToolCall(index=2, function=Function(arguments='{\"a\": 5479749754, \"b\": 9875438979}', name='simple_add'), id='toolu_01NdKo9s2oaknAc3xdFkiNSw', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=159, prompt_tokens=475, total_tokens=634, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d81d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _lite_call_func(tc,ns,raise_on_err=True):\n",
    "    res = call_func(tc.function.name, json.loads(tc.function.arguments),ns=ns)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": str(res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5af607c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=5478954793 + b=547982745\n",
      "TOOL CALLED a=5479749754 + b=9875438979\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'tool_call_id': 'toolu_01J2C6Q72JqHzY33qzfarZeF',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '6026937538'},\n",
       " {'tool_call_id': 'toolu_01NdKo9s2oaknAc3xdFkiNSw',\n",
       "  'role': 'tool',\n",
       "  'name': 'simple_add',\n",
       "  'content': '15355188733'}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tcs = [_lite_call_func(o, ns=globals()) for o in r.choices[0].message.tool_calls]\n",
    "tcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd9871b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def delta_text(msg):\n",
    "    \"Extract printable content from streaming delta, return None if nothing to print\"\n",
    "    c = msg.choices[0]\n",
    "    if not c: return c\n",
    "    if not hasattr(c,'delta'): return None #f'{c}'\n",
    "    delta = msg.choices[0].delta\n",
    "    if delta.content: return delta.content\n",
    "    if delta.tool_calls:\n",
    "        res = ''.join(f\"🔧 {tc.function.name}\" for tc in delta.tool_calls if tc.id and tc.function.name)\n",
    "        if res: return f'\\n{res}'\n",
    "    if hasattr(delta,'reasoning_content'): return '🧠' if delta.reasoning_content else '\\n\\n'\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4790a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'll help you calculate both of those sums using the addition tool. Let me perform both calculations for you:\n",
      "\n",
      "1. First, I'll calculate 5478954793 + 547982745\n",
      "2. Then, I'll calculate 5479749754 + 9875438979\n",
      "🔧 simple_add\n",
      "🔧 simple_add"
     ]
    }
   ],
   "source": [
    "r = completion(messages=[tmsg], model=model, stream=True, tools=[toolsc])\n",
    "r2 = SaveReturn(stream_with_complete(r))\n",
    "for o in r2: print(delta_text(o) or '', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b21118c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'll help you calculate both of those sums using the addition tool. Let me perform both calculations for you:\n",
       "\n",
       "1. First, I'll calculate 5478954793 + 547982745\n",
       "2. Then, I'll calculate 5479749754 + 9875438979\n",
       "\n",
       "🔧 simple_add({\"a\": 5478954793, \"b\": 547982745})\n",
       "\n",
       "\n",
       "\n",
       "🔧 simple_add({\"a\": 5479749754, \"b\": 9875438979})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-624371bc-bf6a-4ceb-8225-29515bf25ea2`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=197, prompt_tokens=0, total_tokens=197, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-624371bc-bf6a-4ceb-8225-29515bf25ea2', created=1753077693, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll help you calculate both of those sums using the addition tool. Let me perform both calculations for you:\\n\\n1. First, I'll calculate 5478954793 + 547982745\\n2. Then, I'll calculate 5479749754 + 9875438979\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5478954793, \"b\": 547982745}', name='simple_add'), id='toolu_01UuRjybMM28vfjVbSyNudxm', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5479749754, \"b\": 9875438979}', name='simple_add'), id='toolu_019m8qsLSmQQZMEDoiPpshBy', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=197, prompt_tokens=0, total_tokens=197, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f355ecbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠🧠\n",
      "\n",
      "I'll find the derivative of f(x) = x³ + 2x² - 5x + 1 using the power rule.\n",
      "\n",
      "**Step-by-step solution:**\n",
      "\n",
      "Using the power rule: if f(x) = xⁿ, then f'(x) = n·xⁿ⁻¹\n",
      "\n",
      "For each term:\n",
      "- d/dx(x³) = 3x²\n",
      "- d/dx(2x²) = 2·2x¹ = 4x  \n",
      "- d/dx(-5x) = -5·1x⁰ = -5\n",
      "- d/dx(1) = 0 (derivative of a constant is zero)\n",
      "\n",
      "**Answer:**\n",
      "f'(x) = 3x² + 4x - 5\n",
      "\n",
      "This derivative tells us the instantaneous rate of change of the original function at any point x."
     ]
    }
   ],
   "source": [
    "msg = mk_user(\"Solve this complex math problem: What is the derivative of x^3 + 2x^2 - 5x + 1?\")\n",
    "r = completion(messages=[msg], model=model, stream=True, reasoning_effort=\"low\")\n",
    "r2 = SaveReturn(stream_with_complete(r))\n",
    "for o in r2: print(delta_text(o) or '', end='')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ff5f1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I'll find the derivative of f(x) = x³ + 2x² - 5x + 1 using the power rule.\n",
       "\n",
       "**Step-by-step solution:**\n",
       "\n",
       "Using the power rule: if f(x) = xⁿ, then f'(x) = n·xⁿ⁻¹\n",
       "\n",
       "For each term:\n",
       "- d/dx(x³) = 3x²\n",
       "- d/dx(2x²) = 2·2x¹ = 4x  \n",
       "- d/dx(-5x) = -5·1x⁰ = -5\n",
       "- d/dx(1) = 0 (derivative of a constant is zero)\n",
       "\n",
       "**Answer:**\n",
       "f'(x) = 3x² + 4x - 5\n",
       "\n",
       "This derivative tells us the instantaneous rate of change of the original function at any point x.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-812acd8e-c1e5-403d-bf9d-8e0ea15991a0`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=395, prompt_tokens=0, total_tokens=395, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=160, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-812acd8e-c1e5-403d-bf9d-8e0ea15991a0', created=1753077697, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"I'll find the derivative of f(x) = x³ + 2x² - 5x + 1 using the power rule.\\n\\n**Step-by-step solution:**\\n\\nUsing the power rule: if f(x) = xⁿ, then f'(x) = n·xⁿ⁻¹\\n\\nFor each term:\\n- d/dx(x³) = 3x²\\n- d/dx(2x²) = 2·2x¹ = 4x  \\n- d/dx(-5x) = -5·1x⁰ = -5\\n- d/dx(1) = 0 (derivative of a constant is zero)\\n\\n**Answer:**\\nf'(x) = 3x² + 4x - 5\\n\\nThis derivative tells us the instantaneous rate of change of the original function at any point x.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None, reasoning_content=\"I need to find the derivative of the function f(x) = x³ + 2x² - 5x + 1.\\n\\nI'll use the power rule for derivatives, which states that if f(x) = xⁿ, then f'(x) = n·xⁿ⁻¹.\\n\\nLet me take the derivative of each term:\\n\\n1) For x³: The derivative is 3x²\\n2) For 2x²: The derivative is 2·2x¹ = 4x\\n3) For -5x: The derivative is -5·1x⁰ = -5\\n4) For 1 (constant): The derivative is 0\\n\\nSo the derivative is: 3x² + 4x - 5\"))], usage=Usage(completion_tokens=395, prompt_tokens=0, total_tokens=395, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=160, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2.value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acd33761",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Otters are fascinating aquatic mammals with some remarkable characteristics:\n",
       "\n",
       "Otters are carnivorous mammals in the subfamily Lutrinae, which is part of the Mustelidae family that includes weasels, badgers, and mink. There are 13 extant otter species that are semiaquatic, aquatic, or marine.\n",
       "\n",
       "**Physical Features:** Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water. Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch. The 13 species range in adult size from 0.6 to 1.8 m in length and 1 to 45 kg in weight.\n",
       "\n",
       "**Behavior:** They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. In winter, otters slide up to 22 feet on ice as their easiest way to get around. Sea otters are one of the only marine mammals that uses tools, using rocks to crack open shellfish.\n",
       "\n",
       "**Diet and Abilities:** Their usual source of food is fish, but they may also eat frogs and birds. River otters can hold their breath for up to 8 minutes, while sea otters have been known to stay submerged for more than 5 minutes. They can swim up to seven miles per hour and dive down 60 feet.\n",
       "\n",
       "**Conservation:** Otters were hunted to near extinction by fur traders in the 18th and 19th centuries, with only about 2,000 remaining scattered in colonies, but many populations have since recovered under protection.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-b8917355-2e2e-4de6-a067-c0552b372fd5`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=665, prompt_tokens=13695, total_tokens=14360, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-b8917355-2e2e-4de6-a067-c0552b372fd5', created=1753077724, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Otters are fascinating aquatic mammals with some remarkable characteristics:\\n\\nOtters are carnivorous mammals in the subfamily Lutrinae, which is part of the Mustelidae family that includes weasels, badgers, and mink. There are 13 extant otter species that are semiaquatic, aquatic, or marine.\\n\\n**Physical Features:** Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water. Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch. The 13 species range in adult size from 0.6 to 1.8 m in length and 1 to 45 kg in weight.\\n\\n**Behavior:** They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. In winter, otters slide up to 22 feet on ice as their easiest way to get around. Sea otters are one of the only marine mammals that uses tools, using rocks to crack open shellfish.\\n\\n**Diet and Abilities:** Their usual source of food is fish, but they may also eat frogs and birds. River otters can hold their breath for up to 8 minutes, while sea otters have been known to stay submerged for more than 5 minutes. They can swim up to seven miles per hour and dive down 60 feet.\\n\\n**Conservation:** Otters were hunted to near extinction by fur traders in the 18th and 19th centuries, with only about 2,000 remaining scattered in colonies, but many populations have since recovered under protection.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': [[{'type': 'web_search_result_location', 'cited_text': 'Otters are carnivorous mammals in the subfamily Lutrinae. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDOV4dkxbOyqFCtM0yxoMuhJTyhPap5NwlmqWIjCTqg1T88o3DKrrv6+jXPL+Ke820caIz+kpNhqifC6mg4vpHx3ghorHn6pSMBOcRKIqE6fJ2ACTuCcOFHjwwHg9yPFPTfAYBA=='}, {'type': 'web_search_result_location', 'cited_text': 'Lutrinae is a branch of the Mustelidae family, which includes weasels, badgers, mink, and wolverines, among other animals. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDMAR400eMDiG+0wv/BoMq/2JOVR5ajDuhpigIjD5EQAD2JbzQDPaZEpbE0hZZ920JtXwzcmY5K4dw5lNGyUcl184IMNP9sDeK6AmaHwqE/K82J5lFhnibDcRJnMBmkQlus8YBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'The 13 extant otter species are all semiaquatic, aquatic, or marine. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDKXD4kSqfTNLxvjdexoMDJT09XJDNBAnj7HoIjB+ejTmF9Avg6gVhR0gwGZDvr+ZBeQS2GgCn1qLzJGBT5kFMU9zMsvO7s6HYJ9IJUcqE4aKEFkO6bc24ogkCnO10slUyhkYBA=='}, {'type': 'web_search_result_location', 'cited_text': 'There are 13 known species of otters, ranging in size and habitat preferences, with some species adapted to cold waters requiring a high metabolic rat...', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDCzHZmnrDOyfM4082hoMT9Ii+vps3ThXd2jKIjBqTRxY9y3bYpyI3kMWRIYW9yrQMw1pYiBWdWb0FBiV1zRwyRjRjdNp17vIs1/s8+0qFJciEzRz/GvMuWD7+b0z+QCTChSWGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in wate...', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDJtUWSllG+HqjmApBRoM7JpqN8TLWi/fwmFjIjBqPkdCOndwNqtGoiY4KMCH79VD5Vb1MQvvh5hLUBiooPg/BZkOXFcDe8W2maVy/jEqE6IcUJebEtl5lFXIRz4mAOryvpgYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch. ', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDMCxSyRXGl/SqGRgnRoM38iKWZGuHjiF2rgqIjAaUCBrSEUk1Dndsqk2sx5wCJvSnRG+0P9rE6pM3/ORKHC1uTLvqHIQTIL3rJCWeN4qE37UjClmN6J8dHCoDSJwhNOF5bAYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'The 13 species range in adult size from 0.6 to 1.8 m (2.0 to 5.9 ft) in length and 1 to 45 kg (2.2 to 99.2 lb) in weight. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDNMAo1c6gxkt9movNxoM+bX3yhCFDlnU2Z6TIjDdLSqYEM9R/Kme1nFlQjOceBL32IV9p54hnlMzJS2c6sis8fPC86V0OBKl0aVShEYqFCEkZ0v+s9ivb+VOnbLza4MDW/YRGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones.', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDOGsGYUUJrBwcaJqQhoM07g2zT6dGYfafQtIIjCHv7VRkPPEzBvJiD1wQMgmJdfY5wU4/2UNHK7WZjiafq6Vw97Pq9ptUZ6gd8/S/NkqEzMhsgJileah7RGxXVFq2M/OEoQYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'In the winter, otters have found the easiest and perhaps most fun way to get around is by sliding. After a few bumps, they can slide up to 22 feet on ...', 'url': 'https://www.nationalforests.org/blog/seven-quick-facts-about-river-otters', 'title': 'Seven Quick Facts About River Otters - National Forest Foundation', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDAwJIbcnREbWi52aaxoMZ2eDUQIy21E5TVddIjCBTzXlLaOA70bu9L7wLqFZuKPKFEr5ArOIEJOGgMqBe25hbv1/J2mSwkRNNSaUamEqFxgb4rQTB2fPijChSqosLn5fChVhgH/pGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters are one of the only marine mammals that uses tools. All of their favorite foods need to be cracked open to eat, so these intelligent animal...', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDCTR6HpN41FA4h1lBhoMfzqaJW0t8/priQHtIjAiOJlIsxSyKIG2GTyZ8l8/l+BPphcNzi1+ausfQuLQ8t86gtU09su7l1+aFQ1vgdgqF/nZvOThjLJuCSZgLJIjllVRrg83Jg4nGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Its usual source of food is fish, and further downriver, eels, but it may sample frogs and birds. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDJahW5uDTiKCJtSKnRoMVFTzBpmsmlcHjAm3IjCzLgIyksMEB5BGT2043ypzgzZWCSUgHs76CFEN0B8aGiOZxMynsLyVo3iotXFJGoAqFNGkDMTkfmXjPtTJcOP6ybI04fldGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'River otters, however, can hold their breath for up to 8 minutes. ', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDJEAZD4a8KPwvdRvMhoMezDDzNxmo3Wh8fg7IjAgeFoZIp+XQlAF1q3Ce7gMhVR+uzvvWq5VaWgIMaPFA7lq1U/4QaGOXagosNl/eKIqFLtgM+z2z0hdj8dhtzHG2X7dGgqBGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters have been known to stay submerged for more than 5 minutes at a time. ', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDGGWyRIjDf2f+iSz4hoMtIzsAH5ZN2PFXZHQIjAEblHyFL0QUYX5uMKT+YX9Tiqctl+bUkjFToMKZ0i9AuiC0Dag97PkyCe67UhkjBEqFJRur6ZJAOJSdRQ3BFNBCiFY90RSGAQ='}], [{'type': 'web_search_result_location', 'cited_text': '... As a sometimes aquatic creature, it shouldn’t be a surprise that otters can swim up to seven miles per hour and dive down 60 feet. ', 'url': 'https://www.nationalforests.org/blog/seven-quick-facts-about-river-otters', 'title': 'Seven Quick Facts About River Otters - National Forest Foundation', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDBn+rK5rbMBYf6Oo1RoMbAii5i8pdJgkcvI1IjDXmgwBzLErBJZUJLjIWepvDhxDxjdH72u6rYx75aR9n3yQpjLg+chhXqezLDZ2zIkqE7Xm08FGy4BohZ5DMItyxXeBMSoYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'Hunted to the edge of extinction by fur traders in the 18th and 19th centuries, the few remaining sea otters (about 2,000 scattered in remnant colonie...', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDF+/XfeuFYTKkrsDyBoMBYmV6k2Rzl4SheX+IjCAkMUcsxfqig2LuyZ6DEf9MLtLY1g7PgeOKPk/ipJsr42TmENA/xYsuaBEgM6suJEqFIlkWc8yCukmmrIwJ01hBorhUmJvGAQ='}]], 'thinking_blocks': None}))], usage=Usage(completion_tokens=665, prompt_tokens=13695, total_tokens=14360, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool = { \"type\": \"web_search_20250305\", \"name\": \"web_search\", \"max_uses\": 3}\n",
    "smsg = mk_user(\"Search the web and tell me very briefly about otters\")\n",
    "r = completion(ms[1], [smsg], tools=[search_tool])\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35497260",
   "metadata": {},
   "source": [
    "Use streaming to get properly located citations. Otherwise, they are all included in a separate dict entry:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf84f8c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'web_search_result_location',\n",
       "  'cited_text': 'Otters are carnivorous mammals in the subfamily Lutrinae. ',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Otter',\n",
       "  'title': 'Otter - Wikipedia',\n",
       "  'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDOV4dkxbOyqFCtM0yxoMuhJTyhPap5NwlmqWIjCTqg1T88o3DKrrv6+jXPL+Ke820caIz+kpNhqifC6mg4vpHx3ghorHn6pSMBOcRKIqE6fJ2ACTuCcOFHjwwHg9yPFPTfAYBA=='},\n",
       " {'type': 'web_search_result_location',\n",
       "  'cited_text': 'Lutrinae is a branch of the Mustelidae family, which includes weasels, badgers, mink, and wolverines, among other animals. ',\n",
       "  'url': 'https://en.wikipedia.org/wiki/Otter',\n",
       "  'title': 'Otter - Wikipedia',\n",
       "  'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDMAR400eMDiG+0wv/BoMq/2JOVR5ajDuhpigIjD5EQAD2JbzQDPaZEpbE0hZZ920JtXwzcmY5K4dw5lNGyUcl184IMNP9sDeK6AmaHwqE/K82J5lFhnibDcRJnMBmkQlus8YBA=='}]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r.choices[0].message.provider_specific_fields['citations'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29018310",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da9223dc",
   "metadata": {},
   "source": [
    "Litellm is pretty bare bones. It doesnt keep track of conversation history or anything.\n",
    "\n",
    "So lets make a claudette style wrapper so we can do streaming, toolcalling, and toolloops without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266f3d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Chat:\n",
    "    def __init__(self, model:str, sp='', temp=0, tools:list=None, hist:list=None, ns:Optional[dict]=None):\n",
    "        \"LiteLLM chat client.\"\n",
    "        self.model = model\n",
    "        hist,tools = listify(hist),listify(tools)\n",
    "        if ns is None and tools: ns = mk_ns(tools)\n",
    "        elif ns is None: ns = globals()\n",
    "        self.tool_schemas = [_lite_mk_func(t) for t in tools] if tools else None\n",
    "        store_attr()\n",
    "    \n",
    "    def _prepare_msgs(self, msg=None):\n",
    "        \"Prepare the messages list for the API call\"\n",
    "        msgs = [{\"role\": \"system\", \"content\": self.sp}] if self.sp else []\n",
    "        self.hist += [{\"role\": \"user\", \"content\": msg}] if isinstance(msg, str) \\\n",
    "            else [msg] if isinstance(msg, dict) \\\n",
    "            else [] if msg is None \\\n",
    "            else msg\n",
    "        return msgs + [m if isinstance(m, dict) else m.model_dump() for m in self.hist]\n",
    "\n",
    "    def _call(self, msg=None, stream=False, max_tool_rounds=1, tool_round=0, final_prompt=None, tool_choice=None, **kwargs):\n",
    "        \"Internal method that always yields responses\"\n",
    "        msgs = self._prepare_msgs(msg)\n",
    "        res = completion(model=self.model, messages=msgs, stream=stream, \n",
    "                         tools=self.tool_schemas, temperature=self.temp, **kwargs)\n",
    "        if stream: res = yield from stream_with_complete(res)        \n",
    "        m = res.choices[0].message\n",
    "        self.hist.append(m)\n",
    "        yield res\n",
    "\n",
    "        if tcs := m.tool_calls:\n",
    "            tool_results = [_lite_call_func(tc, ns=self.ns) for tc in tcs]\n",
    "            if tool_round>=max_tool_rounds-1:\n",
    "                tool_results += ([{\"role\": \"user\", \"content\": final_prompt}] if final_prompt else [])\n",
    "                tool_choice='none'\n",
    "            yield from self._call(\n",
    "                tool_results, stream, max_tool_rounds, tool_round+1,\n",
    "                final_prompt, tool_choice=tool_choice, **kwargs)\n",
    "    \n",
    "    def __call__(self, msg=None, stream=False, max_tool_rounds=1,\n",
    "                 final_prompt=None, return_all=False, **kwargs):\n",
    "        \"Main call method - handles streaming vs non-streaming\"\n",
    "        result_gen = self._call(msg, stream, max_tool_rounds, 0, final_prompt, **kwargs)     \n",
    "        if stream: return result_gen              # streaming\n",
    "        elif return_all: return list(result_gen)  # toolloop behavior\n",
    "        else: return last(result_gen)             # normal chat behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dbf319a",
   "metadata": {},
   "source": [
    "### Test history tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957ccd1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Rens! Nice to meet you. How can I help you today? 😊\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-Bvdt6a06SDhMxMLHVzMjy7D7aCgWY`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=13, total_tokens=30, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-Bvdt6a06SDhMxMLHVzMjy7D7aCgWY', created=1753077724, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hi Rens! Nice to meet you. How can I help you today? 😊', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=17, prompt_tokens=13, total_tokens=30, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(m)\n",
    "res = chat(\"Hey my name is Rens\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5978da56",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Rens!\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-Bvdt7VVxZC4reQJtlTZajPI2cy9OB`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=6, prompt_tokens=41, total_tokens=47, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-Bvdt7VVxZC4reQJtlTZajPI2cy9OB', created=1753077725, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Your name is Rens!', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=6, prompt_tokens=41, total_tokens=47, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"Whats my name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dbb77c7",
   "metadata": {},
   "source": [
    "See now we keep track of history!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46716033",
   "metadata": {},
   "source": [
    "### Testing streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fe74496",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1  \n",
      "2  \n",
      "3  \n",
      "4  \n",
      "5"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1  \n",
       "2  \n",
       "3  \n",
       "4  \n",
       "5\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-Bvdt81J46VtYxHcbnTPvkrn3zrpUp`\n",
       "- model: `gpt-4.1`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=9, prompt_tokens=11, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-Bvdt81J46VtYxHcbnTPvkrn3zrpUp', created=1753077726, model='gpt-4.1', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='1  \\n2  \\n3  \\n4  \\n5', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=9, prompt_tokens=11, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from time import sleep\n",
    "chat2 = Chat(m)\n",
    "stream_gen = chat2(\"Count to 5\", stream=True)\n",
    "for chunk in stream_gen:\n",
    "    sleep(0.1)  # for effect\n",
    "    if isinstance(chunk, ModelResponse): display(chunk)\n",
    "    else: print(delta_text(chunk) or '',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8c3666",
   "metadata": {},
   "source": [
    "## Test tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf17377a",
   "metadata": {},
   "source": [
    "Ok now lets test tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c4cf429",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'=== gemini/gemini-2.5-flash ==='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=5 + b=3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "5 + 3 = 8.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `4dd9aJnDOLCymtkP7J2e-Qg`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=8, prompt_tokens=106, total_tokens=114, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=106, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='4dd9aJnDOLCymtkP7J2e-Qg', created=1753077729, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 = 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=8, prompt_tokens=106, total_tokens=114, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=106, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=== claude-sonnet-4-20250514 ==='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=5 + b=3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "5 + 3 = 8\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-3cf014a8-8f64-473d-afa1-a1667c2b39d0`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=12, prompt_tokens=527, total_tokens=539, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-3cf014a8-8f64-473d-afa1-a1667c2b39d0', created=1753077735, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 = 8', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=12, prompt_tokens=527, total_tokens=539, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "'=== openai/gpt-4.1 ==='"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=5 + b=3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "5 + 3 equals 8.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BvdtJzBUThv9X0lPKzczTIRJwV2HA`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=9, prompt_tokens=101, total_tokens=110, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BvdtJzBUThv9X0lPKzczTIRJwV2HA', created=1753077737, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 equals 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=9, prompt_tokens=101, total_tokens=110, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for m in ms:\n",
    "    display(f'=== {m} ===')\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's 5 + 3?\")\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd9a499",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Otters are carnivorous mammals in the subfamily Lutrinae with 13 extant species that are all semiaquatic, aquatic, or marine. They're distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water.\n",
       "\n",
       "They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch, and an otter's lung capacity is 2.5 times greater than that of similar-sized land mammals, with sea otters able to stay submerged for more than 5 minutes at a time.\n",
       "\n",
       "Their diet mainly consists of fish and sometimes frogs, birds, or shellfish, depending on the species. Sea otters are one of the only marine mammals that uses tools - these intelligent animals will use a rock to crack open their favorite foods.\n",
       "\n",
       "Hunted to the edge of extinction by fur traders in the 18th and 19th centuries, the few remaining sea otters were first protected by the International Fur Seal Treaty in 1911, and today approximately 90 percent of the world's sea otters live in coastal Alaska.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-29063e3d-4236-449a-a8c4-6b5694a3d6fa`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=507, prompt_tokens=13695, total_tokens=14202, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-29063e3d-4236-449a-a8c4-6b5694a3d6fa', created=1753077751, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Otters are carnivorous mammals in the subfamily Lutrinae with 13 extant species that are all semiaquatic, aquatic, or marine. They're distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water.\\n\\nThey are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch, and an otter's lung capacity is 2.5 times greater than that of similar-sized land mammals, with sea otters able to stay submerged for more than 5 minutes at a time.\\n\\nTheir diet mainly consists of fish and sometimes frogs, birds, or shellfish, depending on the species. Sea otters are one of the only marine mammals that uses tools - these intelligent animals will use a rock to crack open their favorite foods.\\n\\nHunted to the edge of extinction by fur traders in the 18th and 19th centuries, the few remaining sea otters were first protected by the International Fur Seal Treaty in 1911, and today approximately 90 percent of the world's sea otters live in coastal Alaska.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': [[{'type': 'web_search_result_location', 'cited_text': 'Otters are carnivorous mammals in the subfamily Lutrinae. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDCRegPsHodji3a0z9RoMVoaiKW3MIigC/pwfIjBujDexOiq9YDNu9uz2ohDA9EKOCUS4+ovogVErpvXJb5c/t7NxwfKpLBvFkM4u/u4qE8jMXzqDcT24sndUPJWXqWHsnxUYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'The 13 extant otter species are all semiaquatic, aquatic, or marine. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDL/Slah1z5RgZKDiRxoM87QtVYyIhgB04XOTIjDKVWUwB9ooMYLW1wxjrDtnSD0KMdLoQB24BNIZYJdUUHKWGcviNJsjw1i3GNqxZbQqE13DvZm8RLlhK+ZcPspHU195GRQYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in wate...', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDBJBYSf/1iFabwKCJhoMybo7/78LZ9z4cmgKIjAH6Rhabi84l3ke8toDCcuxV+7KJSitRh4uKXAYw0R6JfeQpqW7pfWgJeYIpnktbOMqEy/nUMLId+bgQxPiAUHU69u+sygYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones.', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDLedbmWAIH2DuAc4oxoMI1xMZt6ZBAR9jOSBIjARrkS9a9ut5SdnVucPW95t0UjJ+Xh7phRktEsnOb4dzWQp9hDjA2GKfDsfwCoRLQEqE5qb/auoCWkDIeue65XqPxqbD+QYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch. ', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDH7ciIxdGpp/7pHGsBoM6uWsX2ZyQy1He2W3IjCQrfwm6OVylVANQ2tmN2JeIZeBurnik6ldQzH1hxRj69Ox8Tp7gfA92Oxad+ssxvwqEzB2PkMTPoRQtjsbqjJPUjZ56LwYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'An otter’s lung capacity is 2.5 times greater than that of similar-sized land mammals. Sea otters have been known to stay submerged for more than 5 mi...', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDEFjQziRbMjAcZBs4xoMI4aD+SqazUQvN5sbIjAf/mMleu2quh8QODoQbW5xQdkyyhgtbEikkjwb/MfNVwO1HwYghZy+ma1dkaHsm+YqF3/E9xb1CeBIKXulVl9MYA/kGe86EYOfGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'They can live up to 16 years, with their diet mainly consisting of fish and sometimes frogs, birds, or shellfish, depending on the species.', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDEN0w4YD55921Mul8RoMhJp9jMeLzokspdKMIjC2SIF9cKHmlSKang12bcuvjoY9Fc9Ui1eW8UAfE5UanY22fOzWR1A1TqpypX6VlgUqFAaxBkz74fudGIFgFUg7B//7pkyvGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters are one of the only marine mammals that uses tools. All of their favorite foods need to be cracked open to eat, so these intelligent animal...', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDGRqaIxa4k+YAH6ZRhoM3pXbLKgBx6II+KkKIjBC4SZfzfHUSWzIQy/jOdqbDL8cKayJLl99r6diNdgg54HmkUFXCVxNVt8H7eQP358qF7I3wHSwxSKAN1XLDUAnRWMJRyUTNEzKGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Hunted to the edge of extinction by fur traders in the 18th and 19th centuries, the few remaining sea otters (about 2,000 scattered in remnant colonie...', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDMTUyGnmEJUfH1OWvBoMMxs2yMpZZgw/xgkxIjBxlDHqerKwiimLHWWLvhmb0xNdA6qu1VX02n5JJao+GLk2Yg7cVLl79o3tPMar+VoqFDRH33PIKXTyLzo9+GKLQSKgqBRGGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Approximately 90 percent of the world’s sea otters live in coastal Alaska. ', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDAofqciiZQs0hhAKQxoMTjCdrJPyfZKplWSzIjAsQ6vQ9EoRWf9fRaFhLiXaWwn+hLzxuSGmtBf/kRDqMHyLh+FNN00zHaUgoQrG4vAqFKDh+HRLDDxmxKA9VI7QhML5RB1JGAQ='}]], 'thinking_blocks': None}))], usage=Usage(completion_tokens=507, prompt_tokens=13695, total_tokens=14202, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(ms[1], tools=[search_tool])\n",
    "res = chat(\"Search the web and tell me very briefly about otters\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8cce2d9",
   "metadata": {},
   "source": [
    "## Test multi tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "662f7aa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=8 + b=7\n",
      "TOOL CALLED a=15 + b=11\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'll solve this step by step using the addition function.\n",
       "\n",
       "First, let me calculate 5 + 3:\n",
       "\n",
       "🔧 simple_add({\"a\": 5, \"b\": 3})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-226fd536-b757-4990-8ac4-45333928e431`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=96, prompt_tokens=434, total_tokens=530, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-226fd536-b757-4990-8ac4-45333928e431', created=1753077754, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll solve this step by step using the addition function.\\n\\nFirst, let me calculate 5 + 3:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), id='toolu_0163YPp9zhZqdK6jVd5tSeC9', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=96, prompt_tokens=434, total_tokens=530, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Now I'll add 7 to that result (8 + 7):\n",
       "\n",
       "🔧 simple_add({\"a\": 8, \"b\": 7})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-db2a71fc-f6d2-47e0-b73a-9eb2b5f26d6f`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=88, prompt_tokens=543, total_tokens=631, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-db2a71fc-f6d2-47e0-b73a-9eb2b5f26d6f', created=1753077757, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"Now I'll add 7 to that result (8 + 7):\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 8, \"b\": 7}', name='simple_add'), id='toolu_01FeJrSkH9q3BxqHyatgKiZc', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=88, prompt_tokens=543, total_tokens=631, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Finally, I'll add 11 to that result (15 + 11):\n",
       "\n",
       "🔧 simple_add({\"a\": 15, \"b\": 11})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-a2b0069e-1aa5-4558-a88c-c737f3b5b025`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=89, prompt_tokens=644, total_tokens=733, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-a2b0069e-1aa5-4558-a88c-c737f3b5b025', created=1753077759, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"Finally, I'll add 11 to that result (15 + 11):\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 15, \"b\": 11}', name='simple_add'), id='toolu_01BePXmENDDQDeKsABuo4o91', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=89, prompt_tokens=644, total_tokens=733, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "So working step by step:\n",
       "- 5 + 3 = 8\n",
       "- 8 + 7 = 15  \n",
       "- 15 + 11 = 26\n",
       "\n",
       "Therefore, ((5 + 3) + 7) + 11 = **26**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-5bc6e20b-0f4a-49dd-b161-e4e54db6f31d`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=68, prompt_tokens=746, total_tokens=814, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-5bc6e20b-0f4a-49dd-b161-e4e54db6f31d', created=1753077761, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='So working step by step:\\n- 5 + 3 = 8\\n- 8 + 7 = 15  \\n- 15 + 11 = 26\\n\\nTherefore, ((5 + 3) + 7) + 11 = **26**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=68, prompt_tokens=746, total_tokens=814, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "chat = Chat(model, tools=[simple_add])\n",
    "res = chat(\"What's ((5 + 3)+7)+11? Work step by step\", return_all=True, max_tool_rounds=5)\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9de676",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(as_prop=True)\n",
    "def cost(self: Chat):\n",
    "    \"Total cost of all responses in conversation history\"\n",
    "    return sum(getattr(r, '_hidden_params', {}).get('response_cost')  or 0\n",
    "               for r in self.h if hasattr(r, 'choices'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f66101",
   "metadata": {},
   "source": [
    "Some models support parallel tool calling. I.e. sending multiple tool call requests in one conversation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ec77539",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=7 + b=2\n",
      "MULTIPLY: 8 * 9\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "🔧 simple_add({\"a\": 5, \"b\": 3})\n",
       "\n",
       "\n",
       "\n",
       "🔧 simple_add({\"a\": 7, \"b\": 2})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-Bvdtiy7zTv7EH5MWjV2Zf584QF8Ns`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=52, prompt_tokens=110, total_tokens=162, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-Bvdtiy7zTv7EH5MWjV2Zf584QF8Ns', created=1753077762, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), id='call_t5ELrVwAH05QAPcIS022fZIF', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 7, \"b\": 2}', name='simple_add'), id='call_SMsvISRs5lLAo9nxfmVoHggp', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=52, prompt_tokens=110, total_tokens=162, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "🔧 multiply({\"a\":8,\"b\":9})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-Bvdtjs2e1szA0uQVyW1zHxXdPnzlf`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=178, total_tokens=195, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-Bvdtjs2e1szA0uQVyW1zHxXdPnzlf', created=1753077763, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":8,\"b\":9}', name='multiply'), id='call_NW3lLNJ5JB6Z4JNqwa62HV50', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=17, prompt_tokens=178, total_tokens=195, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "(5 + 3) = 8 and (7 + 2) = 9. Multiplying them together: 8 × 9 = 72.\n",
       "\n",
       "So, (5 + 3) × (7 + 2) = 72.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BvdtjudM6i6WJwHywkDYvLfGNyVeu`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=54, prompt_tokens=203, total_tokens=257, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BvdtjudM6i6WJwHywkDYvLfGNyVeu', created=1753077763, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='(5 + 3) = 8 and (7 + 2) = 9. Multiplying them together: 8 × 9 = 72.\\n\\nSo, (5 + 3) × (7 + 2) = 72.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=54, prompt_tokens=203, total_tokens=257, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"Multiply two numbers\"\n",
    "    print(f\"MULTIPLY: {a} * {b}\")\n",
    "    return a * b\n",
    "\n",
    "chat = Chat(ms[-1], tools=[simple_add, multiply])\n",
    "res = chat(\"Calculate (5 + 3) * (7 + 2)\", max_tool_rounds=5, return_all=True)\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dba4958f",
   "metadata": {},
   "source": [
    "See it did the additions in one go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33b17e71",
   "metadata": {},
   "source": [
    "Hit max_tool_rounds limit with final_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa7298c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=10 + b=5\n",
      "MULTIPLY: 15 * 3\n",
      "Got 3 responses\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "🔧 simple_add({\"a\":10,\"b\":5})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BvdtkErTK2l94Cf5C7o47aj5RHWyL`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=18, prompt_tokens=146, total_tokens=164, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BvdtkErTK2l94Cf5C7o47aj5RHWyL', created=1753077764, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":10,\"b\":5}', name='simple_add'), id='call_T7a6CDqyQTslAlpfQmaTVBpR', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=18, prompt_tokens=146, total_tokens=164, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "\n",
       "\n",
       "🔧 multiply({\"a\":15,\"b\":3})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BvdtlxEeR26kOp6xlSiwMKyru0nHM`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=173, total_tokens=190, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BvdtlxEeR26kOp6xlSiwMKyru0nHM', created=1753077765, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":15,\"b\":3}', name='multiply'), id='call_k7K7ZCVsY6hdugMCj2uzCFQD', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=17, prompt_tokens=173, total_tokens=190, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Here’s what I’ve calculated so far, step by step:\n",
       "\n",
       "1. First, I added 10 + 5 to get 15.\n",
       "2. Next, I multiplied the result (15) by 3 to get 45.\n",
       "\n",
       "So, ((10 + 5) * 3) = 45.\n",
       "\n",
       "The next step is to calculate the denominator (2 + 1) and then divide 45 by that result. Would you like me to continue?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BvdtmXBxPxvUYlkOvx3CQgO7lPtb5`\n",
       "- model: `gpt-4.1-2025-04-14`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=95, prompt_tokens=209, total_tokens=304, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BvdtmXBxPxvUYlkOvx3CQgO7lPtb5', created=1753077766, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Here’s what I’ve calculated so far, step by step:\\n\\n1. First, I added 10 + 5 to get 15.\\n2. Next, I multiplied the result (15) by 3 to get 45.\\n\\nSo, ((10 + 5) * 3) = 45.\\n\\nThe next step is to calculate the denominator (2 + 1) and then divide 45 by that result. Would you like me to continue?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=95, prompt_tokens=209, total_tokens=304, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def divide(a: int, b: int) -> float:\n",
    "    \"Divide two numbers\"\n",
    "    display(f\"DIVIDE: {a} / {b}\")\n",
    "    return a / b\n",
    "\n",
    "chat = Chat(m, tools=[simple_add, multiply, divide])\n",
    "res = chat(\"Calculate ((10 + 5) * 3) / (2 + 1) step by step\", \n",
    "           max_tool_rounds=2, return_all=True,\n",
    "           final_prompt=\"Please summarize what you've calculated so far\")\n",
    "print(f\"Got {len(res)} responses\")\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9764dc",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e677e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea23546",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
