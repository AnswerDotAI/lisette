{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> lisette core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import litellm, json\n",
    "from litellm import completion, stream_chunk_builder\n",
    "from litellm.utils import function_to_dict\n",
    "from toolslm.funccall import mk_ns, call_func\n",
    "from typing import Optional\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Litellm provides an easy wrapper for most big LLM providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = [\"gemini/gemini-2.5-flash-preview-04-17\", \"claude-sonnet-4-20250514\", \"openai/gpt-4o-mini\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gemini/gemini-2.5-flash-preview-04-17 ===\n",
      "ModelResponse(id='PfJjaMjXBJfOvdIPiNSzoQU', created=1751380539, model='gemini-2.5-flash-preview-04-17', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hi there! How can I help?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=282, prompt_tokens=4, total_tokens=286, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=274, rejected_prediction_tokens=None, text_tokens=8), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n",
      "=== claude-sonnet-4-20250514 ===\n",
      "ModelResponse(id='chatcmpl-261449a9-cfaa-4e94-b998-380d8252cec5', created=1751380544, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How are you doing today? Is there anything I can help you with?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=20, prompt_tokens=10, total_tokens=30, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n",
      "=== openai/gpt-4o-mini ===\n",
      "ModelResponse(id='chatcmpl-BoWNEogVzOogOrcHTQyN6daCncuOc', created=1751380544, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I assist you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=9, prompt_tokens=10, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n"
     ]
    }
   ],
   "source": [
    "for m in ms:\n",
    "    print(f'=== {m} ===')\n",
    "    res = completion(m,[{'role':'user','content':'Hey there!'}])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add a wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _repr_markdown_(self: litellm.ModelResponse):\n",
    "    # Extract content from the response\n",
    "    message = self.choices[0].message\n",
    "    if message.content:\n",
    "        content = message.content\n",
    "    elif message.tool_calls:\n",
    "        # Show tool calls in a nice format\n",
    "        tool_calls = [f\"ðŸ”§ {tc.function.name}({tc.function.arguments})\\n\" for tc in message.tool_calls]\n",
    "        content = \"\\n\".join(tool_calls)\n",
    "    else:\n",
    "        content = str(message)\n",
    "    \n",
    "    # Create details section\n",
    "    details = []\n",
    "    details.append(f\"id: `{self.id}`\")\n",
    "    details.append(f\"model: `{self.model}`\")\n",
    "    details.append(f\"finish_reason: `{self.choices[0].finish_reason}`\")\n",
    "    if hasattr(self, 'usage') and self.usage:\n",
    "        details.append(f\"usage: `{self.usage}`\")\n",
    "    \n",
    "    det_str = '\\n- '.join(details)\n",
    "    \n",
    "    return f\"\"\"{content}\n",
    "\n",
    "<details>\n",
    "\n",
    "- {det_str}\n",
    "\n",
    "</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gemini/gemini-2.5-flash-preview-04-17 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hey there! What can I do for you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `UvJjaPy-MPbYvdIP27yM6Qo`\n",
       "- model: `gemini-2.5-flash-preview-04-17`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=319, prompt_tokens=4, total_tokens=323, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=308, rejected_prediction_tokens=None, text_tokens=11), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='UvJjaPy-MPbYvdIP27yM6Qo', created=1751380560, model='gemini-2.5-flash-preview-04-17', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hey there! What can I do for you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=319, prompt_tokens=4, total_tokens=323, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=308, rejected_prediction_tokens=None, text_tokens=11), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== claude-sonnet-4-20250514 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello! Nice to meet you. How are you doing today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-f71641c2-2e18-46f1-8d52-7e098bf7ba33`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=16, prompt_tokens=10, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-f71641c2-2e18-46f1-8d52-7e098bf7ba33', created=1751380566, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! Nice to meet you. How are you doing today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=16, prompt_tokens=10, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== openai/gpt-4o-mini ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWNbDOexmZTJtOBPGUegSfjBTB4V`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=9, prompt_tokens=10, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWNbDOexmZTJtOBPGUegSfjBTB4V', created=1751380567, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I assist you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=9, prompt_tokens=10, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for m in ms:\n",
    "    print(f'=== {m} ===')\n",
    "    res = completion(m,[{'role':'user','content':'Hey there!'}])\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Litellm is pretty bare bones. It doesnt keep track of conversation history or anything.\n",
    "\n",
    "So lets make a claudette style wrapper so we can do streaming, toolcalling, and toolloops without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stream_with_complete(gen):\n",
    "    \"Extend streaming response chunks with the complete response\"\n",
    "    chunks = []\n",
    "    for chunk in gen:\n",
    "        chunks.append(chunk)\n",
    "        yield chunk\n",
    "    return stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Chat:\n",
    "    def __init__(self, model: str, sp='', temp=0, tools: list = None, \n",
    "                 hist: list = None, ns: Optional[dict] = None):\n",
    "        \"LiteLLM chat client.\"\n",
    "        self.model = model\n",
    "        if hist is None: hist = []\n",
    "        if tools is None: tools = []\n",
    "        \n",
    "        # Set up namespace following claudette pattern\n",
    "        if ns is None and tools: ns = mk_ns(tools)\n",
    "        elif ns is None: ns = globals()\n",
    "        \n",
    "        # Cache tool schemas\n",
    "        self.tool_schemas = [{'type':'function', 'function':function_to_dict(t)} for t in tools] if tools else None\n",
    "        self.h, self.sp, self.temp, self.tools, self.ns = hist, sp, temp, tools, ns\n",
    "    \n",
    "    def _prepare_messages(self, msg=None):\n",
    "        \"Prepare the messages list for the API call\"\n",
    "        messages = [{\"role\": \"system\", \"content\": self.sp}] if self.sp else []\n",
    "        \n",
    "        if isinstance(msg, str): self.h.append({\"role\": \"user\", \"content\": msg})\n",
    "        elif isinstance(msg, dict): self.h.append(msg)\n",
    "        elif isinstance(msg, list): self.h.extend(msg)\n",
    "        elif msg is None: pass\n",
    "        else: raise ValueError(f\"Can't parse {msg=}\")\n",
    "            \n",
    "        for m in self.h: messages.append(m if isinstance(m, dict) else m.model_dump())\n",
    "        return messages\n",
    "    \n",
    "    def _call(self, msg=None, stream=False, max_tool_rounds=1, tool_round=0, \n",
    "              cont_func=noop, final_prompt=None, **kwargs):\n",
    "        \"Internal call method that always yields responses\"\n",
    "        messages = self._prepare_messages(msg)\n",
    "        \n",
    "        # Make the API call\n",
    "        res = litellm.completion(model=self.model, messages=messages, stream=stream, \n",
    "                               tools=self.tool_schemas, temperature=self.temp, **kwargs)\n",
    "        \n",
    "        if stream: res = yield from stream_with_complete(res)        \n",
    "\n",
    "        m = res.choices[0].message\n",
    "        self.h.append(m)\n",
    "        yield res\n",
    "\n",
    "        \n",
    "        if tcs := m.tool_calls:\n",
    "            tool_results = [_lite_call_func(tc, ns=self.ns) for tc in tcs]\n",
    "            \n",
    "            # Check continuation function: user_msg, llm_response, tool_results\n",
    "            user_msg = self.h[-2] if len(self.h) >= 2 else None\n",
    "            if not cont_func(user_msg, m, tool_results): return\n",
    "                \n",
    "            # Continue with more rounds or final round\n",
    "            if tool_round < max_tool_rounds - 1:\n",
    "                yield from self._call(tool_results, stream, max_tool_rounds, tool_round+1, cont_func, final_prompt, **kwargs)\n",
    "            else:\n",
    "                # Final round - inject final_prompt if provided and set tool_choice=None\n",
    "                final_msg = tool_results + ([{\"role\": \"user\", \"content\": final_prompt}] if final_prompt else [])\n",
    "                yield from self._call(final_msg, stream, max_tool_rounds, tool_round+1, cont_func, final_prompt, tool_choice='none', **kwargs)\n",
    "    \n",
    "    def __call__(self, msg=None, stream=False, max_tool_rounds=1, cont_func=noop, final_prompt=None, return_all=False, **kwargs):\n",
    "        \"Main call method - handles streaming vs non-streaming\"\n",
    "        result_gen = self._call(msg, stream, max_tool_rounds, 0, cont_func, final_prompt, **kwargs)        \n",
    "        if stream: return result_gen              # streaming\n",
    "        elif return_all: return list(result_gen)  # toolloop behavior\n",
    "        else: return last(result_gen)             # normal chat behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test history tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Rens! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWPUgqbgQcmPT7eorIAn3ivT6LvJ`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=11, prompt_tokens=13, total_tokens=24, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWPUgqbgQcmPT7eorIAn3ivT6LvJ', created=1751380684, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hi Rens! How can I assist you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=11, prompt_tokens=13, total_tokens=24, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(m)\n",
    "res = chat(\"Hey my name is Rens\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Rens! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWPVFzKbkud61mWw3gNvx1fVMZjI`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=13, prompt_tokens=35, total_tokens=48, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWPVFzKbkud61mWw3gNvx1fVMZjI', created=1751380685, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Your name is Rens! How can I help you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=13, prompt_tokens=35, total_tokens=48, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"Whats my name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See now we keep track of history!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming:\n",
      "1, 2, 3, 4, 5.\n",
      "\n",
      "Whole response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1, 2, 3, 4, 5.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWRXne5cL6RHZDBgKYGqKKmB4KZS`\n",
       "- model: `gpt-4o-mini`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=14, prompt_tokens=11, total_tokens=25, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWRXne5cL6RHZDBgKYGqKKmB4KZS', created=1751380811, model='gpt-4o-mini', object='chat.completion', system_fingerprint='fp_62a23a81ef', choices=[Choices(finish_reason='stop', index=0, message=Message(content='1, 2, 3, 4, 5.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=14, prompt_tokens=11, total_tokens=25, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from time import sleep\n",
    "chat2 = Chat(m)\n",
    "stream_gen = chat2(\"Count to 5\", stream=True)\n",
    "print(\"Streaming:\")\n",
    "for chunk in stream_gen:\n",
    "    sleep(0.1)  # for effect\n",
    "    if isinstance(chunk,litellm.ModelResponseStream): \n",
    "        if c:=chunk.choices[0].delta.content: print(c,end='')\n",
    "    else: \n",
    "        print(\"\\n\\nWhole response:\")\n",
    "        display(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now lets test tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _lite_call_func(tc,ns,raise_on_err=True):\n",
    "    res = call_func(tc.function.name, json.loads(tc.function.arguments),ns=ns)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": str(res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_add(a: int, b: int=0) -> int:\n",
    "    \"Add two numbers together\"\n",
    "    print(f\"TOOL CALLED {a=} + {b=}\")\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gemini/gemini-2.5-flash-preview-04-17 ===\n",
      "TOOL CALLED a=5 + b=3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "5 + 3 is 8.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `pPNjaKOqF52MvdIPqdrw-Qw`\n",
       "- model: `gemini-2.5-flash-preview-04-17`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=8, prompt_tokens=95, total_tokens=103, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=95, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='pPNjaKOqF52MvdIPqdrw-Qw', created=1751380897, model='gemini-2.5-flash-preview-04-17', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 is 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=8, prompt_tokens=95, total_tokens=103, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=95, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== claude-sonnet-4-20250514 ===\n",
      "TOOL CALLED a=5 + b=3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "5 + 3 = 8\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-dcb22cea-1f66-4efd-b571-2bf24d0af443`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=12, prompt_tokens=492, total_tokens=504, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-dcb22cea-1f66-4efd-b571-2bf24d0af443', created=1751380905, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 = 8', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=12, prompt_tokens=492, total_tokens=504, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== openai/gpt-4o-mini ===\n",
      "TOOL CALLED a=5 + b=3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "5 + 3 equals 8.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWT4qZTU63gc6XM6Buriho1A1uxL`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=8, prompt_tokens=81, total_tokens=89, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWT4qZTU63gc6XM6Buriho1A1uxL', created=1751380906, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 equals 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=8, prompt_tokens=81, total_tokens=89, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the tool with our Chat class\n",
    "for m in ms:\n",
    "    print(f'=== {m} ===')\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's 5 + 3?\")\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gemini/gemini-2.5-flash-preview-04-17 ===\n",
      "ModelResponseStream(id='yvNjaOL5K4iBxs0PysmniAE', created=1751380939, model='gemini-2.5-flash-preview-04-17', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id='call_3a0a0e58-7343-4c74-8540-7d3a516e3b1c', function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None)\n",
      "ModelResponseStream(id='yvNjaOL5K4iBxs0PysmniAE', created=1751380939, model='gemini-2.5-flash-preview-04-17', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None)\n",
      "ModelResponse(id='yvNjaOL5K4iBxs0PysmniAE', created=1751380939, model='gemini-2.5-flash-preview-04-17', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), id='call_3a0a0e58-7343-4c74-8540-7d3a516e3b1c', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=116, prompt_tokens=61, total_tokens=177, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))\n",
      "TOOL CALLED a=5 + b=3\n",
      "ModelResponseStream(id='y_NjaJCqLvOivdIP79ai4QI', created=1751380939, model='gemini-2.5-flash-preview-04-17', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='5', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None, citations=None)\n",
      "ModelResponseStream(id='y_NjaJCqLvOivdIP79ai4QI', created=1751380939, model='gemini-2.5-flash-preview-04-17', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' + 3 is 8.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None, citations=None)\n",
      "ModelResponseStream(id='y_NjaJCqLvOivdIP79ai4QI', created=1751380939, model='gemini-2.5-flash-preview-04-17', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None)\n",
      "ModelResponse(id='y_NjaJCqLvOivdIP79ai4QI', created=1751380939, model='gemini-2.5-flash-preview-04-17', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 is 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=8, prompt_tokens=95, total_tokens=103, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))\n",
      "=== claude-sonnet-4-20250514 ===\n",
      "ModelResponseStream(id='chatcmpl-7c34dea8-6ac6-4e40-943f-fd4f96a5d5d2', created=1751380941, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='I', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-7c34dea8-6ac6-4e40-943f-fd4f96a5d5d2', created=1751380941, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=\"'ll help you add 5 +\", role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-7c34dea8-6ac6-4e40-943f-fd4f96a5d5d2', created=1751380941, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' 3 using the addition function.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-7c34dea8-6ac6-4e40-943f-fd4f96a5d5d2', created=1751380941, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='', role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id='toolu_011kNB2sYatjznJmdUP52dLn', function=Function(arguments='', name='simple_add'), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None)\n",
      "ModelResponseStream(id='chatcmpl-7c34dea8-6ac6-4e40-943f-fd4f96a5d5d2', created=1751380941, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='', role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None)\n",
      "ModelResponseStream(id='chatcmpl-7c34dea8-6ac6-4e40-943f-fd4f96a5d5d2', created=1751380941, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='', role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='{\"a\":', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None)\n",
      "ModelResponseStream(id='chatcmpl-7c34dea8-6ac6-4e40-943f-fd4f96a5d5d2', created=1751380941, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='', role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments=' 5', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None)\n",
      "ModelResponseStream(id='chatcmpl-7c34dea8-6ac6-4e40-943f-fd4f96a5d5d2', created=1751380941, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='', role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments=', \"b\": 3}', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None)\n",
      "ModelResponseStream(id='chatcmpl-7c34dea8-6ac6-4e40-943f-fd4f96a5d5d2', created=1751380941, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='tool_calls', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None)\n",
      "ModelResponse(id='chatcmpl-7c34dea8-6ac6-4e40-943f-fd4f96a5d5d2', created=1751380941, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll help you add 5 + 3 using the addition function.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), id='toolu_011kNB2sYatjznJmdUP52dLn', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=88, prompt_tokens=0, total_tokens=88, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))\n",
      "TOOL CALLED a=5 + b=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponseStream(id='chatcmpl-563fdc41-187b-438c-b9f4-c2356ba7f16c', created=1751380945, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='5', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-563fdc41-187b-438c-b9f4-c2356ba7f16c', created=1751380945, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' + 3', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-563fdc41-187b-438c-b9f4-c2356ba7f16c', created=1751380945, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' = 8', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-563fdc41-187b-438c-b9f4-c2356ba7f16c', created=1751380945, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options=None)\n",
      "ModelResponse(id='chatcmpl-563fdc41-187b-438c-b9f4-c2356ba7f16c', created=1751380945, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 = 8', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=12, prompt_tokens=0, total_tokens=12, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))\n",
      "=== openai/gpt-4o-mini ===\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id='call_JOEFRT9xm9KEoGUOcfPSVvih', function=Function(arguments='', name='simple_add'), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='{\"', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='a', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='\":', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='5', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments=',\"', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='b', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='\":', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='3', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='}', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason='tool_calls', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True}, usage=Usage(completion_tokens=18, prompt_tokens=53, total_tokens=71, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))\n",
      "ModelResponse(id='chatcmpl-BoWTjkoGnJf9dY01OCWZ9JZQccMDp', created=1751380948, model='gpt-4o-mini', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":5,\"b\":3}', name='simple_add'), id='call_JOEFRT9xm9KEoGUOcfPSVvih', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=18, prompt_tokens=53, total_tokens=71, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))\n",
      "TOOL CALLED a=5 + b=3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponseStream(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content='5', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True}, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=' +', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True}, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=' ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True}, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content='3', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True}, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=' equals', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True}, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=' ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True}, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content='8', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True}, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True}, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True})\n",
      "ModelResponseStream(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint='fp_34a54ae93c', choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, stream_options={'include_usage': True}, usage=Usage(completion_tokens=8, prompt_tokens=81, total_tokens=89, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))\n",
      "ModelResponse(id='chatcmpl-BoWTkBkvpEk0xP69v6TCQNFmE12mN', created=1751380948, model='gpt-4o-mini', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 equals 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=8, prompt_tokens=81, total_tokens=89, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))\n"
     ]
    }
   ],
   "source": [
    "# Test the tool with our Chat class\n",
    "# TODO: prettify printing of toolcalls?\n",
    "for m in ms:\n",
    "    print(f'=== {m} ===')\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's 5 + 3?\",stream=True)\n",
    "    for o in res: print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test multi tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== claude-sonnet-4-20250514 ===\n",
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=13 + b=9\n",
      "ModelResponse(id='chatcmpl-20ce68c7-010c-4579-bb09-e12a728ce570', created=1751374936, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The results are:\\n- 5 + 3 = 8\\n- 13 + 9 = 22', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=31, prompt_tokens=612, total_tokens=643, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n",
      "=== openai/gpt-4o-mini ===\n",
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=13 + b=9\n",
      "ModelResponse(id='chatcmpl-BoUuozQcAEhp4a0WG0TFRO3pfJef6', created=1751374938, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of 5 + 3 is 8, and the result of 13 + 9 is 22.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=26, prompt_tokens=135, total_tokens=161, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n"
     ]
    }
   ],
   "source": [
    "# Test parallel tool calling\n",
    "for m in ms:\n",
    "    print(f'=== {m} ===')e\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's 5 + 3? And what is 13 + 9. Use tool tools for each\", stream=False)\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== claude-sonnet-4-20250514 ===\n",
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=8 + b=7\n",
      "TOOL CALLED a=15 + b=11\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'll solve this step by step using the addition function.\n",
       "\n",
       "First, let me calculate 5 + 3:\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-6a229bb0-c27c-47ca-be83-132753beca64`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=96, prompt_tokens=399, total_tokens=495, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-6a229bb0-c27c-47ca-be83-132753beca64', created=1751379030, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll solve this step by step using the addition function.\\n\\nFirst, let me calculate 5 + 3:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), id='toolu_01JonBfC5D18hgie7mh3heD3', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=96, prompt_tokens=399, total_tokens=495, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Now I'll add 7 to that result (8 + 7):\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-b9ca4908-1f3c-41f7-938e-7a3283490be0`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=88, prompt_tokens=508, total_tokens=596, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-b9ca4908-1f3c-41f7-938e-7a3283490be0', created=1751379034, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"Now I'll add 7 to that result (8 + 7):\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 8, \"b\": 7}', name='simple_add'), id='toolu_01AEmxYHWvHzxZ95YZzAsBqb', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=88, prompt_tokens=508, total_tokens=596, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Finally, I'll add 11 to that result (15 + 11):\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-5673c14d-9776-477c-a0f6-bb25714a6780`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=89, prompt_tokens=609, total_tokens=698, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-5673c14d-9776-477c-a0f6-bb25714a6780', created=1751379038, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"Finally, I'll add 11 to that result (15 + 11):\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 15, \"b\": 11}', name='simple_add'), id='toolu_01KdZPMM5AxuvCBCwC1DZ5GJ', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=89, prompt_tokens=609, total_tokens=698, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "So working step by step:\n",
       "- 5 + 3 = 8\n",
       "- 8 + 7 = 15  \n",
       "- 15 + 11 = 26\n",
       "\n",
       "Therefore, ((5 + 3) + 7) + 11 = **26**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-aee46720-01b7-440d-8245-96e13ce02785`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=68, prompt_tokens=711, total_tokens=779, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-aee46720-01b7-440d-8245-96e13ce02785', created=1751379041, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='So working step by step:\\n- 5 + 3 = 8\\n- 8 + 7 = 15  \\n- 15 + 11 = 26\\n\\nTherefore, ((5 + 3) + 7) + 11 = **26**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=68, prompt_tokens=711, total_tokens=779, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== openai/gpt-4o-mini ===\n",
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=7 + b=11\n",
      "TOOL CALLED a=8 + b=18\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ simple_add({\"a\": 5, \"b\": 3})\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 7, \"b\": 11})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoVyzdn8Embb2JFdlGARlyJAjNTvT`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=52, prompt_tokens=61, total_tokens=113, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoVyzdn8Embb2JFdlGARlyJAjNTvT', created=1751379041, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), id='call_UBF25IeSzEpS3DzvkpOZLy3y', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 7, \"b\": 11}', name='simple_add'), id='call_SbXFcphPUjHbr9CO2du4xXzH', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=52, prompt_tokens=61, total_tokens=113, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ simple_add({\"a\":8,\"b\":18})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoVz0xc4DrvUQVeXj18GSlqdD7LiP`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=18, prompt_tokens=129, total_tokens=147, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoVz0xc4DrvUQVeXj18GSlqdD7LiP', created=1751379042, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":8,\"b\":18}', name='simple_add'), id='call_2CRDBfSJY1H4nGNAgUEiFzca', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=18, prompt_tokens=129, total_tokens=147, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Let's break it down step by step:\n",
       "\n",
       "1. First, we calculate \\(5 + 3\\):\n",
       "   \\[\n",
       "   5 + 3 = 8\n",
       "   \\]\n",
       "\n",
       "2. Next, we add \\(7 + 11\\):\n",
       "   \\[\n",
       "   7 + 11 = 18\n",
       "   \\]\n",
       "\n",
       "3. Finally, we add the results from the previous steps:\n",
       "   \\[\n",
       "   8 + 18 = 26\n",
       "   \\]\n",
       "\n",
       "So, \\(((5 + 3) + 7) + 11 = 26\\).\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoVz174p7RakMdVuyAu2cG93IbOO3`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=117, prompt_tokens=156, total_tokens=273, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoVz174p7RakMdVuyAu2cG93IbOO3', created=1751379043, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Let's break it down step by step:\\n\\n1. First, we calculate \\\\(5 + 3\\\\):\\n   \\\\[\\n   5 + 3 = 8\\n   \\\\]\\n\\n2. Next, we add \\\\(7 + 11\\\\):\\n   \\\\[\\n   7 + 11 = 18\\n   \\\\]\\n\\n3. Finally, we add the results from the previous steps:\\n   \\\\[\\n   8 + 18 = 26\\n   \\\\]\\n\\nSo, \\\\(((5 + 3) + 7) + 11 = 26\\\\).\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=117, prompt_tokens=156, total_tokens=273, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test (multi-)toolloop\n",
    "for m in ms:\n",
    "    print(f'=== {m} ===')\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's ((5 + 3)+7)+11? Work step by step\", stream=False, return_all=True,max_tool_rounds=5)\n",
    "    for r in res: display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemini/gemini-2.5-flash-preview-04-17',\n",
       " 'claude-sonnet-4-20250514',\n",
       " 'openai/gpt-4o-mini']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models support parallel tool calling. I.e. sending multiple tool call requests in one conversation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=7 + b=2\n",
      "MULTIPLY: 8 * 9\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ simple_add({\"a\": 5, \"b\": 3})\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 7, \"b\": 2})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWUuJ6x9FVpiW0haODAVGEdvzZbO`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=52, prompt_tokens=81, total_tokens=133, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWUuJ6x9FVpiW0haODAVGEdvzZbO', created=1751381020, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), id='call_VFpgsyCN1hkE10Yu4PcS6H3G', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 7, \"b\": 2}', name='simple_add'), id='call_XZ1q14D7ue7AqsxLx4nTzwFw', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=52, prompt_tokens=81, total_tokens=133, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ multiply({\"a\":8,\"b\":9})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWUvqPaatqHkwOAjrGH7CekpzwAj`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=149, total_tokens=166, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWUvqPaatqHkwOAjrGH7CekpzwAj', created=1751381021, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":8,\"b\":9}', name='multiply'), id='call_drVDKN04ocKZEjlsdm52zv7D', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=17, prompt_tokens=149, total_tokens=166, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The result of the calculation \\((5 + 3) * (7 + 2)\\) is \\(72\\).\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWUwkuaNwiKe9sP3Gd3o6CEoyNeZ`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=26, prompt_tokens=174, total_tokens=200, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWUwkuaNwiKe9sP3Gd3o6CEoyNeZ', created=1751381022, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of the calculation \\\\((5 + 3) * (7 + 2)\\\\) is \\\\(72\\\\).', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=26, prompt_tokens=174, total_tokens=200, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"Multiply two numbers\"\n",
    "    print(f\"MULTIPLY: {a} * {b}\")\n",
    "    return a * b\n",
    "\n",
    "chat = Chat('openai/gpt-4o-mini', tools=[simple_add, multiply])\n",
    "res = chat(\"Calculate (5 + 3) * (7 + 2)\", max_tool_rounds=5, return_all=True)\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See it did the additions in one go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toolloop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh wait above we also demonstrated a toolloop! With litellm we might actually be able to put toolloop straight into the main `__call__` of Chat.\n",
    "\n",
    "We have the new `return_all=False` parameter. It's only relevant when you're not streaming. Because if you're streaming we always send back everything. But if you're not, then `return_all` determines if we only send back the last llm response or all of them. \n",
    "\n",
    "If you set max_tool_rounds to > 1 and return_all=True then you basically have a toolloop I think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets show toolloop hitting a max rounds limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=10 + b=5\n",
      "TOOL CALLED a=2 + b=1\n",
      "MULTIPLY: 15 * 3\n",
      "Got 3 responses\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ simple_add({\"a\": 10, \"b\": 5})\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 2, \"b\": 1})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWXPSlEvAVVfSCFzroiYQnv6KLbq`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=52, prompt_tokens=109, total_tokens=161, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWXPSlEvAVVfSCFzroiYQnv6KLbq', created=1751381175, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='call_Tv6QRnn77WBz5qLLTn69JAqK', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='call_iflH6y6LvqA4oQmKG4qkXrMO', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=52, prompt_tokens=109, total_tokens=161, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ multiply({\"a\":15,\"b\":3})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWXQIpSeKGEEQoEElI7aswVgLpKt`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=177, total_tokens=194, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWXQIpSeKGEEQoEElI7aswVgLpKt', created=1751381176, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":15,\"b\":3}', name='multiply'), id='call_roajsEsiNLMsdjPb7Yu630nV', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=17, prompt_tokens=177, total_tokens=194, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "So far, I have calculated the following steps:\n",
       "\n",
       "1. **Addition**: \\(10 + 5 = 15\\)\n",
       "2. **Addition**: \\(2 + 1 = 3\\)\n",
       "3. **Multiplication**: \\(15 \\times 3 = 45\\)\n",
       "\n",
       "Next, we need to divide \\(45\\) by \\(3\\) to complete the calculation.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWXRpJdfBGrYZup0Rp8h6AbxFMvb`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=80, prompt_tokens=214, total_tokens=294, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWXRpJdfBGrYZup0Rp8h6AbxFMvb', created=1751381177, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='So far, I have calculated the following steps:\\n\\n1. **Addition**: \\\\(10 + 5 = 15\\\\)\\n2. **Addition**: \\\\(2 + 1 = 3\\\\)\\n3. **Multiplication**: \\\\(15 \\\\times 3 = 45\\\\)\\n\\nNext, we need to divide \\\\(45\\\\) by \\\\(3\\\\) to complete the calculation.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=80, prompt_tokens=214, total_tokens=294, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test 2: Hit max_tool_rounds limit with final_prompt\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"Divide two numbers\"\n",
    "    print(f\"DIVIDE: {a} / {b}\")\n",
    "    return a / b\n",
    "\n",
    "chat = Chat(m, tools=[simple_add, multiply, divide])\n",
    "res = chat(\"Calculate ((10 + 5) * 3) / (2 + 1) step by step\", \n",
    "           max_tool_rounds=2, \n",
    "           final_prompt=\"Please summarize what you've calculated so far\",\n",
    "           return_all=True)\n",
    "print(f\"Got {len(res)} responses\")\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOPPING: Found error in tool result\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ error_tool({\"x\":15})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWY1u1xejxD7KwDXJeqHe4NZx5Z8`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=14, prompt_tokens=49, total_tokens=63, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWY1u1xejxD7KwDXJeqHe4NZx5Z8', created=1751381213, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"x\":15}', name='error_tool'), id='call_6frTgHD8jZPKAvZn1IDmR9wk', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=14, prompt_tokens=49, total_tokens=63, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test 3: Custom cont_func to stop early\n",
    "def stop_on_error(user_msg, llm_resp, tool_results):\n",
    "    \"Stop if any tool result contains 'error'\"\n",
    "    for result in tool_results:\n",
    "        if 'error' in str(result['content']).lower():\n",
    "            print(\"STOPPING: Found error in tool result\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def error_tool(x: int) -> str:\n",
    "    \"A tool that sometimes errors\"\n",
    "    if x > 10: return \"Error: number too big!\"\n",
    "    return f\"Success: {x}\"\n",
    "\n",
    "chat = Chat(m, tools=[error_tool])\n",
    "res = chat(\"Try error_tool with 15\", \n",
    "           max_tool_rounds=3,\n",
    "           cont_func=stop_on_error,\n",
    "           return_all=True)\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also show streaming with toolloops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming responses:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ simple_add({\"a\": 4, \"b\": 6})\n",
       "\n",
       "ðŸ”§ multiply({\"a\": 10, \"b\": 2})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWZwFDuE2LwQIM5VhAZltJVOCmWe`\n",
       "- model: `gpt-4o-mini`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=51, prompt_tokens=77, total_tokens=128, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWZwFDuE2LwQIM5VhAZltJVOCmWe', created=1751381333, model='gpt-4o-mini', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 4, \"b\": 6}', name='simple_add'), id='call_KwqjAbq2AV7Wb4duUXXGkc5E', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 2}', name='multiply'), id='call_FErPkteZZe05NZZTsfncSGZC', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=51, prompt_tokens=77, total_tokens=128, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=4 + b=6\n",
      "MULTIPLY: 10 * 2\n",
      "The result of the calculation \\((4 + 6) * 2\\) is \\(20\\)."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The result of the calculation \\((4 + 6) * 2\\) is \\(20\\).\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWZxGlsGaPinEeru0i9py6dfjZiq`\n",
       "- model: `gpt-4o-mini`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=23, prompt_tokens=144, total_tokens=167, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWZxGlsGaPinEeru0i9py6dfjZiq', created=1751381333, model='gpt-4o-mini', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of the calculation \\\\((4 + 6) * 2\\\\) is \\\\(20\\\\).', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=23, prompt_tokens=144, total_tokens=167, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test 4: Streaming with tool loops\n",
    "chat = Chat(m, tools=[simple_add, multiply])\n",
    "stream_gen = chat(\"Calculate (4 + 6) * 2\", max_tool_rounds=3, stream=True)\n",
    "\n",
    "print(\"Streaming responses:\")\n",
    "for chunk in stream_gen:\n",
    "    if isinstance(chunk, litellm.ModelResponseStream): \n",
    "        if c:= chunk.choices[0].delta.content: print(c,end='')\n",
    "    else: display(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
