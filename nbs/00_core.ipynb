{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# core\n",
    "\n",
    "> lisette core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import litellm, json\n",
    "from litellm import completion, stream_chunk_builder\n",
    "from litellm.utils import function_to_dict\n",
    "from toolslm.funccall import mk_ns, call_func\n",
    "from typing import Optional\n",
    "from fastcore.all import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LiteLLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Litellm provides an easy wrapper for most big LLM providers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = [\"gemini/gemini-2.5-flash-preview-04-17\", \"claude-sonnet-4-20250514\", \"openai/gpt-4o-mini\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gemini/gemini-2.5-flash-preview-04-17 ===\n",
      "ModelResponse(id='GAN0aOviEoaO6dkPotzFkAs', created=1752433429, model='gemini-2.5-flash-preview-04-17', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello there!\\n\\nHow can I help you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=635, prompt_tokens=4, total_tokens=639, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=624, rejected_prediction_tokens=None, text_tokens=11), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])\n",
      "=== claude-sonnet-4-20250514 ===\n",
      "ModelResponse(id='chatcmpl-f30384f9-9ab6-4167-8d3d-2d25585ca1e9', created=1752433434, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How are you doing today? What can I help you with?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=18, prompt_tokens=10, total_tokens=28, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n",
      "=== openai/gpt-4o-mini ===\n",
      "ModelResponse(id='chatcmpl-BswHLZ2Auq1HiaM0eTO5c82ZjjAKw', created=1752433435, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_62a23a81ef', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I assist you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=9, prompt_tokens=10, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n"
     ]
    }
   ],
   "source": [
    "for m in ms:\n",
    "    print(f'=== {m} ===')\n",
    "    res = completion(m,[{'role':'user','content':'Hey there!'}])\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets add a wrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def _repr_markdown_(self: litellm.ModelResponse):\n",
    "    # Extract content from the response\n",
    "    message = self.choices[0].message\n",
    "    if message.content:\n",
    "        content = message.content\n",
    "    elif message.tool_calls:\n",
    "        # Show tool calls in a nice format\n",
    "        tool_calls = [f\"ðŸ”§ {tc.function.name}({tc.function.arguments})\\n\" for tc in message.tool_calls]\n",
    "        content = \"\\n\".join(tool_calls)\n",
    "    else:\n",
    "        content = str(message)\n",
    "    \n",
    "    # Create details section\n",
    "    details = []\n",
    "    details.append(f\"id: `{self.id}`\")\n",
    "    details.append(f\"model: `{self.model}`\")\n",
    "    details.append(f\"finish_reason: `{self.choices[0].finish_reason}`\")\n",
    "    if hasattr(self, 'usage') and self.usage:\n",
    "        details.append(f\"usage: `{self.usage}`\")\n",
    "    \n",
    "    det_str = '\\n- '.join(details)\n",
    "    \n",
    "    return f\"\"\"{content}\n",
    "\n",
    "<details>\n",
    "\n",
    "- {det_str}\n",
    "\n",
    "</details>\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gemini/gemini-2.5-flash-preview-04-17 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I help you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `HAN0aLSbMb6yqtsPlu7GyQc`\n",
       "- model: `gemini-2.5-flash-preview-04-17`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=188, prompt_tokens=4, total_tokens=192, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=179, rejected_prediction_tokens=None, text_tokens=9), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='HAN0aLSbMb6yqtsPlu7GyQc', created=1752433435, model='gemini-2.5-flash-preview-04-17', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I help you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=188, prompt_tokens=4, total_tokens=192, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=179, rejected_prediction_tokens=None, text_tokens=9), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== claude-sonnet-4-20250514 ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello! Nice to meet you! How are you doing today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-42934c6d-30bb-48ea-8aec-8232a3ee21d4`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=16, prompt_tokens=10, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-42934c6d-30bb-48ea-8aec-8232a3ee21d4', created=1752433438, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! Nice to meet you! How are you doing today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=16, prompt_tokens=10, total_tokens=26, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== openai/gpt-4o-mini ===\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "Hello! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BswHOVJdOBsgJC0cNEqlAihI0ERLN`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=9, prompt_tokens=10, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BswHOVJdOBsgJC0cNEqlAihI0ERLN', created=1752433438, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hello! How can I assist you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=9, prompt_tokens=10, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "for m in ms:\n",
    "    print(f'=== {m} ===')\n",
    "    res = completion(m,[{'role':'user','content':'Hey there!'}])\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Litellm is pretty bare bones. It doesnt keep track of conversation history or anything.\n",
    "\n",
    "So lets make a claudette style wrapper so we can do streaming, toolcalling, and toolloops without problems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def stream_with_complete(gen):\n",
    "    \"Extend streaming response chunks with the complete response\"\n",
    "    chunks = []\n",
    "    for chunk in gen:\n",
    "        chunks.append(chunk)\n",
    "        yield chunk\n",
    "    return stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Chat:\n",
    "    def __init__(self, model: str, sp='', temp=0, tools: list = None, \n",
    "                 hist: list = None, ns: Optional[dict] = None):\n",
    "        \"LiteLLM chat client.\"\n",
    "        self.model = model\n",
    "        if hist is None: hist = []\n",
    "        if tools is None: tools = []\n",
    "        \n",
    "        # Set up namespace following claudette pattern\n",
    "        if ns is None and tools: ns = mk_ns(tools)\n",
    "        elif ns is None: ns = globals()\n",
    "        \n",
    "        # Cache tool schemas\n",
    "        self.tool_schemas = []\n",
    "        for t in tools:\n",
    "            if isinstance(t, dict): self.tool_schemas.append(t)\n",
    "            elif t: self.tool_schemas += [{'type':'function', 'function':function_to_dict(t)}]\n",
    "        self.h, self.sp, self.temp, self.tools, self.ns = hist, sp, temp, tools, ns\n",
    "    \n",
    "    def add_tool(self, func):\n",
    "        \"\"\"Add a tool function to the chat client\"\"\"\n",
    "        self.tools.append(func)\n",
    "        self.tool_schemas.append({'type': 'function', 'function': function_to_dict(func)})\n",
    "        self.ns[func.__name__] = func\n",
    "\n",
    "    def _prepare_messages(self, msg=None):\n",
    "        \"Prepare the messages list for the API call\"\n",
    "        messages = [{\"role\": \"system\", \"content\": self.sp}] if self.sp else []\n",
    "        \n",
    "        if isinstance(msg, str): self.h.append({\"role\": \"user\", \"content\": msg})\n",
    "        elif isinstance(msg, dict): self.h.append(msg)\n",
    "        elif isinstance(msg, list): self.h.extend(msg)\n",
    "        elif msg is None: pass\n",
    "        else: raise ValueError(f\"Can't parse {msg=}\")\n",
    "            \n",
    "        for m in self.h: messages.append(m if isinstance(m, dict) else m.model_dump())\n",
    "        return messages\n",
    "    \n",
    "    def _call(self, msg=None, stream=False, max_tool_rounds=1, tool_round=0, \n",
    "              cont_func=noop, final_prompt=None, **kwargs):\n",
    "        \"Internal call method that always yields responses\"\n",
    "        messages = self._prepare_messages(msg)\n",
    "        \n",
    "        # Make the API call\n",
    "        res = litellm.completion(model=self.model, messages=messages, stream=stream, \n",
    "                               tools=self.tool_schemas, temperature=self.temp, **kwargs)\n",
    "        \n",
    "        if stream: res = yield from stream_with_complete(res)        \n",
    "\n",
    "        m = res.choices[0].message\n",
    "        self.h.append(m)\n",
    "        yield res\n",
    "\n",
    "        \n",
    "        if tcs := m.tool_calls:\n",
    "            tool_results = [_lite_call_func(tc, ns=self.ns) for tc in tcs]\n",
    "            \n",
    "            # Check continuation function: user_msg, llm_response, tool_results\n",
    "            user_msg = self.h[-2] if len(self.h) >= 2 else None\n",
    "            if not cont_func(user_msg, m, tool_results):\n",
    "                # Send final prompt when cont_func stops the loop\n",
    "                if final_prompt:\n",
    "                    final_msg = tool_results + [{\"role\": \"user\", \"content\": final_prompt}]\n",
    "                    yield from self._call(final_msg, stream, max_tool_rounds, tool_round+1, cont_func, final_prompt, tool_choice='none', **kwargs)\n",
    "                return\n",
    "                \n",
    "            # Continue with more rounds or final round\n",
    "            if tool_round < max_tool_rounds - 1:\n",
    "                yield from self._call(tool_results, stream, max_tool_rounds, tool_round+1, cont_func, final_prompt, **kwargs)\n",
    "            else:\n",
    "                # Final round - inject final_prompt if provided and set tool_choice=None\n",
    "                final_msg = tool_results + ([{\"role\": \"user\", \"content\": final_prompt}] if final_prompt else [])\n",
    "                yield from self._call(final_msg, stream, max_tool_rounds, tool_round+1, cont_func, final_prompt, tool_choice='none', **kwargs)\n",
    "    \n",
    "    def __call__(self, msg=None, stream=False, max_tool_rounds=1, cont_func=noop, final_prompt=None, return_all=False, **kwargs):\n",
    "        \"Main call method - handles streaming vs non-streaming\"\n",
    "        result_gen = self._call(msg, stream, max_tool_rounds, 0, cont_func, final_prompt, **kwargs)        \n",
    "        if stream: return result_gen              # streaming\n",
    "        elif return_all: return list(result_gen)  # toolloop behavior\n",
    "        else: return last(result_gen)             # normal chat behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test history tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Hi Rens! How can I assist you today?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BswHQagCUNxJsSg2y85PjRnZSmCJo`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=12, prompt_tokens=29, total_tokens=41, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BswHQagCUNxJsSg2y85PjRnZSmCJo', created=1752433440, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Hi Rens! How can I assist you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=12, prompt_tokens=29, total_tokens=41, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat(m)\n",
    "res = chat(\"Hey my name is Rens\")\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Your name is Rens. How can I help you today, Rens?\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BswHQ2jMpSOdC4sQs4aqCYOrT96SR`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=51, total_tokens=68, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BswHQ2jMpSOdC4sQs4aqCYOrT96SR', created=1752433440, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Your name is Rens. How can I help you today, Rens?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=17, prompt_tokens=51, total_tokens=68, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat(\"Whats my name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See now we keep track of history!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming:\n",
      "1, 2, 3, 4, 5.\n",
      "\n",
      "Whole response:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "1, 2, 3, 4, 5.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BswHRnmweUWT462FmAuKfQrJzwTiR`\n",
       "- model: `gpt-4o-mini`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=15, prompt_tokens=27, total_tokens=42, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BswHRnmweUWT462FmAuKfQrJzwTiR', created=1752433441, model='gpt-4o-mini', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='1, 2, 3, 4, 5.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=15, prompt_tokens=27, total_tokens=42, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from time import sleep\n",
    "chat2 = Chat(m)\n",
    "stream_gen = chat2(\"Count to 5\", stream=True)\n",
    "print(\"Streaming:\")\n",
    "for chunk in stream_gen:\n",
    "    sleep(0.1)  # for effect\n",
    "    if isinstance(chunk,litellm.ModelResponseStream): \n",
    "        if c:=chunk.choices[0].delta.content: print(c,end='')\n",
    "    else: \n",
    "        print(\"\\n\\nWhole response:\")\n",
    "        display(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test tool use"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now lets test tool use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _lite_call_func(tc,ns,raise_on_err=True):\n",
    "    res = call_func(tc.function.name, json.loads(tc.function.arguments),ns=ns)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": str(res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_add(a: int, b: int=0) -> int:\n",
    "    \"Add two numbers together\"\n",
    "    print(f\"TOOL CALLED {a=} + {b=}\")\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gemini/gemini-2.5-flash-preview-04-17 ===\n",
      "TOOL CALLED a=5 + b=3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "5 + 3 is 8.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `dwN0aI_ZHuKuqtsPvPvmqQc`\n",
       "- model: `gemini-2.5-flash-preview-04-17`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=8, prompt_tokens=95, total_tokens=103, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=95, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='dwN0aI_ZHuKuqtsPvPvmqQc', created=1752433526, model='gemini-2.5-flash-preview-04-17', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 is 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=8, prompt_tokens=95, total_tokens=103, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=95, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== claude-sonnet-4-20250514 ===\n",
      "TOOL CALLED a=5 + b=3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "5 + 3 = 8\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-ab409668-9d29-4270-aa1d-25daf970f140`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=12, prompt_tokens=492, total_tokens=504, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-ab409668-9d29-4270-aa1d-25daf970f140', created=1752433531, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 = 8', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=12, prompt_tokens=492, total_tokens=504, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== openai/gpt-4o-mini ===\n",
      "TOOL CALLED a=5 + b=3\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "5 + 3 equals 8.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BswIuD1NkWqq9uVrOLVEBN4G32a0v`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=8, prompt_tokens=81, total_tokens=89, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BswIuD1NkWqq9uVrOLVEBN4G32a0v', created=1752433532, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 equals 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=8, prompt_tokens=81, total_tokens=89, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the tool with our Chat class\n",
    "for m in ms:\n",
    "    print(f'=== {m} ===')\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's 5 + 3?\")\n",
    "    display(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== gemini/gemini-2.5-flash-preview-04-17 ===\n",
      "ModelResponseStream(id='0wR0aJ3jMrfkqtsP242MmQw', created=1752433876, model='gemini-2.5-flash-preview-04-17', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id='call_990eb790-a64a-4023-8efa-0d862810354f', function=Function(arguments='{\"b\": 3, \"a\": 5}', name='simple_add'), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='0wR0aJ3jMrfkqtsP242MmQw', created=1752433876, model='gemini-2.5-flash-preview-04-17', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='tool_calls', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponse(id='0wR0aJ3jMrfkqtsP242MmQw', created=1752433876, model='gemini-2.5-flash-preview-04-17', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"b\": 3, \"a\": 5}', name='simple_add'), id='call_990eb790-a64a-4023-8efa-0d862810354f', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=116, prompt_tokens=61, total_tokens=177, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))\n",
      "TOOL CALLED a=5 + b=3\n",
      "ModelResponseStream(id='1AR0aMTbKsGDqtsP28KhkAg', created=1752433876, model='gemini-2.5-flash-preview-04-17', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='5', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='1AR0aMTbKsGDqtsP28KhkAg', created=1752433876, model='gemini-2.5-flash-preview-04-17', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' + 3 is 8.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='1AR0aMTbKsGDqtsP28KhkAg', created=1752433876, model='gemini-2.5-flash-preview-04-17', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponse(id='1AR0aMTbKsGDqtsP28KhkAg', created=1752433876, model='gemini-2.5-flash-preview-04-17', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 is 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=8, prompt_tokens=95, total_tokens=103, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))\n",
      "=== claude-sonnet-4-20250514 ===\n",
      "ModelResponseStream(id='chatcmpl-d1781f8b-7ac2-47f0-bdd9-94a8d6eeab94', created=1752433878, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=\"I'll help\", role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-d1781f8b-7ac2-47f0-bdd9-94a8d6eeab94', created=1752433878, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' you add 5 + 3 using the addition', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-d1781f8b-7ac2-47f0-bdd9-94a8d6eeab94', created=1752433878, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' function.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-d1781f8b-7ac2-47f0-bdd9-94a8d6eeab94', created=1752433878, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='', role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id='toolu_01CR7UtRxHd7Neqgff4fs6ui', function=Function(arguments='', name='simple_add'), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-d1781f8b-7ac2-47f0-bdd9-94a8d6eeab94', created=1752433878, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='', role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-d1781f8b-7ac2-47f0-bdd9-94a8d6eeab94', created=1752433878, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='', role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='{\"a\"', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-d1781f8b-7ac2-47f0-bdd9-94a8d6eeab94', created=1752433878, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='', role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments=': 5', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-d1781f8b-7ac2-47f0-bdd9-94a8d6eeab94', created=1752433878, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='', role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments=', \"b\": 3}', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-d1781f8b-7ac2-47f0-bdd9-94a8d6eeab94', created=1752433878, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='tool_calls', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponse(id='chatcmpl-d1781f8b-7ac2-47f0-bdd9-94a8d6eeab94', created=1752433878, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll help you add 5 + 3 using the addition function.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), id='toolu_01CR7UtRxHd7Neqgff4fs6ui', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=88, prompt_tokens=0, total_tokens=88, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))\n",
      "TOOL CALLED a=5 + b=3\n",
      "ModelResponseStream(id='chatcmpl-4f56f73a-3b0f-4c00-b045-99b37517af21', created=1752433880, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content='5', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-4f56f73a-3b0f-4c00-b045-99b37517af21', created=1752433880, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=' + 3 = 8', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-4f56f73a-3b0f-4c00-b045-99b37517af21', created=1752433880, model='claude-sonnet-4-20250514', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponse(id='chatcmpl-4f56f73a-3b0f-4c00-b045-99b37517af21', created=1752433880, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 = 8', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=12, prompt_tokens=0, total_tokens=12, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))\n",
      "=== openai/gpt-4o-mini ===\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id='call_DaCGqSCEhdiYYxkGmz39VSST', function=Function(arguments='', name='simple_add'), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='{\"', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='a', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='\":', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='5', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments=',\"', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='b', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='\":', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='3', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=None, role='assistant', function_call=None, tool_calls=[ChatCompletionDeltaToolCall(id=None, function=Function(arguments='}', name=None), type='function', index=0)], audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='tool_calls', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=18, prompt_tokens=53, total_tokens=71, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))\n",
      "ModelResponse(id='chatcmpl-BswOXa3IFDwlrjAK7YTfnRAgWhypN', created=1752433881, model='gpt-4o-mini', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":5,\"b\":3}', name='simple_add'), id='call_DaCGqSCEhdiYYxkGmz39VSST', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=18, prompt_tokens=53, total_tokens=71, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))\n",
      "TOOL CALLED a=5 + b=3\n",
      "ModelResponseStream(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content='5', role='assistant', function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=' +', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=' ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content='3', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=' equals', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content=' ', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content='8', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, refusal=None, content='.', role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, citations=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason='stop', index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None)\n",
      "ModelResponseStream(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion.chunk', system_fingerprint=None, choices=[StreamingChoices(finish_reason=None, index=0, delta=Delta(provider_specific_fields=None, content=None, role=None, function_call=None, tool_calls=None, audio=None), logprobs=None)], provider_specific_fields=None, usage=Usage(completion_tokens=8, prompt_tokens=81, total_tokens=89, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))\n",
      "ModelResponse(id='chatcmpl-BswOX7Y9KGf3JwPw7zh1Awp1Tu8zV', created=1752433882, model='gpt-4o-mini', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='5 + 3 equals 8.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=8, prompt_tokens=81, total_tokens=89, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))\n"
     ]
    }
   ],
   "source": [
    "# Test the tool with our Chat class\n",
    "# TODO: prettify printing of toolcalls?\n",
    "for m in ms:\n",
    "    print(f'=== {m} ===')\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's 5 + 3?\",stream=True)\n",
    "    for o in res: print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Otters are fascinating aquatic mammals that belong to the subfamily Lutrinae within the weasel family (Mustelidae). Here's what you should know about these remarkable creatures:\n",
       "\n",
       "## Physical Characteristics\n",
       "\n",
       "Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water. The 13 species range in adult size from 0.6 to 1.8 m (2.0 to 5.9 ft) in length and 1 to 45 kg (2.2 to 99.2 lb) in weight. The Asian small-clawed otter is the smallest otter species and the giant otter and sea otter are the largest.\n",
       "\n",
       "Sea otters have particularly remarkable fur - Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch. All this dense fur makes up for the fact that, unlike other marine mammals, sea otters do not have blubber to keep them insulated in chilly ocean waters.\n",
       "\n",
       "## Species and Habitat\n",
       "\n",
       "The 13 extant otter species are all semiaquatic, aquatic, or marine. There are two main types you might encounter:\n",
       "\n",
       "- **River otters**: River otters live primarily in freshwater, though they do swim and hunt in seawater. A semi-aquatic mammal, like the river otter, goes back and forth between land and water. In fact, despite being incredible swimmers, river otters spend about two-thirds of their lives on land.\n",
       "\n",
       "- **Sea otters**: Sea otters live exclusively in the ocean along coastlines. A sea otter can live its whole life without leaving the ocean. They eat, sleep, play and give birth in the sea.\n",
       "\n",
       "Approximately 90 percent of the world's sea otters live in coastal Alaska. Many live in the waters surrounding public lands including Kodiak National Wildlife Refuge, Kenai Fjords National Park, and Glacier Bay National Park.\n",
       "\n",
       "## Behavior and Abilities\n",
       "\n",
       "Otters are incredibly playful animals. They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. In the winter, otters have found the easiest and perhaps most fun way to get around is by sliding. After a few bumps, they can slide up to 22 feet on the ice.\n",
       "\n",
       "Their swimming abilities are impressive:\n",
       "- An otter's lung capacity is 2.5 times greater than that of similar-sized land mammals. Sea otters have been known to stay submerged for more than 5 minutes at a time.\n",
       "- River otters, however, can hold their breath for up to 8 minutes.\n",
       "- As a sometimes aquatic creature, it shouldn't be a surprise that otters can swim up to seven miles per hour and dive down 60 feet.\n",
       "\n",
       "## Diet and Feeding\n",
       "\n",
       "Its usual source of food is fish, and further downriver, eels, but it may sample frogs and birds. Sea otters can eat 25 per cent of their body weight in food each day. Their diet is varied but you can commonly find them munching on sea urchins, crabs, mussels and clams.\n",
       "\n",
       "Sea otters are one of the only marine mammals that uses tools. All of their favorite foods need to be cracked open to eat, so these intelligent animals will use a rock to crack them open. While floating on their backs, sea otters not only nap, but also use rocks to help them open mussels or other shellfish. Otters place a rock on their chests and smash the shellfish against it until it breaks open to reveal the tasty meat inside.\n",
       "\n",
       "## Social Behavior and Reproduction\n",
       "\n",
       "Sea otters, particularly mothers and pups, sometimes hold hands while floating on their backs. Hand-holding keeps the otters from drifting away from each other and their food source while they sleep. When they sleep, sea otters wrap themselves in kelp to ensure they do not float away. They will also hold each other's hands to stay close while sleeping.\n",
       "\n",
       "The gestation period in otters is about 60 to 86 days. The newborn pup is cared for by the bitch, dog, and older offspring. A newborn pup needs constant attention and will stay with its mother for six months until it develops survival skills. Fun fact: An otter pup's fur is so dense that it can't dive underwater until it gets its adult fur.\n",
       "\n",
       "## Conservation Status\n",
       "\n",
       "Unfortunately, otters face significant conservation challenges. All otter species appear on the International Union for Conservation of Nature's Red List of Threatened Species, and only one is listed as \"least concern.\" Of the 13 species of otter, IUCN lists five as endangered, five as near-threatened, and two as vulnerable. Only the North American river otter is a species of least concern.\n",
       "\n",
       "Hunted to the edge of extinction by fur traders in the 18th and 19th centuries, the few remaining sea otters (about 2,000 scattered in remnant colonies throughout the North Pacific rim) were first protected by the International Fur Seal Treaty in 1911.\n",
       "\n",
       "Otters play a crucial ecological role as well. Sea otters are considered a keystone species. Keystone species are plants or animals that play an important role in how an entire ecosystem functions. Without them, there would be large scale, cascading impacts on the communities and other species within their ecosystem.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-15d60d57-7735-404d-912d-8ecbc52402e3`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=1811, prompt_tokens=13491, total_tokens=15302, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-15d60d57-7735-404d-912d-8ecbc52402e3', created=1752433761, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Otters are fascinating aquatic mammals that belong to the subfamily Lutrinae within the weasel family (Mustelidae). Here\\'s what you should know about these remarkable creatures:\\n\\n## Physical Characteristics\\n\\nOtters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water. The 13 species range in adult size from 0.6 to 1.8 m (2.0 to 5.9 ft) in length and 1 to 45 kg (2.2 to 99.2 lb) in weight. The Asian small-clawed otter is the smallest otter species and the giant otter and sea otter are the largest.\\n\\nSea otters have particularly remarkable fur - Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch. All this dense fur makes up for the fact that, unlike other marine mammals, sea otters do not have blubber to keep them insulated in chilly ocean waters.\\n\\n## Species and Habitat\\n\\nThe 13 extant otter species are all semiaquatic, aquatic, or marine. There are two main types you might encounter:\\n\\n- **River otters**: River otters live primarily in freshwater, though they do swim and hunt in seawater. A semi-aquatic mammal, like the river otter, goes back and forth between land and water. In fact, despite being incredible swimmers, river otters spend about two-thirds of their lives on land.\\n\\n- **Sea otters**: Sea otters live exclusively in the ocean along coastlines. A sea otter can live its whole life without leaving the ocean. They eat, sleep, play and give birth in the sea.\\n\\nApproximately 90 percent of the world\\'s sea otters live in coastal Alaska. Many live in the waters surrounding public lands including Kodiak National Wildlife Refuge, Kenai Fjords National Park, and Glacier Bay National Park.\\n\\n## Behavior and Abilities\\n\\nOtters are incredibly playful animals. They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. In the winter, otters have found the easiest and perhaps most fun way to get around is by sliding. After a few bumps, they can slide up to 22 feet on the ice.\\n\\nTheir swimming abilities are impressive:\\n- An otter\\'s lung capacity is 2.5 times greater than that of similar-sized land mammals. Sea otters have been known to stay submerged for more than 5 minutes at a time.\\n- River otters, however, can hold their breath for up to 8 minutes.\\n- As a sometimes aquatic creature, it shouldn\\'t be a surprise that otters can swim up to seven miles per hour and dive down 60 feet.\\n\\n## Diet and Feeding\\n\\nIts usual source of food is fish, and further downriver, eels, but it may sample frogs and birds. Sea otters can eat 25 per cent of their body weight in food each day. Their diet is varied but you can commonly find them munching on sea urchins, crabs, mussels and clams.\\n\\nSea otters are one of the only marine mammals that uses tools. All of their favorite foods need to be cracked open to eat, so these intelligent animals will use a rock to crack them open. While floating on their backs, sea otters not only nap, but also use rocks to help them open mussels or other shellfish. Otters place a rock on their chests and smash the shellfish against it until it breaks open to reveal the tasty meat inside.\\n\\n## Social Behavior and Reproduction\\n\\nSea otters, particularly mothers and pups, sometimes hold hands while floating on their backs. Hand-holding keeps the otters from drifting away from each other and their food source while they sleep. When they sleep, sea otters wrap themselves in kelp to ensure they do not float away. They will also hold each other\\'s hands to stay close while sleeping.\\n\\nThe gestation period in otters is about 60 to 86 days. The newborn pup is cared for by the bitch, dog, and older offspring. A newborn pup needs constant attention and will stay with its mother for six months until it develops survival skills. Fun fact: An otter pup\\'s fur is so dense that it can\\'t dive underwater until it gets its adult fur.\\n\\n## Conservation Status\\n\\nUnfortunately, otters face significant conservation challenges. All otter species appear on the International Union for Conservation of Nature\\'s Red List of Threatened Species, and only one is listed as \"least concern.\" Of the 13 species of otter, IUCN lists five as endangered, five as near-threatened, and two as vulnerable. Only the North American river otter is a species of least concern.\\n\\nHunted to the edge of extinction by fur traders in the 18th and 19th centuries, the few remaining sea otters (about 2,000 scattered in remnant colonies throughout the North Pacific rim) were first protected by the International Fur Seal Treaty in 1911.\\n\\nOtters play a crucial ecological role as well. Sea otters are considered a keystone species. Keystone species are plants or animals that play an important role in how an entire ecosystem functions. Without them, there would be large scale, cascading impacts on the communities and other species within their ecosystem.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': [[{'type': 'web_search_result_location', 'cited_text': 'Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in wate...', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDJQMVoTRyRNMKUpPvxoMHPcb3yihAmz/5HF3IjCOt5dVovUkeSu9lc7dCXo+i57R2ow2WVfc/znElu5a1WV/NyfZOxL8lJ9j69ith8AqE+yVKGsvdTf4hXvYxtHSnEP/ypEYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'The 13 species range in adult size from 0.6 to 1.8 m (2.0 to 5.9 ft) in length and 1 to 45 kg (2.2 to 99.2 lb) in weight. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDA8qR1Nlso+sWgWdgRoM7m682KmOBBtgh7QuIjADeytTDQ1mzZcDVwBmODDRPwvnjJ5afWKCuiPAhNZeVToBvQEVmyPb2up2xOj6FssqFGq8MKHsdYvwYzO4xLE6IBUYK6B+GAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'The Asian small-clawed otter is the smallest otter species and the giant otter and sea otter are the largest.', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDKzdeQtgbDzRu2qdPhoM37QnaYnKaxuIzIPPIjDeoHpr6xmYN69nG95k5b/L7jGb+VAqEUBuB5qTEcPmSwvv/Bljuq+4aa4lHkhlkjsqFPND1AfDwZ26mga3nezc3j1qxukFGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch. ', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDNFYk/kKDlmrxgzFyBoMuj6vIROVzB65jMzfIjBf2AjP/zqaLkIe3u8+o7tgMNTxJODg9vVMss7pz0HbbDF9BgWRAsutqYjw3wgABjoqE3yv56KOfqtzi2yNeRRSS2hDxB8YBA=='}, {'type': 'web_search_result_location', 'cited_text': 'Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch. ', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDH/AV7QTBHzt9wHLFxoMVK1hA8KiFfdyWcfJIjBc2cSri3tI0IJHbEv6A+9M1Em0rhuAYZFJTJs+EKMdj0tHyypvzoq8rNyOIdhUubkqFM+0lm6toe2v6+J5mnVAi72UUDBzGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'All this dense fur makes up for the fact that, unlike other marine mammals, sea otters do not have blubber to keep them insulated in chilly ocean wate...', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDEJsS7txPTR3d/VAZxoMLnqSYzALPWwgaY/PIjAeMIocoAJwwvRPH+w/p+hF1pA6VxyxmDU6rrHUZn1mSO8PF+2CfUTwt6cKb17xq7gqE7oAf3xLTQj8QVtVlTAkxnRvK64YBA=='}, {'type': 'web_search_result_location', 'cited_text': 'All this dense fur makes up for the fact that, unlike other marine mammals, sea otters do not have blubber to keep them insulated in chilly ocean wate...', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDHnYdS3E6NbLG8XZzBoMlq0HOdLe2ev8aVnwIjAoqUocRtmgDWb2Ps/l7NZxAOPGFPHaZr/eVjCHJEt1FYBVAegtTtxwYMQ+pynVPLUqFJ/hngjCfGfBiATgAUofUV5ug/eyGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'The 13 extant otter species are all semiaquatic, aquatic, or marine. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDO/sQ3WtgTzYG4GZoRoMB8ljyTRUiMclOmIBIjALdoRqXpNNBarbBIX8rUBIKFxRjhWYuGBNKUfIBaFOglmnrQXwHtOspxawCh6pockqEz/FrqfFD4zodKp/BBdty2xWLD4YBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'River otters live primarily in freshwater, though they do swim and hunt in seawater. ', 'url': 'https://www.treehugger.com/fascinating-facts-about-otters-4869357', 'title': '15 Fascinating Facts About Otters', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDELV7261w6D/o4VI2RoMF5+HHWo3Mp7QNVapIjCAM7vbTXH4Gl5m1NXJ7gHhwv2HhN+kTqTItripdoKWos89QZvzuYsfm3oDwjFkJ50qE4XiQV12cQDNStf5AsW+als6teYYBA=='}, {'type': 'web_search_result_location', 'cited_text': 'River otters live primarily in freshwater, though they do swim and hunt in seawater. ', 'url': 'https://www.treehugger.com/fascinating-facts-about-otters-4869357', 'title': '15 Fascinating Facts About Otters', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDIyiXnzioSvg7AMrSxoMOWsAGGmalsKfD2fVIjAhV34NidIPj6FhIYyRMA7LvKtZDb9O4qxfUbypvGfzbnndD8t2I+U580wPwi+67TgqEzn+QMVOduEljYGA1/FW5c2OLbYYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'A semi-aquatic mammal, like the river otter, goes back and forth between land and water. In fact, despite being incredible swimmers, river otters spen...', 'url': 'https://www.cbf.org/blogs/save-the-bay/2021/09/five-facts-you-otter-know.html', 'title': 'Five Facts You Otter Know - Chesapeake Bay Foundation', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDEnmFkzu8kTiT7wHpxoM/S5px+lE7R4+nVmsIjBryXP8x6Tx6TAzUVAqGvtw1DbXFVEBVIyCDa4dSv8D+jo85VcxRtFU2773aXulmAcqF1pNqmlNroxnmMheA7izDBvw3agy3uD/GAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters live exclusively in the ocean along coastlines. ', 'url': 'https://www.treehugger.com/fascinating-facts-about-otters-4869357', 'title': '15 Fascinating Facts About Otters', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDFnl9eSQidmhnieuNRoMKpXNPsj/fTZSmYVtIjB309ZP4GyDpTEfokAgQdFrNpFSNfUEKwNiRT5EI1+0mae+9XDxowMNHEVv3llm5QIqEzWW70NGW3ZeoLdlVrjdGMbKaKYYBA=='}, {'type': 'web_search_result_location', 'cited_text': 'Sea otters live exclusively in the ocean along coastlines. ', 'url': 'https://www.treehugger.com/fascinating-facts-about-otters-4869357', 'title': '15 Fascinating Facts About Otters', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDDjofKxnpeJGvflM2RoMdmOP7DCKRZuvYbVsIjACDhII9oABVkoQ32+oK32vbJfhHqUkckKiNI++AEnV3WpPopb+3iXTcDLLSK0DFKIqFAU4nwWl2mrf2dFQjfOGerhA1MLZGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'A sea otter can live its whole life without leaving the ocean. They eat, sleep, play and give birth in the sea.  ', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'EpEBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDO+RTlgHKfMoKycvMRoMPFlbSsCoL/mBGRBRIjB5jMewXiBsh6N/R9b7pHw7QspCHfTxNwLd46xb13bqR6BSGOTJEqSJGnRiB8ec7wcqFYswieEJTuU5piJOtjogpZtQdjX6yBgE'}], [{'type': 'web_search_result_location', 'cited_text': 'Approximately 90 percent of the worldâ€™s sea otters live in coastal Alaska. Many live in the waters surrounding public lands including Kodiak National ...', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDCEtwBsz/t0ZWaDGgBoM6OQu+PIDoRxHlngHIjDeBj9O4YbAEo1+CdXQzqI9gSuZbzAJ/B42fA6z9P9UijCjsWojIPJbFJt3HZJTznMqF80ialdOVVltYsXRt2Mjn+DLKkH3rzJDGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones.', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDOEaHW82jSPI5g7FsBoM60+epfI7LgPnKEV5IjBiJGxsDBp8QCqx8A7kWx3ofMlVIjuZuFTZTn8M7SwSdxnhOSMBh+5SOqPRvX5B5KQqEzcNsVtrRyqPEw5Tao9oLhIv5W0YBA=='}, {'type': 'web_search_result_location', 'cited_text': 'They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDOKZkPznRHYoNNvqCRoM6twIrlL/x75hAxnaIjCD/hqdszfZTIEXm32Sm9lk634XbdlPueNMlCaJN5eBw/6HwMev7QzVKrOG1zJ/jIQqFPENGSQFdhEPHkTSKhx+spreAR6rGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'In the winter, otters have found the easiest and perhaps most fun way to get around is by sliding. After a few bumps, they can slide up to 22 feet on ...', 'url': 'https://www.nationalforests.org/blog/seven-quick-facts-about-river-otters', 'title': 'Seven Quick Facts About River Otters - National Forest Foundation', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDOJ0KBplvltpuIlQzxoMlK2D3XCG21FxpRKYIjAFdrVw0Tpscr2pi91GTfswru0V+0wuV5JYfcyU0N+0WeMPKOn//pkPeSSDAT4m9QoqF+P5fndrc+WPHzlI/aLJ9Y/45M68MSk+GAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'An otterâ€™s lung capacity is 2.5 times greater than that of similar-sized land mammals. Sea otters have been known to stay submerged for more than 5 mi...', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDBHYavjFS67QNU3gLRoMy2WwbWBGoAWd9wu1IjCxD8HRvqQE8ro6qeJuWnD90wW+9vSvjNrVQAhvktsfPFTOeN9jyZAXhUDK6Y6zeAEqF5xodKqc0tV1e1rRSSKfNsb/d13AJoaOGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'River otters, however, can hold their breath for up to 8 minutes. ', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDBlK9Q6Xx1YfoQmDvxoMHfSraAisqgei9qKoIjC9yhvctFQwoZp2aD8vHUxbRWZYSC0zlJjxmltPF3CVY54U5yJIrKeRZtBaVvRQRxsqFDggo3RPUSz/FSKznB9xfErf2IyEGAQ='}], [{'type': 'web_search_result_location', 'cited_text': '... As a sometimes aquatic creature, it shouldnâ€™t be a surprise that otters can swim up to seven miles per hour and dive down 60 feet. ', 'url': 'https://www.nationalforests.org/blog/seven-quick-facts-about-river-otters', 'title': 'Seven Quick Facts About River Otters - National Forest Foundation', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDDpFJZoFlDI84oickxoMFJPXU5qN0OIrvTySIjDSr1UJHOXm0BmDmvsSiJWKZ7rPdNz5U0ccV6E/smEBVAmtv6dVK8bksPc9UffbKfEqE4/opcc7u+bEgUV+1K/1fEBWpF0YBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'Its usual source of food is fish, and further downriver, eels, but it may sample frogs and birds. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDPg6uveMK499mOTU+hoMZheFfmCpUllr+Iu2IjCNP3rl/a6wJSEAMhmSltUJYE3djLl2q8n3LMlOiEDePC6dMRVeKLZjVKHsaWT4uXsqFNtW4DWBcz/3674v314DoQnjXW5KGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters can eat 25 per cent of their body weight in food each day. Their diet is varied but you can commonly find them munching on sea urchins, cra...', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDNzuGthV90sYZpm1XhoMX4c4E2bsm7Mr4y/RIjBm93pW7odAlzZWlkoEn4YC/6QgGlbcogvlCckYJxY+HwYSdx7vVDWUZMYhoi1JdT4qF8tJSCp5GxonsTDNRjObMPlZq27u0dUAGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters are one of the only marine mammals that uses tools. All of their favorite foods need to be cracked open to eat, so these intelligent animal...', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDK84fNFk3DL46oPd3BoM3C4QHfoxtOobd62pIjCju9ndJOrhu1ER/0S2FsAeFYV3DE0EQI4z079GoYvlFxXpWlUn8WicVmVXkGnIllAqF0H2ktpHG37NYhp4ZmEdV2ZMHV3pgwHFGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'While floating on their backs, sea otters not only nap, but also use rocks to help them open mussels or other shellfish. Otters place a rock on their ...', 'url': 'https://kids.nationalgeographic.com/animals/mammals/facts/sea-otter', 'title': 'Sea Otter - Animal profile, pictures, facts, range map | National Geographic Kids', 'encrypted_index': 'EpEBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDOcIKNrv7AjQ4xjUuBoMdWF1+I/zEKmbRjsoIjC8g+6bmPdEzJGnId/Tz6MxuzFPyuGGhyXSXXN5I+RtdPujp054xSBgOTTU3mQlEroqFXvsgNtKuu7thNdrGGipReDeHEhzSBgE'}], [{'type': 'web_search_result_location', 'cited_text': '... Sea otters, particularly mothers and pups, sometimes hold hands while floating on their backs. Hand-holding keeps the otters from drifting away fr...', 'url': 'https://www.treehugger.com/fascinating-facts-about-otters-4869357', 'title': '15 Fascinating Facts About Otters', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDN+85YOp8fLb7ZYrXBoM14iRy79mw7rWwqBGIjAXKGt39ibO+mfYUSr7AnNf4DHeA/KaP8wUPqYikuoX6iIjrA7M2xbWFV1lRvsGFQIqF6ER1tz2m3HVH17XamHFKpBgDREE+AqWGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'When they sleep, sea otters wrap themselves in kelp to ensure they do not float away. They will also hold each otherâ€™s hands to stay close while sleep...', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDL6tLM0XesQ9aCfXmxoMMH6iuVcP+mfQMButIjCAaR3q2hdM1x6etzzpwNyoBx3DEekuiT1q/pFds3v+x9K3RrU2aZP3x7w0zw9pv5IqFxsy/lAJFSHWRgDMNXmE6GjgDDguLpCeGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'The gestation period in otters is about 60 to 86 days. The newborn pup is cared for by the bitch, dog, and older offspring. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDOorME2q55ULZg4QexoMBYcQwN/oTNDYAbMoIjAHoCUHCevOOxUn8J5R/csCz0oa52656xa4J/qUQjn0cXEyPEqTo7nddK4GxM8OsFoqF73u5ZcTFrawFDlo3jjKFg80bKGwoU6IGAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'A newborn pup needs constant attention and will stay with its mother for six months until it develops survival skills. ', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDA1Q6NW2NuMqBTu6+xoMgSqESeCjkcreb178IjD2qcXK3mnfjTZoLUaSm0b21TiN36qMJQQ9tea5kPhLgIQIvtrrtaFtg+NkGR52uoMqE+6R96aPLltmXaRTSjcu/ft9k+4YBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'Fun fact: An otter pupâ€™s fur is so dense that it canâ€™t dive underwater until it gets its adult fur. ', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDLCI5ucRTtFzZ4XschoMf3Z+nP1tGa+Fv6lFIjA5QwBa1Cd6xF6erot6QKgO9AgQm33eGexWKNWJLFEIVIH4Ei3awsCWCG7OO1CdMiYqE4LViCgxlk7Qu4ujtU8pwI0TxPwYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'All otter species appear on the International Union for Conservation of Nature\\'s Red List of Threatened Species, and only one is listed as \"least conc...', 'url': 'https://www.treehugger.com/fascinating-facts-about-otters-4869357', 'title': '15 Fascinating Facts About Otters', 'encrypted_index': 'Eo8BCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDASm7wwKfqJ02rY6VxoMXJYge5L/1uJvxBe8IjABLTHPk9qrY95KaZ889xUQ+XjPhnjMiFyX2lWqBHwvb9pL/75S4IP2Jh8wInL23lEqE9f1hknwfp5RQgPNnZu/46XbaPQYBA=='}, {'type': 'web_search_result_location', 'cited_text': 'All otter species appear on the IUCN Red List of Threatened Species, and only one is listed as \"least concern.\"', 'url': 'https://www.treehugger.com/fascinating-facts-about-otters-4869357', 'title': '15 Fascinating Facts About Otters', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDJx/9wsOw7GKP1s53hoM5GPBIbEQFgsRSSE3IjAVqnPPJEJgORjUONrhE7i04BCJn2ts9tjinfci0T0WZymeXKeS3vfDkor5fA75NakqFHXOal4oSeyoyzghKcLMVfH6gJK9GAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Of the 13 species of otter, IUCN lists five as endangered, five as near-threatened, and two as vulnerable. Only the North American river otter is a sp...', 'url': 'https://www.treehugger.com/fascinating-facts-about-otters-4869357', 'title': '15 Fascinating Facts About Otters', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDFZLxNBLp93xRAV4ORoM66TP5RaC9Xfltn5uIjDHmNRpik/508kClvqbLqTCIqzIa823196P0hZ4M1eAyq+HQ2CYdZ0jRGTpl6bWC10qF7v7bcWcDZI6ZUOVQYgCMHrIW0g8jpX7GAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Hunted to the edge of extinction by fur traders in the 18th and 19th centuries, the few remaining sea otters (about 2,000 scattered in remnant colonie...', 'url': 'https://www.doi.gov/blog/12-facts-about-otters-sea-otter-awareness-week', 'title': '12 Facts About Otters for Sea Otter Awareness Week | U.S. Department of the Interior', 'encrypted_index': 'EpABCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDA3dcZXwc0U2rbk+JRoMBSDj8VOhkFq4/E9hIjACgN/5zJTFnLNXGhBJU5WNcIOfvqgxgIZIf0cMYd/+N3+QdbNlKS2O/qkX4OtjSekqFHBRFepEDtcJDSV6bIbX2MoN7kD1GAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters are considered a keystone species.Keystone species are plants or animals that play an important role in how an entire ecosystem functions. ...', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'EpMBCioIBRgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDD+fdOuV2GkpTk+8GRoMIhzRE2myYyYUO0XqIjADWdi41JkxghuIIEryX90fGrOwdr4CEFKlg1s+QEmG32hPvXF4bCltKJNWCPtMndsqF2XboFYsfHMc1QK1x7QSsCwHgvPpsuz5GAQ='}]], 'thinking_blocks': None}))], usage=Usage(completion_tokens=1811, prompt_tokens=13491, total_tokens=15302, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools = [{\n",
    "    \"type\": \"web_search_20250305\",\n",
    "    \"name\": \"web_search\",\n",
    "    \"max_uses\": 5\n",
    "}]\n",
    "chat = Chat(ms[1], tools=tools)\n",
    "res = chat(\"Search the web and tell me about otters\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test multi tool calling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== claude-sonnet-4-20250514 ===\n",
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=13 + b=9\n",
      "ModelResponse(id='chatcmpl-20ce68c7-010c-4579-bb09-e12a728ce570', created=1751374936, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The results are:\\n- 5 + 3 = 8\\n- 13 + 9 = 22', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=31, prompt_tokens=612, total_tokens=643, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))\n",
      "=== openai/gpt-4o-mini ===\n",
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=13 + b=9\n",
      "ModelResponse(id='chatcmpl-BoUuozQcAEhp4a0WG0TFRO3pfJef6', created=1751374938, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of 5 + 3 is 8, and the result of 13 + 9 is 22.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=26, prompt_tokens=135, total_tokens=161, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')\n"
     ]
    }
   ],
   "source": [
    "class Chat:\n",
    "    def __init__(self, model: str, sp='', temp=0, tools: list = None, \n",
    "                 hist: list = None, ns: Optional[dict] = None):\n",
    "        \"LiteLLM chat client.\"\n",
    "        self.model = model\n",
    "        if hist is None: hist = []\n",
    "        if tools is None: tools = []\n",
    "        \n",
    "        # Set up namespace following claudette pattern\n",
    "        if ns is None and tools: ns = mk_ns(tools)\n",
    "        elif ns is None: ns = globals()\n",
    "        \n",
    "        # Cache tool schemas\n",
    "        self.tool_schemas = []\n",
    "        for t in tools:\n",
    "            if isinstance(t, dict): self.tool_schemas.append(t)\n",
    "            elif t: self.tool_schemas += [{'type':'function', 'function':function_to_dict(t)}]\n",
    "        self.h, self.sp, self.temp, self.tools, self.ns = hist, sp, temp, tools, ns\n",
    "    \n",
    "    def _prepare_messages(self, msg=None):\n",
    "        \"Prepare the messages list for the API call, extracting from ModelResponse objects\"\n",
    "        messages = [{\"role\": \"system\", \"content\": self.sp}] if self.sp else []\n",
    "        \n",
    "        if isinstance(msg, str): self.h.append({\"role\": \"user\", \"content\": msg})\n",
    "        elif isinstance(msg, dict): self.h.append(msg)\n",
    "        elif isinstance(msg, list): self.h.extend(msg)\n",
    "        elif msg is None: pass\n",
    "        else: raise ValueError(f\"Can't parse {msg=}\")\n",
    "            \n",
    "        for m in self.h: \n",
    "            if hasattr(m, 'choices'):  messages.append(m.choices[0].message.model_dump())\n",
    "            else:  messages.append(m if isinstance(m, dict) else m.model_dump())\n",
    "        return messages\n",
    "    \n",
    "    def _call(self, msg=None, stream=False, max_tool_rounds=1, tool_round=0, \n",
    "              cont_func=noop, final_prompt=None, **kwargs):\n",
    "        \"Internal call method that always yields responses\"\n",
    "        messages = self._prepare_messages(msg)\n",
    "        \n",
    "        # Make the API call\n",
    "        res = litellm.completion(model=self.model, messages=messages, stream=stream, \n",
    "                               tools=self.tool_schemas, temperature=self.temp, **kwargs)\n",
    "        \n",
    "        if stream: res = yield from stream_with_complete(res)        \n",
    "\n",
    "        self.h.append(res)\n",
    "        yield res\n",
    "\n",
    "        m = res.choices[0].message\n",
    "        if tcs := m.tool_calls:\n",
    "            tool_results = [_lite_call_func(tc, ns=self.ns) for tc in tcs]\n",
    "            \n",
    "            # Check continuation function: user_msg, llm_response, tool_results\n",
    "            user_msg = self.h[-2] if len(self.h) >= 2 else None\n",
    "            if not cont_func(user_msg, m, tool_results):\n",
    "                # Send final prompt when cont_func stops the loop\n",
    "                if final_prompt:\n",
    "                    final_msg = tool_results + [{\"role\": \"user\", \"content\": final_prompt}]\n",
    "                    yield from self._call(final_msg, stream, max_tool_rounds, tool_round+1, cont_func, final_prompt, tool_choice='none', **kwargs)\n",
    "                return\n",
    "                \n",
    "            # Continue with more rounds or final round\n",
    "            if tool_round < max_tool_rounds - 1:\n",
    "                yield from self._call(tool_results, stream, max_tool_rounds, tool_round+1, cont_func, final_prompt, **kwargs)\n",
    "            else:\n",
    "                # Final round - inject final_prompt if provided and set tool_choice=None\n",
    "                final_msg = tool_results + ([{\"role\": \"user\", \"content\": final_prompt}] if final_prompt else [])\n",
    "                yield from self._call(final_msg, stream, max_tool_rounds, tool_round+1, cont_func, final_prompt, tool_choice='none', **kwargs)\n",
    "    \n",
    "    def __call__(self, msg=None, stream=False, max_tool_rounds=1, cont_func=noop, final_prompt=None, return_all=False, **kwargs):\n",
    "        \"Main call method - handles streaming vs non-streaming\"\n",
    "        result_gen = self._call(msg, stream, max_tool_rounds, 0, cont_func, final_prompt, **kwargs)        \n",
    "        if stream: return result_gen              # streaming\n",
    "        elif return_all: return list(result_gen)  # toolloop behavior\n",
    "        else: return last(result_gen)             # normal chat behavior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== claude-sonnet-4-20250514 ===\n",
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=8 + b=7\n",
      "TOOL CALLED a=15 + b=11\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "I'll solve this step by step using the addition function.\n",
       "\n",
       "First, let me calculate 5 + 3:\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-6a229bb0-c27c-47ca-be83-132753beca64`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=96, prompt_tokens=399, total_tokens=495, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-6a229bb0-c27c-47ca-be83-132753beca64', created=1751379030, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll solve this step by step using the addition function.\\n\\nFirst, let me calculate 5 + 3:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), id='toolu_01JonBfC5D18hgie7mh3heD3', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=96, prompt_tokens=399, total_tokens=495, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Now I'll add 7 to that result (8 + 7):\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-b9ca4908-1f3c-41f7-938e-7a3283490be0`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=88, prompt_tokens=508, total_tokens=596, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-b9ca4908-1f3c-41f7-938e-7a3283490be0', created=1751379034, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"Now I'll add 7 to that result (8 + 7):\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 8, \"b\": 7}', name='simple_add'), id='toolu_01AEmxYHWvHzxZ95YZzAsBqb', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=88, prompt_tokens=508, total_tokens=596, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Finally, I'll add 11 to that result (15 + 11):\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-5673c14d-9776-477c-a0f6-bb25714a6780`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=89, prompt_tokens=609, total_tokens=698, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-5673c14d-9776-477c-a0f6-bb25714a6780', created=1751379038, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"Finally, I'll add 11 to that result (15 + 11):\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 15, \"b\": 11}', name='simple_add'), id='toolu_01KdZPMM5AxuvCBCwC1DZ5GJ', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=89, prompt_tokens=609, total_tokens=698, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "So working step by step:\n",
       "- 5 + 3 = 8\n",
       "- 8 + 7 = 15  \n",
       "- 15 + 11 = 26\n",
       "\n",
       "Therefore, ((5 + 3) + 7) + 11 = **26**\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-aee46720-01b7-440d-8245-96e13ce02785`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=68, prompt_tokens=711, total_tokens=779, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-aee46720-01b7-440d-8245-96e13ce02785', created=1751379041, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='So working step by step:\\n- 5 + 3 = 8\\n- 8 + 7 = 15  \\n- 15 + 11 = 26\\n\\nTherefore, ((5 + 3) + 7) + 11 = **26**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=68, prompt_tokens=711, total_tokens=779, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== openai/gpt-4o-mini ===\n",
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=7 + b=11\n",
      "TOOL CALLED a=8 + b=18\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ simple_add({\"a\": 5, \"b\": 3})\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 7, \"b\": 11})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoVyzdn8Embb2JFdlGARlyJAjNTvT`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=52, prompt_tokens=61, total_tokens=113, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoVyzdn8Embb2JFdlGARlyJAjNTvT', created=1751379041, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), id='call_UBF25IeSzEpS3DzvkpOZLy3y', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 7, \"b\": 11}', name='simple_add'), id='call_SbXFcphPUjHbr9CO2du4xXzH', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=52, prompt_tokens=61, total_tokens=113, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ simple_add({\"a\":8,\"b\":18})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoVz0xc4DrvUQVeXj18GSlqdD7LiP`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=18, prompt_tokens=129, total_tokens=147, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoVz0xc4DrvUQVeXj18GSlqdD7LiP', created=1751379042, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":8,\"b\":18}', name='simple_add'), id='call_2CRDBfSJY1H4nGNAgUEiFzca', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=18, prompt_tokens=129, total_tokens=147, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Let's break it down step by step:\n",
       "\n",
       "1. First, we calculate \\(5 + 3\\):\n",
       "   \\[\n",
       "   5 + 3 = 8\n",
       "   \\]\n",
       "\n",
       "2. Next, we add \\(7 + 11\\):\n",
       "   \\[\n",
       "   7 + 11 = 18\n",
       "   \\]\n",
       "\n",
       "3. Finally, we add the results from the previous steps:\n",
       "   \\[\n",
       "   8 + 18 = 26\n",
       "   \\]\n",
       "\n",
       "So, \\(((5 + 3) + 7) + 11 = 26\\).\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoVz174p7RakMdVuyAu2cG93IbOO3`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=117, prompt_tokens=156, total_tokens=273, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoVz174p7RakMdVuyAu2cG93IbOO3', created=1751379043, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Let's break it down step by step:\\n\\n1. First, we calculate \\\\(5 + 3\\\\):\\n   \\\\[\\n   5 + 3 = 8\\n   \\\\]\\n\\n2. Next, we add \\\\(7 + 11\\\\):\\n   \\\\[\\n   7 + 11 = 18\\n   \\\\]\\n\\n3. Finally, we add the results from the previous steps:\\n   \\\\[\\n   8 + 18 = 26\\n   \\\\]\\n\\nSo, \\\\(((5 + 3) + 7) + 11 = 26\\\\).\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=117, prompt_tokens=156, total_tokens=273, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test (multi-)toolloop\n",
    "for m in ms:\n",
    "    print(f'=== {m} ===')\n",
    "    chat = Chat(m, tools=[simple_add])\n",
    "    res = chat(\"What's ((5 + 3)+7)+11? Work step by step\", stream=False, return_all=True,max_tool_rounds=5)\n",
    "    for r in res: display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gemini/gemini-2.5-flash-preview-04-17',\n",
       " 'claude-sonnet-4-20250514',\n",
       " 'openai/gpt-4o-mini']"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@patch(as_prop=True)\n",
    "def cost(self: Chat):\n",
    "    \"Total cost of all responses in conversation history\"\n",
    "    return sum(getattr(r, '_hidden_params', {}).get('response_cost')  or 0\n",
    "               for r in self.h if hasattr(r, 'choices'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some models support parallel tool calling. I.e. sending multiple tool call requests in one conversation step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=5 + b=3\n",
      "TOOL CALLED a=7 + b=2\n",
      "MULTIPLY: 8 * 9\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ simple_add({\"a\": 5, \"b\": 3})\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 7, \"b\": 2})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWUuJ6x9FVpiW0haODAVGEdvzZbO`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=52, prompt_tokens=81, total_tokens=133, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWUuJ6x9FVpiW0haODAVGEdvzZbO', created=1751381020, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 5, \"b\": 3}', name='simple_add'), id='call_VFpgsyCN1hkE10Yu4PcS6H3G', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 7, \"b\": 2}', name='simple_add'), id='call_XZ1q14D7ue7AqsxLx4nTzwFw', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=52, prompt_tokens=81, total_tokens=133, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ multiply({\"a\":8,\"b\":9})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWUvqPaatqHkwOAjrGH7CekpzwAj`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=149, total_tokens=166, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWUvqPaatqHkwOAjrGH7CekpzwAj', created=1751381021, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":8,\"b\":9}', name='multiply'), id='call_drVDKN04ocKZEjlsdm52zv7D', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=17, prompt_tokens=149, total_tokens=166, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "The result of the calculation \\((5 + 3) * (7 + 2)\\) is \\(72\\).\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWUwkuaNwiKe9sP3Gd3o6CEoyNeZ`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=26, prompt_tokens=174, total_tokens=200, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWUwkuaNwiKe9sP3Gd3o6CEoyNeZ', created=1751381022, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of the calculation \\\\((5 + 3) * (7 + 2)\\\\) is \\\\(72\\\\).', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=26, prompt_tokens=174, total_tokens=200, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def multiply(a: int, b: int) -> int:\n",
    "    \"Multiply two numbers\"\n",
    "    print(f\"MULTIPLY: {a} * {b}\")\n",
    "    return a * b\n",
    "\n",
    "chat = Chat('openai/gpt-4o-mini', tools=[simple_add, multiply])\n",
    "res = chat(\"Calculate (5 + 3) * (7 + 2)\", max_tool_rounds=5, return_all=True)\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See it did the additions in one go!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toolloop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh wait above we also demonstrated a toolloop! With litellm we might actually be able to put toolloop straight into the main `__call__` of Chat.\n",
    "\n",
    "We have the new `return_all=False` parameter. It's only relevant when you're not streaming. Because if you're streaming we always send back everything. But if you're not, then `return_all` determines if we only send back the last llm response or all of them. \n",
    "\n",
    "If you set max_tool_rounds to > 1 and return_all=True then you basically have a toolloop I think."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets show toolloop hitting a max rounds limit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=10 + b=5\n",
      "TOOL CALLED a=2 + b=1\n",
      "MULTIPLY: 15 * 3\n",
      "Got 3 responses\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ simple_add({\"a\": 10, \"b\": 5})\n",
       "\n",
       "ðŸ”§ simple_add({\"a\": 2, \"b\": 1})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWXPSlEvAVVfSCFzroiYQnv6KLbq`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=52, prompt_tokens=109, total_tokens=161, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWXPSlEvAVVfSCFzroiYQnv6KLbq', created=1751381175, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='call_Tv6QRnn77WBz5qLLTn69JAqK', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='call_iflH6y6LvqA4oQmKG4qkXrMO', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=52, prompt_tokens=109, total_tokens=161, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ multiply({\"a\":15,\"b\":3})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWXQIpSeKGEEQoEElI7aswVgLpKt`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=177, total_tokens=194, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWXQIpSeKGEEQoEElI7aswVgLpKt', created=1751381176, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":15,\"b\":3}', name='multiply'), id='call_roajsEsiNLMsdjPb7Yu630nV', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=17, prompt_tokens=177, total_tokens=194, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "So far, I have calculated the following steps:\n",
       "\n",
       "1. **Addition**: \\(10 + 5 = 15\\)\n",
       "2. **Addition**: \\(2 + 1 = 3\\)\n",
       "3. **Multiplication**: \\(15 \\times 3 = 45\\)\n",
       "\n",
       "Next, we need to divide \\(45\\) by \\(3\\) to complete the calculation.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWXRpJdfBGrYZup0Rp8h6AbxFMvb`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=80, prompt_tokens=214, total_tokens=294, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWXRpJdfBGrYZup0Rp8h6AbxFMvb', created=1751381177, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='So far, I have calculated the following steps:\\n\\n1. **Addition**: \\\\(10 + 5 = 15\\\\)\\n2. **Addition**: \\\\(2 + 1 = 3\\\\)\\n3. **Multiplication**: \\\\(15 \\\\times 3 = 45\\\\)\\n\\nNext, we need to divide \\\\(45\\\\) by \\\\(3\\\\) to complete the calculation.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=80, prompt_tokens=214, total_tokens=294, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test 2: Hit max_tool_rounds limit with final_prompt\n",
    "def divide(a: int, b: int) -> float:\n",
    "    \"Divide two numbers\"\n",
    "    print(f\"DIVIDE: {a} / {b}\")\n",
    "    return a / b\n",
    "\n",
    "chat = Chat(m, tools=[simple_add, multiply, divide])\n",
    "res = chat(\"Calculate ((10 + 5) * 3) / (2 + 1) step by step\", \n",
    "           max_tool_rounds=2, \n",
    "           final_prompt=\"Please summarize what you've calculated so far\",\n",
    "           return_all=True)\n",
    "print(f\"Got {len(res)} responses\")\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STOPPING: Found error in tool result\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ error_tool({\"x\":15})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWY1u1xejxD7KwDXJeqHe4NZx5Z8`\n",
       "- model: `gpt-4o-mini-2024-07-18`\n",
       "- finish_reason: `tool_calls`\n",
       "- usage: `Usage(completion_tokens=14, prompt_tokens=49, total_tokens=63, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWY1u1xejxD7KwDXJeqHe4NZx5Z8', created=1751381213, model='gpt-4o-mini-2024-07-18', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"x\":15}', name='error_tool'), id='call_6frTgHD8jZPKAvZn1IDmR9wk', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=14, prompt_tokens=49, total_tokens=63, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test 3: Custom cont_func to stop early\n",
    "def stop_on_error(user_msg, llm_resp, tool_results):\n",
    "    \"Stop if any tool result contains 'error'\"\n",
    "    for result in tool_results:\n",
    "        if 'error' in str(result['content']).lower():\n",
    "            print(\"STOPPING: Found error in tool result\")\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def error_tool(x: int) -> str:\n",
    "    \"A tool that sometimes errors\"\n",
    "    if x > 10: return \"Error: number too big!\"\n",
    "    return f\"Success: {x}\"\n",
    "\n",
    "chat = Chat(m, tools=[error_tool])\n",
    "res = chat(\"Try error_tool with 15\", \n",
    "           max_tool_rounds=3,\n",
    "           cont_func=stop_on_error,\n",
    "           return_all=True)\n",
    "for r in res: display(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also show streaming with toolloops:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming responses:\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "ðŸ”§ simple_add({\"a\": 4, \"b\": 6})\n",
       "\n",
       "ðŸ”§ multiply({\"a\": 10, \"b\": 2})\n",
       "\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWZwFDuE2LwQIM5VhAZltJVOCmWe`\n",
       "- model: `gpt-4o-mini`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=51, prompt_tokens=77, total_tokens=128, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWZwFDuE2LwQIM5VhAZltJVOCmWe', created=1751381333, model='gpt-4o-mini', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 4, \"b\": 6}', name='simple_add'), id='call_KwqjAbq2AV7Wb4duUXXGkc5E', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 2}', name='multiply'), id='call_FErPkteZZe05NZZTsfncSGZC', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=51, prompt_tokens=77, total_tokens=128, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TOOL CALLED a=4 + b=6\n",
      "MULTIPLY: 10 * 2\n",
      "The result of the calculation \\((4 + 6) * 2\\) is \\(20\\)."
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The result of the calculation \\((4 + 6) * 2\\) is \\(20\\).\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-BoWZxGlsGaPinEeru0i9py6dfjZiq`\n",
       "- model: `gpt-4o-mini`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=23, prompt_tokens=144, total_tokens=167, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-BoWZxGlsGaPinEeru0i9py6dfjZiq', created=1751381333, model='gpt-4o-mini', object='chat.completion', system_fingerprint='fp_34a54ae93c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of the calculation \\\\((4 + 6) * 2\\\\) is \\\\(20\\\\).', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=23, prompt_tokens=144, total_tokens=167, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test 4: Streaming with tool loops\n",
    "chat = Chat(m, tools=[simple_add, multiply])\n",
    "stream_gen = chat(\"Calculate (4 + 6) * 2\", max_tool_rounds=3, stream=True)\n",
    "\n",
    "print(\"Streaming responses:\")\n",
    "for chunk in stream_gen:\n",
    "    if isinstance(chunk, litellm.ModelResponseStream): \n",
    "        if c:= chunk.choices[0].delta.content: print(c,end='')\n",
    "    else: display(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def _lite_call_func(tc,ns,raise_on_err=True):\n",
    "    res = call_func(tc.function.name, json.loads(tc.function.arguments),ns=ns)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": str(res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
