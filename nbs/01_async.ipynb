{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81495cb6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ea33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp asink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json,asyncio\n",
    "from litellm import acompletion, ModelResponse, stream_chunk_builder\n",
    "from toolslm.funccall import call_func_async\n",
    "from fastcore.utils import *\n",
    "from lisette.core import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _lite_call_func_async(tc, ns, raise_on_err=True):\n",
    "    res = await call_func_async(tc.function.name, json.loads(tc.function.arguments), ns=ns)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": str(res)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af586d9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AsyncStreamResult:\n",
    "    def __init__(self, agen, postproc=noop):\n",
    "        self.agen = agen\n",
    "        self.postproc = postproc\n",
    "        self.complete = None\n",
    "        \n",
    "    async def __aiter__(self):\n",
    "        chunks = []\n",
    "        async for chunk in self.agen:\n",
    "            chunks.append(chunk)\n",
    "            yield chunk\n",
    "        self.postproc(chunks)\n",
    "        self.complete = stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94610e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AsyncChat(Chat):\n",
    "    async def _call(self, msg=None, stream=False, max_tool_rounds=1, tool_round=0, final_prompt=None, tool_choice=None, **kwargs):\n",
    "        \"Internal method that always yields responses\"\n",
    "        msgs = self._prepare_msgs(msg)\n",
    "        res = await acompletion(model=self.model, messages=msgs, stream=stream,\n",
    "                         tools=self.tool_schemas, temperature=self.temp, **kwargs)\n",
    "        if stream:\n",
    "            res = AsyncStreamResult(res, postproc=cite_footnotes)\n",
    "            async for chunk in res: yield chunk\n",
    "            res = res.complete\n",
    "        \n",
    "        yield res\n",
    "        m = res.choices[0].message\n",
    "        self.hist.append(m)\n",
    "\n",
    "        if tcs := m.tool_calls:\n",
    "            tool_results = [await _lite_call_func_async(tc, ns=self.ns) for tc in tcs]\n",
    "            if tool_round>=max_tool_rounds-1:\n",
    "                tool_results += ([{\"role\": \"user\", \"content\": final_prompt}] if final_prompt else [])\n",
    "                tool_choice='none'\n",
    "            async for result in self._call(\n",
    "                tool_results, stream, max_tool_rounds, tool_round+1,\n",
    "                final_prompt, tool_choice=tool_choice, **kwargs):\n",
    "                    yield result\n",
    "    \n",
    "    async def __call__(self, msg=None, stream=False, max_tool_rounds=1, final_prompt=None, return_all=False, **kwargs):\n",
    "        \"Main call method - handles streaming vs non-streaming\"\n",
    "        if stream: return self._call(msg, stream, max_tool_rounds, 0, final_prompt, **kwargs)\n",
    "        result_gen = self._call(msg, stream, max_tool_rounds, 0, final_prompt, **kwargs)\n",
    "        if return_all: return [result async for result in result_gen] # toolloop behavior\n",
    "        else: return [result async for result in result_gen][-1]      # normal chat behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d055a48",
   "metadata": {},
   "source": [
    "## Async chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf293d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2+2 equals 4.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-CBijouwJTVhEI68vZKkpBiCEH3oYM`\n",
       "- model: `gpt-3.5-turbo-0125`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=7, prompt_tokens=14, total_tokens=21, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-CBijouwJTVhEI68vZKkpBiCEH3oYM', created=1756909616, model='gpt-3.5-turbo-0125', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='2+2 equals 4.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=7, prompt_tokens=14, total_tokens=21, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = AsyncChat(model=\"gpt-3.5-turbo\")\n",
    "await chat(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c346c4",
   "metadata": {},
   "source": [
    "## Async chat w tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_add(a: int, b: int) -> int:\n",
    "    \"Add two numbers asynchronously\"\n",
    "    print('>>> async add is being called!')\n",
    "    await asyncio.sleep(0.1)  # Simulate async work\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00399522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "async add is being called!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The result of 5 + 7 is 12.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-0a44c055-e3c2-4222-bcb3-b34c24cfb593`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=528, total_tokens=545, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-0a44c055-e3c2-4222-bcb3-b34c24cfb593', created=1756909674, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of 5 + 7 is 12.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=17, prompt_tokens=528, total_tokens=545, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_tools = AsyncChat(model=\"claude-sonnet-4-20250514\", tools=[async_add])\n",
    "await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac5495",
   "metadata": {},
   "source": [
    "## Streaming Async Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18cc48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AsyncChat(model=\"claude-sonnet-4-20250514\")\n",
    "stream_gen = await chat(\"Count to 50\", stream=True)\n",
    "\n",
    "async for chunk in stream_gen:\n",
    "    if isinstance(chunk, ModelResponse): display(chunk)\n",
    "    else: print(delta_text(chunk) or '',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af541c2",
   "metadata": {},
   "source": [
    "## Streaming Async Chat w tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76751bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_stream_tools = AsyncChat(model=\"claude-sonnet-4-20250514\", tools=[async_add])\n",
    "stream_gen = await chat_stream_tools(\"What's 15 + 23? Use the tool and then explain the result.\", stream=True)\n",
    "\n",
    "async for chunk in stream_gen:\n",
    "    if isinstance(chunk, ModelResponse): display(chunk)\n",
    "    else: print(delta_text(chunk) or '', end='')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
