{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81495cb6",
   "metadata": {},
   "source": [
    "# Async\n",
    "\n",
    "> Implements an `AsyncChat` version that mirrors the regular `lisette.Chat` as closely as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ea33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp asink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json,asyncio\n",
    "from litellm import acompletion, ModelResponse, ModelResponseStream, stream_chunk_builder\n",
    "from toolslm.funccall import call_func_async\n",
    "from fastcore.utils import *\n",
    "from lisette.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7371d580",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _alite_call_func(tc, ns, raise_on_err=True):\n",
    "    res = await call_func_async(tc.function.name, json.loads(tc.function.arguments), ns=ns)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": str(res)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7b457",
   "metadata": {},
   "source": [
    "As you cannot receive the return value of an async generator we have to write a little wrapper to capture this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e72153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@asave_iter\n",
    "async def astream_result(self, agen, postproc=noop):\n",
    "    chunks = []\n",
    "    async for chunk in agen:\n",
    "        chunks.append(chunk)\n",
    "        yield chunk\n",
    "    postproc(chunks)\n",
    "    self.value = stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94610e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AsyncChat(Chat):\n",
    "    async def _call(self, msg=None, prefill=None, temp=None, think=None, stream=False, max_tool_rounds=1, tool_round=0, final_prompt=None, tool_choice=None, **kwargs):\n",
    "        \"Internal method that always yields responses\"\n",
    "        msgs = self._prepare_msgs(msg, prefill)\n",
    "        res = await acompletion(model=self.model, messages=msgs, stream=stream,\n",
    "                         tools=self.tool_schemas, reasoning_effort=effort.get(think), \n",
    "                         # temperature is not supported when reasoning\n",
    "                         temperature=None if think else (temp if temp is not None else self.temp), \n",
    "                         **kwargs)\n",
    "        if stream:\n",
    "            res = astream_result(res, postproc=cite_footnotes)\n",
    "            async for chunk in res: yield chunk\n",
    "            res = res.value\n",
    "        \n",
    "        yield res\n",
    "        self.hist.append(m:=res.choices[0].message)\n",
    "\n",
    "        if tcs := m.tool_calls:\n",
    "            tool_results = []\n",
    "            for tc in tcs:\n",
    "                result = await _alite_call_func(tc, ns=self.ns)\n",
    "                tool_results.append(result)\n",
    "                yield result\n",
    "            \n",
    "            if tool_round>=max_tool_rounds-1:\n",
    "                tool_results += ([{\"role\": \"user\", \"content\": final_prompt}] if final_prompt else [])\n",
    "                tool_choice='none'\n",
    "            \n",
    "            async for result in self._call(\n",
    "                tool_results, stream, max_tool_rounds, tool_round+1,\n",
    "                final_prompt, tool_choice=tool_choice, **kwargs):\n",
    "                    yield result\n",
    "    \n",
    "    async def __call__(self, msg=None, prefill=None, temp=None, think=None, stream=False, max_tool_rounds=1, final_prompt=None, return_all=False, **kwargs):\n",
    "        \"Main call method - handles streaming vs non-streaming\"\n",
    "        result_gen = self._call(msg, prefill, temp, think, stream, max_tool_rounds, 0, final_prompt, **kwargs)\n",
    "        if stream or return_all: return result_gen\n",
    "        async for res in result_gen: pass\n",
    "        return res # normal chat behavior only return last msg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662f90c",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d055a48",
   "metadata": {},
   "source": [
    "### Async chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf293d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2 + 2 = 4\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-30629f13-f308-4835-a014-db345c686773`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-30629f13-f308-4835-a014-db345c686773', created=1757398259, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='2 + 2 = 4', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = AsyncChat(model=\"claude-sonnet-4-20250514\")\n",
    "await chat(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c346c4",
   "metadata": {},
   "source": [
    "### Async chat w tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_add(a: int, b: int) -> int:\n",
    "    \"Add two numbers asynchronously\"\n",
    "    print('>>> async add is being called!')\n",
    "    await asyncio.sleep(0.1)  # Simulate async work\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00399522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> async add is being called!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The result of 5 + 7 is 12.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-dea3c9f6-d28d-4e9f-b514-326d45125bd6`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=528, total_tokens=545, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-dea3c9f6-d28d-4e9f-b514-326d45125bd6', created=1757398265, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of 5 + 7 is 12.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=17, prompt_tokens=528, total_tokens=545, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_tools = AsyncChat(model=\"claude-sonnet-4-20250514\", tools=[async_add])\n",
    "await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac5495",
   "metadata": {},
   "source": [
    "### Streaming Async Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18cc48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AsyncChat(model=\"claude-sonnet-4-20250514\")\n",
    "stream_gen = await chat(\"Count to 50\", stream=True)\n",
    "\n",
    "async for chunk in stream_gen:\n",
    "    if   isinstance(chunk, ModelResponseStream): print(delta_text(chunk) or '',end='')\n",
    "    elif isinstance(chunk, ModelResponse):       display(chunk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af541c2",
   "metadata": {},
   "source": [
    "### Streaming Async Chat w tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d3658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_stream_tools = AsyncChat(model=\"claude-sonnet-4-20250514\", tools=[async_add])\n",
    "stream_gen = await chat_stream_tools(\"What's 15 + 23? Use the tool and then explain the result.\", stream=True)\n",
    "\n",
    "async for chunk in stream_gen:\n",
    "    if isinstance(chunk, ModelResponse): display(chunk)\n",
    "    elif isinstance(chunk,dict): continue  # tool result\n",
    "    else: print(delta_text(chunk) or '', end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d916e5",
   "metadata": {},
   "source": [
    "### Streaming Async Thinking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb638d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AsyncChat(model=\"claude-sonnet-4-20250514\")\n",
    "res = await chat(\"What's the most efficient way to sort a list of 1000 random integers?\", think='l',stream=True)\n",
    "\n",
    "async for chunk in res:\n",
    "    if isinstance(chunk, ModelResponse): display(chunk)\n",
    "    elif isinstance(chunk,dict): continue  # tool result\n",
    "    else: print(delta_text(chunk) or '', end='')"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
