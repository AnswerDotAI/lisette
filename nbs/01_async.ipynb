{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "81495cb6",
   "metadata": {},
   "source": [
    "# Async\n",
    "\n",
    "> Implements an `AsyncChat` version that mirrors the regular `lisette.Chat` as closely as possible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ea33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp asink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3a98fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import json,asyncio\n",
    "from litellm import acompletion, ModelResponse, stream_chunk_builder\n",
    "from toolslm.funccall import call_func_async\n",
    "from fastcore.utils import *\n",
    "from lisette.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7371d580",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ba2c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "async def _alite_call_func(tc, ns, raise_on_err=True):\n",
    "    res = await call_func_async(tc.function.name, json.loads(tc.function.arguments), ns=ns)\n",
    "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": str(res)}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8c7b457",
   "metadata": {},
   "source": [
    "As you cannot receive the return value of an async generator we have to write a little wrapper to capture this result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43e72153",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@asave_iter\n",
    "async def astream_result(self, agen, postproc=noop):\n",
    "    chunks = []\n",
    "    async for chunk in agen:\n",
    "        chunks.append(chunk)\n",
    "        yield chunk\n",
    "    postproc(chunks)\n",
    "    self.value = stream_chunk_builder(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94610e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class AsyncChat(Chat):\n",
    "    async def _call(self, msg=None, stream=False, max_tool_rounds=1, tool_round=0, final_prompt=None, tool_choice=None, **kwargs):\n",
    "        \"Internal method that always yields responses\"\n",
    "        msgs = self._prepare_msgs(msg)\n",
    "        res = await acompletion(model=self.model, messages=msgs, stream=stream,\n",
    "                         tools=self.tool_schemas, temperature=self.temp, **kwargs)\n",
    "        if stream:\n",
    "            res = astream_result(res, postproc=cite_footnotes)\n",
    "            async for chunk in res: yield chunk\n",
    "            res = res.value\n",
    "        \n",
    "        yield res\n",
    "        m = res.choices[0].message\n",
    "        self.hist.append(m)\n",
    "\n",
    "        if tcs := m.tool_calls:\n",
    "            tool_results = [await _alite_call_func(tc, ns=self.ns) for tc in tcs]\n",
    "            if tool_round>=max_tool_rounds-1:\n",
    "                tool_results += ([{\"role\": \"user\", \"content\": final_prompt}] if final_prompt else [])\n",
    "                tool_choice='none'\n",
    "            async for result in self._call(\n",
    "                tool_results, stream, max_tool_rounds, tool_round+1,\n",
    "                final_prompt, tool_choice=tool_choice, **kwargs):\n",
    "                    yield result\n",
    "    \n",
    "    async def __call__(self, msg=None, stream=False, max_tool_rounds=1, final_prompt=None, return_all=False, **kwargs):\n",
    "        \"Main call method - handles streaming vs non-streaming\"\n",
    "        if stream: return self._call(msg, stream, max_tool_rounds, 0, final_prompt, **kwargs)\n",
    "        result_gen = self._call(msg, stream, max_tool_rounds, 0, final_prompt, **kwargs)\n",
    "        if return_all: return [result async for result in result_gen] # toolloop behavior\n",
    "        else: return [result async for result in result_gen][-1]      # normal chat behavior"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8662f90c",
   "metadata": {},
   "source": [
    "## Demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d055a48",
   "metadata": {},
   "source": [
    "### Async chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf293d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2 + 2 = 4\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-12621e18-5717-4459-a075-f23e188eac0a`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-12621e18-5717-4459-a075-f23e188eac0a', created=1756911527, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='2 + 2 = 4', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = AsyncChat(model=\"claude-sonnet-4-20250514\")\n",
    "await chat(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43c346c4",
   "metadata": {},
   "source": [
    "### Async chat w tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f37702",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def async_add(a: int, b: int) -> int:\n",
    "    \"Add two numbers asynchronously\"\n",
    "    print('>>> async add is being called!')\n",
    "    await asyncio.sleep(0.1)  # Simulate async work\n",
    "    return a + b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00399522",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> async add is being called!\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The result of 5 + 7 is 12.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-9d86d993-9dde-4802-9c91-7880870c8835`\n",
       "- model: `claude-sonnet-4-20250514`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=17, prompt_tokens=528, total_tokens=545, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-9d86d993-9dde-4802-9c91-7880870c8835', created=1756911531, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The result of 5 + 7 is 12.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=17, prompt_tokens=528, total_tokens=545, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_with_tools = AsyncChat(model=\"claude-sonnet-4-20250514\", tools=[async_add])\n",
    "await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac5495",
   "metadata": {},
   "source": [
    "### Streaming Async Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18cc48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = AsyncChat(model=\"claude-sonnet-4-20250514\")\n",
    "stream_gen = await chat(\"Count to 50\", stream=True)\n",
    "\n",
    "async for chunk in stream_gen:\n",
    "    if isinstance(chunk, ModelResponse): display(chunk)\n",
    "    else: print(delta_text(chunk) or '',end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af541c2",
   "metadata": {},
   "source": [
    "### Streaming Async Chat w tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76751bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_stream_tools = AsyncChat(model=\"claude-sonnet-4-20250514\", tools=[async_add])\n",
    "stream_gen = await chat_stream_tools(\"What's 15 + 23? Use the tool and then explain the result.\", stream=True)\n",
    "\n",
    "async for chunk in stream_gen:\n",
    "    if isinstance(chunk, ModelResponse): display(chunk)\n",
    "    else: print(delta_text(chunk) or '', end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29144365",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
