{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40ea620d",
   "metadata": {},
   "source": [
    "# cachy\n",
    "\n",
    "> Cache your LLM requests and make your notebooks fast again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dbaf2b",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fddaca9",
   "metadata": {},
   "source": [
    "`cachy` caches your litellm api requests. It does this by saving the results of each llm call to a local `cachy.txt` file. Before calling an LLM api (e.g. Anthropic) it will check if the request exists in `cachy.txt`. If it does it will return the cached result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acdd6f69",
   "metadata": {},
   "source": [
    "Under the hood LiteLLM uses `httpx.Client` and `httpx.AsyncClient` to call an LLM API. `cachy` patches the `send` method of both clients and injects a simple caching mechanism:\n",
    "\n",
    "- create a cache key from the request\n",
    "- if the key exists in `cachy.txt` return the cached response\n",
    "- if not call the LLM API and save the response to `cachy.txt`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61997871",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp cachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb638e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import hashlib, httpx, json\n",
    "from fastcore.utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d87ca43",
   "metadata": {},
   "source": [
    "We want to restrict our caching to LLM API calls only. A simple way to do this is to check the request url. By default we cache OpenAI, Anthropic, Gemini and DeepSeek API calls."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac388b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "doms = ['api.openai.com', 'api.anthropic.com', 'generativelanguage.googleapis.com', 'api.deepseek.com'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a12c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _should_cache(url, doms): return any(dom in str(url) for dom in doms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e063049",
   "metadata": {},
   "source": [
    "`cachy.txt` contains 1 llm request/response per line. \n",
    "\n",
    "Each line has the following syntax `key|response` where `key` is a hash of the llm request. \n",
    "\n",
    "```txt\n",
    "416f3b8c|{\"id\":\"chatcmpl-C74Ljo3pEBjR5tvlghY3RtHLFBcMQ\",\"object\":\"chat.completion\",\"created\":1755801051,\"model\":\"gpt-4o-2024-08-06\",\"choices\":[{\"index\":0,\"message\":{\"role\":\"assistant\",\"content\":\"Hello! How can I assist you today?\",\"refusal\":null,\"annotations\":[]},\"logprobs\":null,\"finish_reason\":\"stop\"}],\"usage\":{\"prompt_tokens\":9,\"completion_tokens\":9,\"total_tokens\":18,\"prompt_tokens_details\":{\"cached_tokens\":0,\"audio_tokens\":0},\"completion_tokens_details\":{\"reasoning_tokens\":0,\"audio_tokens\":0,\"accepted_prediction_tokens\":0,\"rejected_prediction_tokens\":0}},\"service_tier\":\"default\",\"system_fingerprint\":\"fp_80956533cb\"}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f92f28fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _cache(key, cfp):\n",
    "    with open(cfp, 'r') as f:\n",
    "        line = first(f, lambda x: x.startswith(key))\n",
    "        return last(line.strip().split('|',1)) if line else None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426c4f45",
   "metadata": {},
   "source": [
    "An LLM response can include the `|` char so that's why we use `.split('|', 1)` to ensure that we don't accidentally split the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cace0b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _write_cache(key, content, cfp):\n",
    "    with open(cfp, 'a') as f: f.write(f\"{key}|{str(content)}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50269cee",
   "metadata": {},
   "source": [
    "We patch `Client.send`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c0d896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_sync_patch(cfp, doms):    \n",
    "    @patch\n",
    "    def send(self:httpx._client.Client, r, **kwargs):\n",
    "        is_stream = kwargs.get('stream')\n",
    "        if not _should_cache(r.url, doms): return self._orig_send(r, **kwargs)\n",
    "        key = hashlib.sha256((str(r.url)+r.content.decode()+str(is_stream)).encode()).hexdigest()[:8]\n",
    "        if res := _cache(key, cfp): \n",
    "            return httpx.Response(status_code=200, content=json.loads(res) if is_stream else res, request=r)\n",
    "        res = self._orig_send(r, **kwargs)\n",
    "        if is_stream: content = b''.join(list(res.iter_bytes())).decode()\n",
    "        else: content = json.dumps(json.loads(res.read().decode()), separators=(',',':'))\n",
    "        _write_cache(key, json.dumps(content) if is_stream else content , cfp)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec379ee",
   "metadata": {},
   "source": [
    "We patch `AsyncClient.send`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c93b439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _apply_async_patch(cfp, doms):    \n",
    "    @patch\n",
    "    async def send(self:httpx._client.AsyncClient, r, **kwargs):\n",
    "        is_stream = kwargs.get('stream')\n",
    "        if not _should_cache(r.url, doms): return await self._orig_send(r, **kwargs)\n",
    "        key = hashlib.sha256((str(r.url)+r.content.decode()+str(is_stream)).encode()).hexdigest()[:8]\n",
    "        if res := _cache(key, cfp): \n",
    "            return httpx.Response(status_code=200, content=json.loads(res) if is_stream else res, request=r)\n",
    "        res = await self._orig_send(r, **kwargs)\n",
    "        if is_stream: content = b''.join([c async for c in res.aiter_bytes()]).decode()\n",
    "        else: content = json.dumps(json.loads(res.read().decode()), separators=(',',':'))\n",
    "        _write_cache(key, json.dumps(content) if is_stream else content , cfp)\n",
    "        return httpx.Response(status_code=res.status_code, content=content, request=r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722f07fd",
   "metadata": {},
   "source": [
    "Now let's define a method that makes it easy for the user to enable caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35081f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def enable_cachy(cache_dir=None, doms=doms):\n",
    "    cfp = Path(cache_dir or Config.find('settings.ini').config_path or '.') / 'cachy.txt'\n",
    "    cfp.touch(exist_ok=True)   \n",
    "    _apply_sync_patch(cfp, doms)\n",
    "    _apply_async_patch(cfp, doms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10ff62e9",
   "metadata": {},
   "source": [
    "## Tests "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75383e5a",
   "metadata": {},
   "source": [
    "Let's test `enable_req_cache` on the scenarios below for each provider:\n",
    "\n",
    "- sync\n",
    "- sync (with streaming)\n",
    "- async\n",
    "- async (with streaming)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ceda279",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971c76a7",
   "metadata": {},
   "source": [
    "### Sync Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a2e49c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4623f6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mods: ant=\"claude-sonnet-4-20250514\"; oai=\"gpt-4o\"; gem=\"gemini/gemini-2.0-flash\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49c0a761",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mk_msgs(m): return [{\"role\": \"user\", \"content\": f\"write 1 words about {m}\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0760cde2",
   "metadata": {},
   "source": [
    "Let's define a helper method to display a streamed response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73fedfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _stream(r): \n",
    "    for ch in r: print(ch.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1b70d0b",
   "metadata": {},
   "source": [
    "#### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8268e151",
   "metadata": {},
   "source": [
    "Let's test `claude-sonnet-x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052883c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-5c241db5-4bfc-4751-9b97-47a8c966227e', created=1756123821, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Coordination.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=6, prompt_tokens=16, total_tokens=22, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"ant sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0611cd95",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-08800b92-edf0-4c11-8f9e-9b275077536a', created=1756123823, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Coordination.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=6, prompt_tokens=16, total_tokens=22, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"ant sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0620bb4b",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "835a619d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "Asynchronous**\n",
      "\n",
      "(Ant sync\n",
      " stream likely refers to asynchronous streaming - handling\n",
      " data flows that don't require synchronized,\n",
      " real-time processing between sender and receiver.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"ant sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172a66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**\n",
      "Asynchronous**\n",
      "\n",
      "(Ant sync\n",
      " stream likely refers to asynchronous streaming - handling\n",
      " data flows that don't require synchronized,\n",
      " real-time processing between sender and receiver.)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"ant sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234e25e8",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3465f310",
   "metadata": {},
   "source": [
    "Let's test `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b65f57f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-C8QJx7gpuXLgyh7bndGwsPtN1Ny4a', created=1756123837, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_df0f7b956c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Synchronization', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=1, prompt_tokens=16, total_tokens=17, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"oai sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cbbb10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-C8QJx7gpuXLgyh7bndGwsPtN1Ny4a', created=1756123837, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_df0f7b956c', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Synchronization', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=1, prompt_tokens=16, total_tokens=17, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"oai sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ba3bea",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7365d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff\n",
      "icient\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"oai sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f76115c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff\n",
      "icient\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.oai, messages=mk_msgs(\"oai sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da65e298",
   "metadata": {},
   "source": [
    "#### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db85a3ac",
   "metadata": {},
   "source": [
    "Let's test `2.0-flash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f198de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='xVKsaI-SF6qxxN8P58LQyAI', created=1756123845, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Synchronization.\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=3, prompt_tokens=8, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=8, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"gem sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156cbb3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='xVKsaI-SF6qxxN8P58LQyAI', created=1756123846, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Synchronization.\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=3, prompt_tokens=8, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=8, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"gem sync...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbfcba2",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abf359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff\n",
      "ortless.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"gem sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c257f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eff\n",
      "ortless.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = completion(model=mods.gem, messages=mk_msgs(\"gem sync stream...\"), stream=True)\n",
    "_stream(r)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2158c3bf",
   "metadata": {},
   "source": [
    "### Async Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "964e5669",
   "metadata": {},
   "outputs": [],
   "source": [
    "from litellm import acompletion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4122d050",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def _astream(r):\n",
    "    async for chunk in r: print(chunk.choices[0].delta.content or \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02ae4c9",
   "metadata": {},
   "source": [
    "#### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8996f02",
   "metadata": {},
   "source": [
    "Let's test `claude-sonnet-x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24a1501e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-c1e613e2-8f3d-4060-8224-7f59eff9b56e', created=1756123859, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Concurrency.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=7, prompt_tokens=16, total_tokens=23, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"ant async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9b6420",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-489b76c3-27c7-46a0-a88d-8acf9f3fbed4', created=1756123860, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Concurrency.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=7, prompt_tokens=16, total_tokens=23, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"ant async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a65bb813",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b461401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As\n",
      "ynchronous streams enable non-blocking iteration\n",
      " over data sequences that arrive over time, allowing\n",
      " efficient processing of real-time events\n",
      ", API responses, or file chunks without\n",
      " freezing the main thread.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"ant async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdaa5205",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "As\n",
      "ynchronous streams enable non-blocking iteration\n",
      " over data sequences that arrive over time, allowing\n",
      " efficient processing of real-time events\n",
      ", API responses, or file chunks without\n",
      " freezing the main thread.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.ant, messages=mk_msgs(\"ant async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1211d3f",
   "metadata": {},
   "source": [
    "#### OpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "488b6fd4",
   "metadata": {},
   "source": [
    "Let's test `gpt-4o`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ac74e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-C8QKVEoMm2fEvWkbDlE43jRVKosVV', created=1756123871, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_80956533cb', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficiency.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=2, prompt_tokens=16, total_tokens=18, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"oai async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e384bff8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-C8QKVEoMm2fEvWkbDlE43jRVKosVV', created=1756123871, model='gpt-4o-2024-08-06', object='chat.completion', system_fingerprint='fp_80956533cb', choices=[Choices(finish_reason='stop', index=0, message=Message(content='Efficiency.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]), provider_specific_fields={})], usage=Usage(completion_tokens=2, prompt_tokens=16, total_tokens=18, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None)), service_tier='default')"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"oai async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd1983c",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5830f2ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Eff\n",
      "icient\n",
      "\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"oai async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73034e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\n",
      "Eff\n",
      "icient\n",
      "\"\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.oai, messages=mk_msgs(\"oai async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b753620c",
   "metadata": {},
   "source": [
    "#### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbdadb46",
   "metadata": {},
   "source": [
    "Let's test `2.0-flash`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee834224",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='5VKsaM37OrqzxN8PyPrC0Ak', created=1756123877, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Concurrency.\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=3, prompt_tokens=8, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=8, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"gem async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3d8394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='5VKsaM37OrqzxN8PyPrC0Ak', created=1756123879, model='gemini-2.0-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Concurrency.\\n', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=3, prompt_tokens=8, total_tokens=11, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=8, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"gem async...\"))\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066244ec",
   "metadata": {},
   "source": [
    "Now, with streaming enabled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7127e7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concurrent\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"gem async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72edbfe1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concurrent\n",
      ".\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "r = await acompletion(model=mods.gem, messages=mk_msgs(\"gem async stream...\"), stream=True)\n",
    "await(_astream(r))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e207f4a",
   "metadata": {},
   "source": [
    "### Tool Calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87e7b91c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"get_current_weather\",\n",
    "            \"description\": \"Get the current weather in a given location\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"location\": {\"type\":\"string\", \"description\":\"The city e.g. Reims\"},\n",
    "                    \"unit\": {\"type\":\"string\", \"enum\":[\"celsius\", \"fahrenheit\"]},\n",
    "                },\n",
    "                \"required\": [\"location\"],\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c001aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-39afdd40-b7f1-4dfc-9093-17282b055d5f', created=1756124252, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Reims\"}', name='get_current_weather'), id='toolu_01Bx7sqvFrX71pcEYVQtA7PJ', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=57, prompt_tokens=427, total_tokens=484, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f77d1d0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-44db0e0c-9a39-444b-865b-fde6b2e1afb8', created=1756124246, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=0, function=Function(arguments='{\"location\": \"Reims\"}', name='get_current_weather'), id='toolu_01Bx7sqvFrX71pcEYVQtA7PJ', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=57, prompt_tokens=427, total_tokens=484, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r = completion(model=mods.ant, messages=mk_msgs(\"Is it raining in Reims?\"), tools=tools)\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc96c4f",
   "metadata": {},
   "source": [
    "### Citations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca43cc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-7edbf05a-6e23-4cca-a722-e758ecb9d6c4', created=1756124350, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Here's 1 word about searching the web: Informative.\\n\\nAnd briefly about otters: Otters are carnivorous mammals in the subfamily Lutrinae. The 14 extant otter species are all semiaquatic, both freshwater and marine. Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water. They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': [[{'type': 'web_search_result_location', 'cited_text': 'Otters are carnivorous mammals in the subfamily Lutrinae. The 14 extant otter species are all semiaquatic, both freshwater and marine. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'EpEBCioIBhgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDEe6zaskgiVKA22eshoMzyTMwQc9gc377/eIIjAZ/PX6BmA5pQJZxwfWu5JRNHKklQl4NC2bsh1D+7YbWpBxw8/tHNL268TumbZqmPwqFXGhSf+8YkEEaY3gvgGn5xvq/Zgg1RgE'}], [{'type': 'web_search_result_location', 'cited_text': 'Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in wate...', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBhgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDF8ErVgbfgq9Aa35xxoMZ2hozMtaVAcXFu2MIjDKN5iAXF7Om6svxf04+l6wUJFc/Ce6ZlUIFxf5XB+pL/jzUlNs76XjZnITPu5YrD8qEyZ09+i1hb3x52qKHdOitLqcglcYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBhgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDAMI4rSXW25SjXcWYhoMrsWRYzrB3tibs4HNIjCwidhfj+8uIHN3CBSmF94xMGwbLXDiNQ4PvKjFsHiJaEC87bBLxg8wbV55Nw/OdtUqE8bUayg9QNPujqonQZ8c2+1wo5EYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch. ', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'EpABCioIBhgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDAmPTPXAAHqjdXFqQxoM/d6JPWhpGEmpYsVKIjDpa52LtiIp9hA8eh5A/xIrYER2l/iPmm4TNmZQvh/cnhjXIa4me0OBPXIH0PT9lZMqFFL0xfiWUboZg4/8ibL6EVuUYXmUGAQ='}]], 'thinking_blocks': None}))], usage=Usage(completion_tokens=268, prompt_tokens=13660, total_tokens=13928, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool = {\"type\":\"web_search_20250305\", \"name\":\"web_search\", \"max_uses\":3}\n",
    "r = completion(\n",
    "    \"claude-sonnet-4-20250514\", tools=[search_tool],\n",
    "    messages=mk_msgs(\"Search the web and tell me very briefly about otters\"), \n",
    ")\n",
    "r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f930bdc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ModelResponse(id='chatcmpl-33b68f32-ebc7-4fa5-9251-5c93e909d8d1', created=1756124362, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Here's 1 word about searching the web: Informative.\\n\\nAnd briefly about otters: Otters are carnivorous mammals in the subfamily Lutrinae. The 14 extant otter species are all semiaquatic, both freshwater and marine. Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water. They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': [[{'type': 'web_search_result_location', 'cited_text': 'Otters are carnivorous mammals in the subfamily Lutrinae. The 14 extant otter species are all semiaquatic, both freshwater and marine. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'EpEBCioIBhgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDEe6zaskgiVKA22eshoMzyTMwQc9gc377/eIIjAZ/PX6BmA5pQJZxwfWu5JRNHKklQl4NC2bsh1D+7YbWpBxw8/tHNL268TumbZqmPwqFXGhSf+8YkEEaY3gvgGn5xvq/Zgg1RgE'}], [{'type': 'web_search_result_location', 'cited_text': 'Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in wate...', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBhgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDF8ErVgbfgq9Aa35xxoMZ2hozMtaVAcXFu2MIjDKN5iAXF7Om6svxf04+l6wUJFc/Ce6ZlUIFxf5XB+pL/jzUlNs76XjZnITPu5YrD8qEyZ09+i1hb3x52qKHdOitLqcglcYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. ', 'url': 'https://en.wikipedia.org/wiki/Otter', 'title': 'Otter - Wikipedia', 'encrypted_index': 'Eo8BCioIBhgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDAMI4rSXW25SjXcWYhoMrsWRYzrB3tibs4HNIjCwidhfj+8uIHN3CBSmF94xMGwbLXDiNQ4PvKjFsHiJaEC87bBLxg8wbV55Nw/OdtUqE8bUayg9QNPujqonQZ8c2+1wo5EYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'Sea otters have the densest fur of any animal on earth with an estimated 1 million hairs per square inch. ', 'url': 'https://oceana.ca/en/blog/10-amazing-facts-about-sea-otters/', 'title': '10 Amazing Facts about Sea otters! Learn more - Oceana Canada', 'encrypted_index': 'EpABCioIBhgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDAmPTPXAAHqjdXFqQxoM/d6JPWhpGEmpYsVKIjDpa52LtiIp9hA8eh5A/xIrYER2l/iPmm4TNmZQvh/cnhjXIa4me0OBPXIH0PT9lZMqFFL0xfiWUboZg4/8ibL6EVuUYXmUGAQ='}]], 'thinking_blocks': None}))], usage=Usage(completion_tokens=268, prompt_tokens=13660, total_tokens=13928, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search_tool = {\"type\":\"web_search_20250305\", \"name\":\"web_search\", \"max_uses\":3}\n",
    "r = completion(\n",
    "    \"claude-sonnet-4-20250514\", tools=[search_tool],\n",
    "    messages=mk_msgs(\"Search the web and tell me very briefly about otters\"), \n",
    ")\n",
    "r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da72f65",
   "metadata": {},
   "source": [
    "## Export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a694617e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
