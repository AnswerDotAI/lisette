{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd3dbfa",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "> Lisette usage and cost monitoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6856c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from litellm.integrations.custom_logger import CustomLogger\n",
    "import time\n",
    "try: \n",
    "    from fastlite import *\n",
    "    from fastlite.core import dataclass\n",
    "except ImportError: raise ImportError(\"Please install `fastlite` to use sqlite based lisette usage logging.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743eedcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm, importlib, httpx\n",
    "from lisette.core import Chat, AsyncChat, patch_litellm\n",
    "from fastcore.all import *\n",
    "from cachy import enable_cachy,disable_cachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69144ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff8990",
   "metadata": {},
   "source": [
    "## Lisette Usage Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acabfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(litellm); # to re-run the notebook without kernel restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85329c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# litellm._turn_on_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_litellm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed71558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Usage: id:int; timestamp:float; model:str; user_id:str; prompt_tokens:int; completion_tokens:int; total_tokens:int; cached_tokens:int; cache_creation_tokens:int; cache_read_tokens:int; web_search_requests:int; response_cost:int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf5fc1",
   "metadata": {},
   "source": [
    "The precomputed response cost provided is available in `kwargs['response_cost']` according to the [litellm docs](https://docs.litellm.ai/docs/observability/custom_callback#whats-available-in-kwargs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LisetteUsageLogger(CustomLogger):\n",
    "    def __init__(self, db_path): \n",
    "        self.db = Database(db_path)\n",
    "        self.usage = self.db.create(Usage)\n",
    "    \n",
    "    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time): self._log_usage(response_obj, kwargs['response_cost'], start_time, end_time)\n",
    "    def log_success_event(self, kwargs, response_obj, start_time, end_time):             self._log_usage(response_obj, kwargs['response_cost'], start_time, end_time)\n",
    "    def _log_usage(self, response_obj, response_cost, start_time, end_time):\n",
    "        usage = response_obj.usage\n",
    "        ptd   = usage.prompt_tokens_details\n",
    "        self.usage.insert(Usage(timestamp=time.time(), \n",
    "                                model=response_obj.model, \n",
    "                                user_id=self.user_id_fn(), \n",
    "                                prompt_tokens=usage.prompt_tokens, \n",
    "                                completion_tokens=usage.completion_tokens,\n",
    "                                total_tokens=usage.total_tokens, \n",
    "                                cached_tokens=ptd.cached_tokens if ptd else 0, # used by gemini (read tokens)\n",
    "                                cache_creation_tokens=nested_idx(usage, 'cache_creation_input_tokens'),\n",
    "                                cache_read_tokens=nested_idx(usage, 'cache_read_input_tokens'), # used by anthropic \n",
    "                                web_search_requests=nested_idx(usage, 'server_tool_use', 'web_search_requests'),\n",
    "                                response_cost=response_cost))\n",
    "                  \n",
    "    def user_id_fn(self): raise NotImplementedError('Please implement `LisetteUsageLogger.user_id_fn` before initializing, e.g using fastcore.patch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdfd5ca",
   "metadata": {},
   "source": [
    "## Cost Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce652ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixDict(dict):\n",
    "    def __getitem__(self, key):\n",
    "        if key in self.keys(): return super().__getitem__(key)\n",
    "        for k in self.keys(): \n",
    "            if key.startswith(k): return super().__getitem__(k)\n",
    "        raise KeyError(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847758d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prices = PrefixDict({\n",
    "    'claude-sonnet-4-5': dict(input_prc = 3/1e6, cache_write_prc = 3.75/1e6, cache_read_prc = 0.3/1e6, output_prc = 15/1e6, web_search_prc = 10/1e3)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be909f",
   "metadata": {},
   "source": [
    "Simplified cost utils to demonstrate total cost calculation (use `Usage.response_cost` in prod):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(as_prop=True)\n",
    "def inp_cost(self:Usage):         return model_prices[self.model]['input_prc'] * (self.prompt_tokens - self.cache_read_tokens)\n",
    "@patch(as_prop=True)\n",
    "def cache_write_cost(self:Usage): return model_prices[self.model]['cache_write_prc'] * self.cache_creation_tokens\n",
    "@patch(as_prop=True)\n",
    "def cache_read_cost(self:Usage):  return model_prices[self.model]['cache_read_prc'] * self.cache_read_tokens\n",
    "@patch(as_prop=True)\n",
    "def out_cost(self:Usage):         return model_prices[self.model]['output_prc'] * self.completion_tokens\n",
    "@patch(as_prop=True)\n",
    "def web_cost(self:Usage):         return model_prices[self.model]['web_search_prc'] * ifnone(self.web_search_requests, 0)\n",
    "@patch(as_prop=True)\n",
    "def cost(self:Usage):             return self.inp_cost + self.cache_write_cost + self.cache_read_cost + self.out_cost + self.web_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ef6d0",
   "metadata": {},
   "source": [
    "A mapping of model pricing is also available in litellm, which is used to calculate the `response_cost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90af6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pricing = dict2obj(httpx.get(litellm.model_cost_map_url).json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc0ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_pricing['claude-sonnet-4-5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_pricing['gemini-3-pro-preview']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2987b",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfp = Path('.lisette/litellm-usage.db')\n",
    "dbfp.parent.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def user_id_fn(self:LisetteUsageLogger): return 'user-123'\n",
    "logger = LisetteUsageLogger(dbfp)\n",
    "litellm.callbacks = [logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slc = ','.join('id model user_id prompt_tokens completion_tokens total_tokens cached_tokens cache_creation_tokens cache_read_tokens web_search_requests response_cost'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# litellm.set_verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0af81a",
   "metadata": {},
   "source": [
    "A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9215558",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat('claude-sonnet-4-5-20250929')\n",
    "r = chat(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b82ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=1, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=14, completion_tokens=11, total_tokens=25, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=None, response_cost=0.000207)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3) # wait for callback db write\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88fe4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Usage(id=1, timestamp=1764082053.107739, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=14, completion_tokens=11, total_tokens=25, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=None, response_cost=0.000207)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39087125",
   "metadata": {},
   "source": [
    "Our calculated cost matches litellm's `response_cost`. In some cases it might be better to use the custom calculation as we'll see in the remaining of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367cb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55735017",
   "metadata": {},
   "source": [
    "Now, let's test with streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat('claude-sonnet-4-5')\n",
    "res = chat(\"Count from 1 to 5\", stream=True)\n",
    "for o in res: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8eb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=2, timestamp=UNSET, model='claude-sonnet-4-5', user_id='user-123', prompt_tokens=15, completion_tokens=17, total_tokens=32, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=None, response_cost=0.00030000000000000003)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fadb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9bb4b",
   "metadata": {},
   "source": [
    "Streaming logged successfully. Let's also verify async chat calls are logged properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270a8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "3 + 3 = 6\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='3 + 3 = 6', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_async = AsyncChat('claude-sonnet-4-5-20250929')\n",
    "await chat_async(\"What is 3+3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a75d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=2, timestamp=UNSET, model='claude-sonnet-4-5', user_id='user-123', prompt_tokens=15, completion_tokens=17, total_tokens=32, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=None, response_cost=0.00030000000000000003)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82d440",
   "metadata": {},
   "source": [
    "Finally, let's test async streaming to ensure all API patterns are covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7791bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='10, 11, 12, 13, 14, 15', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=20, prompt_tokens=38, total_tokens=58, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "res = await chat_async(\"Count from 10 to 15\", stream=True)\n",
    "async for o in res: pass\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=4, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=38, completion_tokens=20, total_tokens=58, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=None, response_cost=0.00041400000000000003)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60ec86",
   "metadata": {},
   "source": [
    "Now let's run a prompt with web search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d3c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the current weather information for New York City today (November 24, 2025):\n",
       "\n",
       "Today's weather is sunny with a high of 54°F and northwest winds at 5 to 10 mph. Tonight will see partly cloudy skies in the evening, then becoming cloudy overnight with a low of 43°F.\n",
       "\n",
       "The air quality has reached a high level of pollution and is unhealthy for sensitive groups, so those with respiratory sensitivities may want to limit outdoor activities.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=226, prompt_tokens=9451, total_tokens=9677, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content=\"Based on the current weather information for New York City today (November 24, 2025):\\n\\nToday's weather is sunny with a high of 54°F and northwest winds at 5 to 10 mph. Tonight will see partly cloudy skies in the evening, then becoming cloudy overnight with a low of 43°F.\\n\\nThe air quality has reached a high level of pollution and is unhealthy for sensitive groups, so those with respiratory sensitivities may want to limit outdoor activities.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': [[{'type': 'web_search_result_location', 'cited_text': 'zoom out · Showing Stations · Hourly Forecast for Today, Monday 11/24Hourly for Today, Mon 11/24 · Tomorrow 11/24 · 3% / 0 in · Sunny. High 54F. Winds...', 'url': 'https://www.wunderground.com/hourly/us/ny/new-york-city', 'title': 'New York City, NY Hourly Weather Forecast | Weather Underground', 'encrypted_index': 'EpIBCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDDkEpHmFTHuW0U/IYxoM8ROTXR76Q/b7Im6UIjC+3YVJV/vSBuigz3wzwRGKdxTd6QbFpOH/xRlOAYN6KZlaLa940WL+mdvzOBFhLc0qFm+RKPiz05m+wVTolVoJKvji2ulhXawYBA==', 'supported_text': \"Today's weather is sunny with a high of 54°F and northwest winds at 5 to 10 mph\"}], [{'type': 'web_search_result_location', 'cited_text': 'Tomorrow 11/24 · 6 % / 0 in · Partly cloudy skies in the evening, then becoming cloudy overnight. Low 43F. ', 'url': 'https://www.wunderground.com/hourly/us/ny/new-york-city', 'title': 'New York City, NY Hourly Weather Forecast | Weather Underground', 'encrypted_index': 'EpMBCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDByYL7N7INDetqtiTRoMLi+LgBIHvGWlAI/iIjCYecmI6ZeKPgBkXHJBOg8V69VmSr0S9Yq4DUvWa2EMWtM5hYkmXrsiaRJjHmOr6jMqFzQlLxyYR3Qn0ive2naRkWc4DQFtL8KMGAQ=', 'supported_text': 'Tonight will see partly cloudy skies in the evening, then becoming cloudy overnight with a low of 43°F'}], [{'type': 'web_search_result_location', 'cited_text': 'The air has reached a high level of pollution and is unhealthy for sensitive groups.', 'url': 'https://www.accuweather.com/en/us/new-york/10021/weather-forecast/14-349727_1_al', 'title': 'New York City, NY Weather Forecast | AccuWeather', 'encrypted_index': 'Eo8BCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDOGJiVeKdf0A5IROTBoMJAyx9KBUizhootd6IjBS24GPde2euGdSY4gHiFvoQ7UTJ1mzZDyTi+wqhqeFvoRDhB31prBSVtdjQJLH/q8qExTLIRISG/CcClj31VIBO8xsBSAYBA==', 'supported_text': 'The air quality has reached a high level of pollution and is unhealthy for sensitive groups'}]], 'thinking_blocks': None}))], usage=Usage(completion_tokens=226, prompt_tokens=9451, total_tokens=9677, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat('claude-sonnet-4-5-20250929')\n",
    "chat(\"What is the weather like in NYC? Search web.\", search=\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86f247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=5, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=9451, completion_tokens=226, total_tokens=9677, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=1, response_cost=0.031743)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d58e3",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "Litellm's `response_cost` doesn't take search request cost into account!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a35480",
   "metadata": {},
   "source": [
    "Now, this is a case where using the custom calculations is better as it will also include the web search request cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost + model_prices[u.model]['web_search_prc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8929bae",
   "metadata": {},
   "source": [
    "Web search with streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat('claude-sonnet-4-5')\n",
    "res = chat(\"What is the weather like in NYC? Search web.\", search=\"m\", stream=True)\n",
    "for o in res: pass\n",
    "# print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0c219",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "Web search requests are not included in usage when `stream=True`. Here is an open [Issue](https://github.com/BerriAI/litellm/issues/16631)\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf0c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=6, timestamp=UNSET, model='claude-sonnet-4-5', user_id='user-123', prompt_tokens=9451, completion_tokens=226, total_tokens=9677, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=None, response_cost=0.031743)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce4d9b",
   "metadata": {},
   "source": [
    "Once this [PR](https://github.com/BerriAI/litellm/pull/16826) is merged `web_search_requests` will be included with `stream=True`, and the following test should pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f51ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_eq(u.cost, u.response_cost + model_prices[u.model]['web_search_prc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13257134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# u.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(logger.usage()), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126e3064",
   "metadata": {},
   "source": [
    "Let's implement a utility to get the total cost including web search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def total_cost(self:Usage, sc=0.01): return self.response_cost + sc * ifnone(self.web_search_requests, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_close((L(logger.usage()).map(lambda o:o.total_cost(sc=0.01)).sum()), 0.086, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e57bed",
   "metadata": {},
   "source": [
    "A simple Gemini example (requires min tokens and running twice to see `cached_tokens`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad22437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2 + 2 = 4\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=41, prompt_tokens=7010, total_tokens=7051, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=34, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=3058, text_tokens=3952, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='2 + 2 = 4', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=41, prompt_tokens=7010, total_tokens=7051, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=34, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=3058, text_tokens=3952, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat('gemini/gemini-2.5-flash')\n",
    "chat(\"What is 2+2?\"* 500)\n",
    "time.sleep(5)\n",
    "chat(\"What is 2+2?\"* 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0621b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(0.3) # wait for callback db write\n",
    "u = logger.usage(select=slc)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(logger.usage()), 8)\n",
    "test_eq(logger.usage()[-1].cached_tokens > 3000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3577f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfp.parent.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e335da",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620e45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python",
   "language": "python",
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
