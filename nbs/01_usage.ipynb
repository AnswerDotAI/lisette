{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd3dbfa",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "> Lisette usage and cost monitoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6856c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from litellm.integrations.custom_logger import CustomLogger\n",
    "from fastcore.utils import *\n",
    "import time\n",
    "try: from fastlite import *\n",
    "except ImportError: raise ImportError(\"Please install `fastlite` to use sqlite based lisette usage logging.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743eedcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm, importlib, httpx\n",
    "from lisette.core import Chat, AsyncChat, patch_litellm\n",
    "from cachy import enable_cachy, disable_cachy\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69144ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff8990",
   "metadata": {},
   "source": [
    "## Lisette Usage Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acabfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = importlib.reload(litellm) # to re-run the notebook without kernel restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f85329c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# litellm._turn_on_debug()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_litellm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed71558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Usage: id:int; timestamp:float; model:str; user_id:str; prompt_tokens:int; completion_tokens:int; total_tokens:int; cached_tokens:int; cache_creation_tokens:int; cache_read_tokens:int; web_search_requests:int; response_cost:int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc4301de",
   "metadata": {},
   "source": [
    "Anthropic provides web search request counts directly via `usage.server_tool_use.web_search_requests`, billed at $10 per 1,000 searches ([pricing](https://docs.claude.com/en/docs/about-claude/pricing)). Gemini returns queries in `groundingMetadata.webSearchQueries`—each query counts as a separate billable use—with 5,000 free prompts per month, then $14 per 1,000 search queries (coming soon) ([pricing](https://ai.google.dev/gemini-api/docs/pricing), [grounding docs](https://ai.google.dev/gemini-api/docs/google-search))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323da97",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def search_count(r):\n",
    "    if cnt := nested_idx(r.usage, 'server_tool_use', 'web_search_requests'): return cnt # Anthropic\n",
    "    if meta := getattr(r, 'vertex_ai_grounding_metadata', None): # Gemini\n",
    "        if meta and (queries := meta[0].get('webSearchQueries')): return len(queries)\n",
    "    if cnt := nested_idx(r.usage, 'prompt_tokens_details', 'web_search_requests'): return cnt # streaming with `include_usage`\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf5fc1",
   "metadata": {},
   "source": [
    "The precomputed response cost provided is available in `kwargs['response_cost']` according to the [litellm docs](https://docs.litellm.ai/docs/observability/custom_callback#whats-available-in-kwargs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LisetteUsageLogger(CustomLogger):\n",
    "    def __init__(self, db_path): \n",
    "        self.db = Database(db_path)\n",
    "        self.usage = self.db.create(Usage)\n",
    "    \n",
    "    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time): self._log_usage(response_obj, kwargs['response_cost'], start_time, end_time)\n",
    "    def log_success_event(self, kwargs, response_obj, start_time, end_time):             self._log_usage(response_obj, kwargs['response_cost'], start_time, end_time)\n",
    "    def _log_usage(self, response_obj, response_cost, start_time, end_time):\n",
    "        usage = response_obj.usage\n",
    "        ptd   = usage.prompt_tokens_details\n",
    "        self.usage.insert(Usage(timestamp=time.time(),\n",
    "                                model=response_obj.model,\n",
    "                                user_id=self.user_id_fn(),\n",
    "                                prompt_tokens=usage.prompt_tokens,\n",
    "                                completion_tokens=usage.completion_tokens,\n",
    "                                total_tokens=usage.total_tokens,\n",
    "                                cached_tokens=ptd.cached_tokens if ptd else 0, # used by gemini (read tokens)\n",
    "                                cache_creation_tokens=nested_idx(usage, 'cache_creation_input_tokens'),\n",
    "                                cache_read_tokens=nested_idx(usage, 'cache_read_input_tokens'), # used by anthropic\n",
    "                                web_search_requests=search_count(response_obj),\n",
    "                                response_cost=response_cost))\n",
    "                  \n",
    "    def user_id_fn(self): raise NotImplementedError('Please implement `LisetteUsageLogger.user_id_fn` before initializing, e.g using fastcore.patch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdfd5ca",
   "metadata": {},
   "source": [
    "## Cost Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce652ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixDict(dict):\n",
    "    def __getitem__(self, key):\n",
    "        if key in self.keys(): return super().__getitem__(key)\n",
    "        for k in self.keys(): \n",
    "            if key.startswith(k): return super().__getitem__(k)\n",
    "        raise KeyError(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847758d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prices = PrefixDict({\n",
    "    'claude-sonnet-4-5': dict(input_prc = 3/1e6, cache_write_prc = 3.75/1e6, cache_read_prc = 0.3/1e6, output_prc = 15/1e6, web_search_prc = 10/1e3)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be909f",
   "metadata": {},
   "source": [
    "Simplified cost utils to demonstrate total cost calculation (use `Usage.response_cost` in prod):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(as_prop=True)\n",
    "def inp_cost(self:Usage):         return model_prices[self.model]['input_prc'] * (self.prompt_tokens - self.cache_read_tokens)\n",
    "@patch(as_prop=True)\n",
    "def cache_write_cost(self:Usage): return model_prices[self.model]['cache_write_prc'] * self.cache_creation_tokens\n",
    "@patch(as_prop=True)\n",
    "def cache_read_cost(self:Usage):  return model_prices[self.model]['cache_read_prc'] * self.cache_read_tokens\n",
    "@patch(as_prop=True)\n",
    "def out_cost(self:Usage):         return model_prices[self.model]['output_prc'] * self.completion_tokens\n",
    "@patch(as_prop=True)\n",
    "def web_cost(self:Usage):         return model_prices[self.model]['web_search_prc'] * ifnone(self.web_search_requests, 0)\n",
    "@patch(as_prop=True)\n",
    "def cost(self:Usage):             return self.inp_cost + self.cache_write_cost + self.cache_read_cost + self.out_cost + self.web_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ef6d0",
   "metadata": {},
   "source": [
    "A mapping of model pricing is also available in litellm, which is used to calculate the `response_cost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90af6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pricing = dict2obj(httpx.get(litellm.model_cost_map_url).json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc0ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_pricing['claude-sonnet-4-5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff68bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_pricing['gemini-3-pro-preview']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2987b",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a045f396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tempfile._TemporaryFileWrapper>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tempfile import NamedTemporaryFile\n",
    "tf =NamedTemporaryFile(suffix='.db')\n",
    "tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def user_id_fn(self:LisetteUsageLogger): return 'user-123'\n",
    "tf=NamedTemporaryFile(suffix='.db')\n",
    "logger = LisetteUsageLogger(tf.name)\n",
    "litellm.callbacks = [logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slc = ','.join('id model user_id prompt_tokens completion_tokens total_tokens cached_tokens cache_creation_tokens cache_read_tokens web_search_requests response_cost'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# litellm.set_verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0af81a",
   "metadata": {},
   "source": [
    "A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9215558",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat('claude-sonnet-4-5-20250929')\n",
    "r = chat(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b82ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=1, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=14, completion_tokens=11, total_tokens=25, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=0, response_cost=0.000207)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3) # wait for callback db write\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88fe4d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Usage(id=1, timestamp=1765646201.991512, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=14, completion_tokens=11, total_tokens=25, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=0, response_cost=0.000207)]"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39087125",
   "metadata": {},
   "source": [
    "Our calculated cost matches litellm's `response_cost`. In some cases it might be better to use the custom calculation as we'll see in the remaining of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367cb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55735017",
   "metadata": {},
   "source": [
    "Now, let's test with streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat('claude-sonnet-4-5')\n",
    "res = chat(\"Count from 1 to 5\", stream=True)\n",
    "for o in res: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8eb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=2, timestamp=UNSET, model='claude-sonnet-4-5', user_id='user-123', prompt_tokens=15, completion_tokens=17, total_tokens=32, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=0, response_cost=0.00030000000000000003)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fadb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9bb4b",
   "metadata": {},
   "source": [
    "Streaming logged successfully. Let's also verify async chat calls are logged properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270a8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "3 + 3 = 6\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='3 + 3 = 6', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_async = AsyncChat('claude-sonnet-4-5-20250929')\n",
    "await chat_async(\"What is 3+3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a75d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=2, timestamp=UNSET, model='claude-sonnet-4-5', user_id='user-123', prompt_tokens=15, completion_tokens=17, total_tokens=32, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=0, response_cost=0.00030000000000000003)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82d440",
   "metadata": {},
   "source": [
    "Finally, let's test async streaming to ensure all API patterns are covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7791bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='10, 11, 12, 13, 14, 15', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=20, prompt_tokens=38, total_tokens=58, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "res = await chat_async(\"Count from 10 to 15\", stream=True)\n",
    "async for o in res: pass\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=4, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=38, completion_tokens=20, total_tokens=58, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=0, response_cost=0.00041400000000000003)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050cf3a5",
   "metadata": {},
   "source": [
    "### Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60ec86",
   "metadata": {},
   "source": [
    "Now let's run a prompt with web search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c79df374",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The weather in New York City on Saturday, December 13, 2025, at 11:56 AM EST is cloudy with a temperature of 37°F (3°C), feeling like 34°F (1°C). The humidity is around 50%.\n",
       "\n",
       "For the rest of Saturday, it is expected to remain cloudy during the day with a 10% chance of snow, and then snow at night with a 65% chance of snow. The temperature will range between 31°F (-1°C) and 39°F (4°C). A Winter Weather Advisory is in effect from 10:00 PM on December 13th until 1:00 PM on December 14th.\n",
       "\n",
       "The air quality in New York City has reached a high level of pollution and is considered unhealthy for sensitive groups.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=343, prompt_tokens=12, total_tokens=449, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=139, rejected_prediction_tokens=None, text_tokens=204, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=12, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='The weather in New York City on Saturday, December 13, 2025, at 11:56 AM EST is cloudy with a temperature of 37°F (3°C), feeling like 34°F (1°C). The humidity is around 50%.\\n\\nFor the rest of Saturday, it is expected to remain cloudy during the day with a 10% chance of snow, and then snow at night with a 65% chance of snow. The temperature will range between 31°F (-1°C) and 39°F (4°C). A Winter Weather Advisory is in effect from 10:00 PM on December 13th until 1:00 PM on December 14th.\\n\\nThe air quality in New York City has reached a high level of pollution and is considered unhealthy for sensitive groups.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None, annotations=[{'type': 'url_citation', 'url_citation': {'end_index': 177, 'start_index': 150, 'title': 'Current time information in New York, NY, US.', 'url': 'https://www.google.com/search?q=time+in+New+York,+NY,+US'}}, {'type': 'url_citation', 'url_citation': {'end_index': 395, 'start_index': 329, 'title': 'Weather information for New York, NY, US', 'url': 'https://www.google.com/search?q=weather+in+New York, NY,+US'}}, {'type': 'url_citation', 'url_citation': {'end_index': 497, 'start_index': 396, 'title': 'weather.gov', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGn3osvWma67azzQ-Vn41yiQAQgcslizElcp14KHt-7jGi3LtJuo8bOK8NCMqG6avhudJJvWrLCTOwm7hmFsNKtHa0TP8C30GH6W1Ol1JcN8h6ZoMVtL_f7LnkERgr5VcCfpPLy5XZqphwXxRsR_edRQUGsuedYdoskJBgK'}}, {'type': 'url_citation', 'url_citation': {'end_index': 619, 'start_index': 499, 'title': 'accuweather.com', 'url': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSvL82FTdxBPzHbPJlLMkVIqzYbzc-orKxwIEHDLPlIckNUL89Lpj3p6UbHYBMCRJw_2Yt3daMsMe63AMsyqpnK1EK0T20QLc3dQfhJMOoCZo6esHSkgh4umfwm8Hr1SVhJdMW0Uvx6vTqlK0gOjd-qQfF0kVCIkOr1rzLJMziYTNp'}}]))], usage=Usage(completion_tokens=343, prompt_tokens=12, total_tokens=449, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=139, rejected_prediction_tokens=None, text_tokens=204, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=12, image_tokens=None)), vertex_ai_grounding_metadata=[{'searchEntryPoint': {'renderedContent': '<style>\\n.container {\\n  align-items: center;\\n  border-radius: 8px;\\n  display: flex;\\n  font-family: Google Sans, Roboto, sans-serif;\\n  font-size: 14px;\\n  line-height: 20px;\\n  padding: 8px 12px;\\n}\\n.chip {\\n  display: inline-block;\\n  border: solid 1px;\\n  border-radius: 16px;\\n  min-width: 14px;\\n  padding: 5px 16px;\\n  text-align: center;\\n  user-select: none;\\n  margin: 0 8px;\\n  -webkit-tap-highlight-color: transparent;\\n}\\n.carousel {\\n  overflow: auto;\\n  scrollbar-width: none;\\n  white-space: nowrap;\\n  margin-right: -12px;\\n}\\n.headline {\\n  display: flex;\\n  margin-right: 4px;\\n}\\n.gradient-container {\\n  position: relative;\\n}\\n.gradient {\\n  position: absolute;\\n  transform: translate(3px, -9px);\\n  height: 36px;\\n  width: 9px;\\n}\\n@media (prefers-color-scheme: light) {\\n  .container {\\n    background-color: #fafafa;\\n    box-shadow: 0 0 0 1px #0000000f;\\n  }\\n  .headline-label {\\n    color: #1f1f1f;\\n  }\\n  .chip {\\n    background-color: #ffffff;\\n    border-color: #d2d2d2;\\n    color: #5e5e5e;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:focus {\\n    background-color: #f2f2f2;\\n  }\\n  .chip:active {\\n    background-color: #d8d8d8;\\n    border-color: #b6b6b6;\\n  }\\n  .logo-dark {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #fafafa 15%, #fafafa00 100%);\\n  }\\n}\\n@media (prefers-color-scheme: dark) {\\n  .container {\\n    background-color: #1f1f1f;\\n    box-shadow: 0 0 0 1px #ffffff26;\\n  }\\n  .headline-label {\\n    color: #fff;\\n  }\\n  .chip {\\n    background-color: #2c2c2c;\\n    border-color: #3c4043;\\n    color: #fff;\\n    text-decoration: none;\\n  }\\n  .chip:hover {\\n    background-color: #353536;\\n  }\\n  .chip:focus {\\n    background-color: #353536;\\n  }\\n  .chip:active {\\n    background-color: #464849;\\n    border-color: #53575b;\\n  }\\n  .logo-light {\\n    display: none;\\n  }\\n  .gradient {\\n    background: linear-gradient(90deg, #1f1f1f 15%, #1f1f1f00 100%);\\n  }\\n}\\n</style>\\n<div class=\"container\">\\n  <div class=\"headline\">\\n    <svg class=\"logo-light\" width=\"18\" height=\"18\" viewBox=\"9 9 35 35\" fill=\"none\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M42.8622 27.0064C42.8622 25.7839 42.7525 24.6084 42.5487 23.4799H26.3109V30.1568H35.5897C35.1821 32.3041 33.9596 34.1222 32.1258 35.3448V39.6864H37.7213C40.9814 36.677 42.8622 32.2571 42.8622 27.0064V27.0064Z\" fill=\"#4285F4\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 43.8555C30.9659 43.8555 34.8687 42.3195 37.7213 39.6863L32.1258 35.3447C30.5898 36.3792 28.6306 37.0061 26.3109 37.0061C21.8282 37.0061 18.0195 33.9811 16.6559 29.906H10.9194V34.3573C13.7563 39.9841 19.5712 43.8555 26.3109 43.8555V43.8555Z\" fill=\"#34A853\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M16.6559 29.8904C16.3111 28.8559 16.1074 27.7588 16.1074 26.6146C16.1074 25.4704 16.3111 24.3733 16.6559 23.3388V18.8875H10.9194C9.74388 21.2072 9.06992 23.8247 9.06992 26.6146C9.06992 29.4045 9.74388 32.022 10.9194 34.3417L15.3864 30.8621L16.6559 29.8904V29.8904Z\" fill=\"#FBBC05\"/>\\n      <path fill-rule=\"evenodd\" clip-rule=\"evenodd\" d=\"M26.3109 16.2386C28.85 16.2386 31.107 17.1164 32.9095 18.8091L37.8466 13.8719C34.853 11.082 30.9659 9.3736 26.3109 9.3736C19.5712 9.3736 13.7563 13.245 10.9194 18.8875L16.6559 23.3388C18.0195 19.2636 21.8282 16.2386 26.3109 16.2386V16.2386Z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <svg class=\"logo-dark\" width=\"18\" height=\"18\" viewBox=\"0 0 48 48\" xmlns=\"http://www.w3.org/2000/svg\">\\n      <circle cx=\"24\" cy=\"23\" fill=\"#FFF\" r=\"22\"/>\\n      <path d=\"M33.76 34.26c2.75-2.56 4.49-6.37 4.49-11.26 0-.89-.08-1.84-.29-3H24.01v5.99h8.03c-.4 2.02-1.5 3.56-3.07 4.56v.75l3.91 2.97h.88z\" fill=\"#4285F4\"/>\\n      <path d=\"M15.58 25.77A8.845 8.845 0 0 0 24 31.86c1.92 0 3.62-.46 4.97-1.31l4.79 3.71C31.14 36.7 27.65 38 24 38c-5.93 0-11.01-3.4-13.45-8.36l.17-1.01 4.06-2.85h.8z\" fill=\"#34A853\"/>\\n      <path d=\"M15.59 20.21a8.864 8.864 0 0 0 0 5.58l-5.03 3.86c-.98-2-1.53-4.25-1.53-6.64 0-2.39.55-4.64 1.53-6.64l1-.22 3.81 2.98.22 1.08z\" fill=\"#FBBC05\"/>\\n      <path d=\"M24 14.14c2.11 0 4.02.75 5.52 1.98l4.36-4.36C31.22 9.43 27.81 8 24 8c-5.93 0-11.01 3.4-13.45 8.36l5.03 3.85A8.86 8.86 0 0 1 24 14.14z\" fill=\"#EA4335\"/>\\n    </svg>\\n    <div class=\"gradient-container\"><div class=\"gradient\"></div></div>\\n  </div>\\n  <div class=\"carousel\">\\n    <a class=\"chip\" href=\"https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQEXRbpA2bEBW4x6FcAJpQkpnPVGruCTUPTOfMyUK29etFFrcQdUOVJrjxqZpD58qITBF1BFFP4aQWSnq8_7q0n4_VYvGZjzenQFxpK0YoRqZryG45shEcc-9T6EdVfOZPVQDvOsrf7KJlp0auakrkX1F8_r8Zm2VkVhbTWM8SD4gqEhXkRvNkiGFxcy2udVPys=\">weather in NYC</a>\\n  </div>\\n</div>\\n'}, 'groundingChunks': [{'web': {'uri': 'https://www.google.com/search?q=time+in+New+York,+NY,+US', 'title': 'Current time information in New York, NY, US.'}}, {'web': {'uri': 'https://www.google.com/search?q=weather+in+New York, NY,+US', 'title': 'Weather information for New York, NY, US'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGn3osvWma67azzQ-Vn41yiQAQgcslizElcp14KHt-7jGi3LtJuo8bOK8NCMqG6avhudJJvWrLCTOwm7hmFsNKtHa0TP8C30GH6W1Ol1JcN8h6ZoMVtL_f7LnkERgr5VcCfpPLy5XZqphwXxRsR_edRQUGsuedYdoskJBgK', 'title': 'weather.gov'}}, {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQHSvL82FTdxBPzHbPJlLMkVIqzYbzc-orKxwIEHDLPlIckNUL89Lpj3p6UbHYBMCRJw_2Yt3daMsMe63AMsyqpnK1EK0T20QLc3dQfhJMOoCZo6esHSkgh4umfwm8Hr1SVhJdMW0Uvx6vTqlK0gOjd-qQfF0kVCIkOr1rzLJMziYTNp', 'title': 'accuweather.com'}}], 'groundingSupports': [{'segment': {'startIndex': 150, 'endIndex': 177, 'text': 'The humidity is around 50%.'}, 'groundingChunkIndices': [0, 1]}, {'segment': {'startIndex': 329, 'endIndex': 395, 'text': 'The temperature will range between 31°F (-1°C) and 39°F (4°C).'}, 'groundingChunkIndices': [1]}, {'segment': {'startIndex': 396, 'endIndex': 497, 'text': 'A Winter Weather Advisory is in effect from 10:00 PM on December 13th until 1:00 PM on December 14th.'}, 'groundingChunkIndices': [2]}, {'segment': {'startIndex': 499, 'endIndex': 619, 'text': 'The air quality in New York City has reached a high level of pollution and is considered unhealthy for sensitive groups.'}, 'groundingChunkIndices': [3]}], 'webSearchQueries': ['weather in NYC']}], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat('gemini/gemini-2.5-flash')\n",
    "chat(\"What is the weather like in NYC? Search web.\", search=\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430a280f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=5, timestamp=UNSET, model='gemini-2.5-flash', user_id='user-123', prompt_tokens=12, completion_tokens=343, total_tokens=449, cached_tokens=None, cache_creation_tokens=None, cache_read_tokens=None, web_search_requests=1, response_cost=0.0008611000000000001)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b597375",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.web_search_requests,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d3c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the current weather information for New York City:\n",
       "\n",
       "**Today (December 2, 2025):**\n",
       "Rain, mainly after 7am, with a high near 43°F. Chance of precipitation is 100% with new precipitation amounts between three quarters and one inch possible. North wind 5 to 13 mph.\n",
       "\n",
       "**Tonight:**\n",
       "A 20 percent chance of rain before 10pm, then cloudy during the early evening with gradual clearing and a low around 32°F.\n",
       "\n",
       "**Air Quality:**\n",
       "The air has reached a high level of pollution and is unhealthy for sensitive groups.\n",
       "\n",
       "The weather is rainy and cool today with temperatures in the low 40s, and it will clear up tonight with temperatures dropping to freezing.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=318, prompt_tokens=12012, total_tokens=12330, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Based on the current weather information for New York City:\\n\\n**Today (December 2, 2025):**\\nRain, mainly after 7am, with a high near 43°F. Chance of precipitation is 100% with new precipitation amounts between three quarters and one inch possible. North wind 5 to 13 mph.\\n\\n**Tonight:**\\nA 20 percent chance of rain before 10pm, then cloudy during the early evening with gradual clearing and a low around 32°F.\\n\\n**Air Quality:**\\nThe air has reached a high level of pollution and is unhealthy for sensitive groups.\\n\\nThe weather is rainy and cool today with temperatures in the low 40s, and it will clear up tonight with temperatures dropping to freezing.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': [[{'type': 'web_search_result_location', 'cited_text': 'Rain, mainly after 7am. High near 43. ', 'url': 'https://forecast.weather.gov/MapClick.php?lon=-73.99419&lat=40.71593', 'title': 'National Weather Service', 'encrypted_index': 'EpEBCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDPut3lyE5unJezaGqxoMLq+4uYjJON8nKg1nIjA7lcAEX2A6D3a/veHjegcDsT0KulT7jKeP4y/2XC1DaHuFcvF7po/tURoRYaVmVSsqFXMRP85K6T1OnbQETLrgzILBi+j6VRgE', 'supported_text': 'Rain, mainly after 7am, with a high near 43°F'}], [{'type': 'web_search_result_location', 'cited_text': 'Chance of precipitation is 100%. New precipitation amounts between three quarters and one inch possible. ', 'url': 'https://forecast.weather.gov/MapClick.php?lon=-73.99419&lat=40.71593', 'title': 'National Weather Service', 'encrypted_index': 'EpIBCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDBPyfYnD1KvAYxpJ7xoMQMIdjxheRZpRkKv9IjBIdn3trXl8W6Xl86I3+kdQCHC8I4Pu1zqxAGo6MpY99o1mgW/UwykEfsBJqf+nsHMqFgH22UUcNMVqgS72HaSINLjarvBKDpIYBA==', 'supported_text': 'Chance of precipitation is 100% with new precipitation amounts between three quarters and one inch possible'}], [{'type': 'web_search_result_location', 'cited_text': 'North wind 5 to 13 mph. ', 'url': 'https://forecast.weather.gov/MapClick.php?lon=-73.99419&lat=40.71593', 'title': 'National Weather Service', 'encrypted_index': 'Eo8BCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDBttfpOIWWVA2INwUhoMnDpaPw9Qh4aSA5B1IjBt5Bg0tYYbwgOgPOSE9m0Mei7eKzvppWhJMe/OC/7l4xd6qBljbYA0KF7MVUpVUsMqE4kHtHeW8lFPOgtkPK4D/c9pPSIYBA==', 'supported_text': 'North wind 5 to 13 mph'}], [{'type': 'web_search_result_location', 'cited_text': '... A 20 percent chance of rain before 10pm. Cloudy during the early evening, then gradual clearing, with a low around 32. ', 'url': 'https://forecast.weather.gov/MapClick.php?lon=-73.99419&lat=40.71593', 'title': 'National Weather Service', 'encrypted_index': 'EpMBCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDMQgy3vA/OPch/mlshoMkb0uiMHEj2IsSDADIjBs1ZmSgXe4vvNkuIJTrfiqMssxNE7WI2BsH6TOkP3Ju5I2aQcLQe0i2neuN/f3np4qF5Nfez6AxWSnoHdggd26t8ahsiGx+X20GAQ=', 'supported_text': 'A 20 percent chance of rain before 10pm, then cloudy during the early evening with gradual clearing and a low around 32°F'}], [{'type': 'web_search_result_location', 'cited_text': 'The air has reached a high level of pollution and is unhealthy for sensitive groups. ', 'url': 'https://www.accuweather.com/en/us/new-york/10021/weather-forecast/14-349727_1_al', 'title': 'New York City, NY Weather Forecast | AccuWeather', 'encrypted_index': 'EpABCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDGIn8NEKIH9Cs42ihBoMvrQe3BWWn1KfZUgiIjCD3kmwLyr+q0mUxgWBvQupYVn/WbLKqgsPKXiBo1UQUIciAUdF24B5UOMfMq8ES+cqFL2z+0v8U0GhAknxzr8NnuPslNllGAQ=', 'supported_text': 'The air has reached a high level of pollution and is unhealthy for sensitive groups'}]], 'thinking_blocks': None}))], usage=Usage(completion_tokens=318, prompt_tokens=12012, total_tokens=12330, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat('claude-sonnet-4-5-20250929')\n",
    "chat(\"What is the weather like in NYC? Search web.\", search=\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86f247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=6, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=12012, completion_tokens=318, total_tokens=12330, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=1, response_cost=0.040805999999999995)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50379ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.web_search_requests,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d58e3",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "Litellm's `response_cost` doesn't take search request cost into account!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a35480",
   "metadata": {},
   "source": [
    "Now, this is a case where using the custom calculations is better as it will also include the web search request cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost + u.web_search_requests * model_prices[u.model]['web_search_prc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77e9b5a8",
   "metadata": {},
   "source": [
    "### Search with streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8929bae",
   "metadata": {},
   "source": [
    "Web search with streaming:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b5d1da0",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "Gemini web search requests are part of `prompt_tokens_details` which is only included with `stream_options={\"include_usage\": True}`. \n",
    "\n",
    "There is currently a bug with gemini web search request counts, [Open an Issue](https://github.com/BerriAI/litellm/issues/17919) and [PR](https://github.com/BerriAI/litellm/pull/17921)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1faec06f",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat('gemini/gemini-2.5-flash')\n",
    "res = chat(\"What is the weather like in NYC? Search web.\", search=\"m\", stream=True, stream_options={\"include_usage\": True})\n",
    "for o in res: pass\n",
    "# print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21056927",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=7, timestamp=UNSET, model='gemini-2.5-flash', user_id='user-123', prompt_tokens=12, completion_tokens=341, total_tokens=353, cached_tokens=None, cache_creation_tokens=None, cache_read_tokens=None, web_search_requests=1, response_cost=0.0358561)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae560895",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "Anthropic web search requests are not included in usage when `stream=True`. The web search requests are not available even with `stream_options={\"include_usage\": True}`. Here is an open [Issue](https://github.com/BerriAI/litellm/issues/16631) and the merged fix [PR](https://github.com/BerriAI/litellm/pull/16826)\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat('claude-sonnet-4-5')\n",
    "res = chat(\"What is the weather like in NYC now? Search web.\", search=\"m\", stream=True, stream_options={\"include_usage\": True})\n",
    "for o in res: pass\n",
    "# print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0212a483",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'completion_tokens': 413,\n",
       " 'prompt_tokens': 26416,\n",
       " 'total_tokens': 26829,\n",
       " 'completion_tokens_details': CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None),\n",
       " 'prompt_tokens_details': PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0),\n",
       " 'cache_creation_input_tokens': 0,\n",
       " 'cache_read_input_tokens': 0}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dict(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c9bb1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=8, timestamp=UNSET, model='claude-sonnet-4-5', user_id='user-123', prompt_tokens=26416, completion_tokens=413, total_tokens=26829, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=0, response_cost=0.085443)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0c219",
   "metadata": {},
   "source": [
    "Once we upgrade to `litellm>1.80.5` `web_search_requests` will be included with `stream=True` via `server_tool_use`, and the following test should pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f51ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_eq(u.cost, u.response_cost + u.web_search_requests * model_prices[u.model]['web_search_prc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13257134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# u.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(logger.usage()), 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def total_cost(self:Usage, sc=0.01): return self.response_cost + sc * ifnone(self.web_search_requests, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_close((L(logger.usage()).map(lambda o:o.total_cost(sc=0.01)).sum()), 0.086, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a00a720",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e57bed",
   "metadata": {},
   "source": [
    "A simple Gemini example (requires min tokens and running twice to see `cached_tokens`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad22437",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2 + 2 = 4\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `gemini-2.5-flash`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=41, prompt_tokens=7010, total_tokens=7051, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=34, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=7010, image_tokens=None))`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='2 + 2 = 4', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None))], usage=Usage(completion_tokens=41, prompt_tokens=7010, total_tokens=7051, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=34, rejected_prediction_tokens=None, text_tokens=7, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=7010, image_tokens=None)), vertex_ai_grounding_metadata=[], vertex_ai_url_context_metadata=[], vertex_ai_safety_results=[], vertex_ai_citation_metadata=[])"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat('gemini/gemini-2.5-flash')\n",
    "chat(\"What is 2+2?\"* 500)\n",
    "time.sleep(5)\n",
    "chat(\"What is 2+2?\"* 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0621b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "time.sleep(0.3) # wait for callback db write\n",
    "u = logger.usage(select=slc)[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6133e694",
   "metadata": {},
   "outputs": [],
   "source": [
    "# flaky test\n",
    "# test_eq(len(logger.usage()), 8)\n",
    "# test_eq(logger.usage()[-1].cached_tokens > 3000, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3577f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e335da",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620e45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
