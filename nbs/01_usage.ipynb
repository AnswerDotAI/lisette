{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3bd3dbfa",
   "metadata": {},
   "source": [
    "# Usage\n",
    "\n",
    "> Lisette usage and cost monitoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9427c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6856c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from litellm.integrations.custom_logger import CustomLogger\n",
    "import time\n",
    "try: \n",
    "    from fastlite import *\n",
    "    from fastlite.core import dataclass\n",
    "except ImportError: raise ImportError(\"Please install `fastlite` to use sqlite based lisette usage logging.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "743eedcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import litellm, importlib, httpx\n",
    "from lisette.core import Chat, AsyncChat, patch_litellm\n",
    "from fastcore.all import *\n",
    "from cachy import enable_cachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69144ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cff8990",
   "metadata": {},
   "source": [
    "## Lisette Usage Logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9acabfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(litellm); # to re-run the notebook without kernel restart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7beb5064",
   "metadata": {},
   "outputs": [],
   "source": [
    "patch_litellm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed71558",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Usage: id:int; timestamp:float; model:str; user_id:str; prompt_tokens:int; completion_tokens:int; total_tokens:int; cached_tokens:int; cache_creation_tokens:int; cache_read_tokens:int; web_search_requests:int; response_cost:int"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9bf5fc1",
   "metadata": {},
   "source": [
    "The precomputed response cost provided is available in `kwargs['response_cost']` according to the [litellm docs](https://docs.litellm.ai/docs/observability/custom_callback#whats-available-in-kwargs):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad2e088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class LisetteUsageLogger(CustomLogger):\n",
    "    def __init__(self, db_path): \n",
    "        self.db = Database(db_path)\n",
    "        self.usage = self.db.create(Usage)\n",
    "    \n",
    "    async def async_log_success_event(self, kwargs, response_obj, start_time, end_time): self._log_usage(response_obj, kwargs['response_cost'], start_time, end_time)\n",
    "    def log_success_event(self, kwargs, response_obj, start_time, end_time):             self._log_usage(response_obj, kwargs['response_cost'], start_time, end_time)\n",
    "    def _log_usage(self, response_obj, response_cost, start_time, end_time):\n",
    "        usage = response_obj.usage\n",
    "        ptd   = usage.prompt_tokens_details\n",
    "        self.usage.insert(Usage(timestamp=time.time(), model=response_obj.model, user_id=self.user_id_fn(), prompt_tokens=usage.prompt_tokens, completion_tokens=usage.completion_tokens,\n",
    "                                    total_tokens=usage.total_tokens, cached_tokens=ptd.cached_tokens if ptd else 0, cache_creation_tokens=usage.cache_creation_input_tokens, \n",
    "                                    cache_read_tokens=usage.cache_read_input_tokens, web_search_requests=nested_idx(usage, 'server_tool_use', 'web_search_requests'), response_cost=response_cost))\n",
    "                  \n",
    "    def user_id_fn(self): raise NotImplementedError('Please implement `LisetteUsageLogger.user_id_fn` before initializing, e.g using fastcore.patch.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bdfd5ca",
   "metadata": {},
   "source": [
    "## Cost Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce652ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PrefixDict(dict):\n",
    "    def __getitem__(self, key):\n",
    "        if key in self.keys(): return super().__getitem__(key)\n",
    "        for k in self.keys(): \n",
    "            if key.startswith(k): return super().__getitem__(k)\n",
    "        raise KeyError(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847758d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_prices = PrefixDict({\n",
    "    'claude-sonnet-4-5': dict(input_prc = 3/1e6, cache_write_prc = 3.75/1e6, cache_read_prc = 0.3/1e6, output_prc = 15/1e6, web_search_prc = 10/1e3)\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be909f",
   "metadata": {},
   "source": [
    "Simplified cost utils to demonstrate total cost calculation (use `Usage.response_cost` in prod):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6624d659",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch(as_prop=True)\n",
    "def inp_cost(self:Usage):         return model_prices[self.model]['input_prc'] * (self.prompt_tokens - self.cache_read_tokens)\n",
    "@patch(as_prop=True)\n",
    "def cache_write_cost(self:Usage): return model_prices[self.model]['cache_write_prc'] * self.cache_creation_tokens\n",
    "@patch(as_prop=True)\n",
    "def cache_read_cost(self:Usage):  return model_prices[self.model]['cache_read_prc'] * self.cache_read_tokens\n",
    "@patch(as_prop=True)\n",
    "def out_cost(self:Usage):         return model_prices[self.model]['output_prc'] * self.completion_tokens\n",
    "@patch(as_prop=True)\n",
    "def web_cost(self:Usage):         return model_prices[self.model]['web_search_prc'] * ifnone(self.web_search_requests, 0)\n",
    "@patch(as_prop=True)\n",
    "def cost(self:Usage):             return self.inp_cost + self.cache_write_cost + self.cache_read_cost + self.out_cost + self.web_cost\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "432ef6d0",
   "metadata": {},
   "source": [
    "A mapping of model pricing is also available in litellm, which is used to calculate the `response_cost`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90af6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pricing = dict2obj(httpx.get(litellm.model_cost_map_url).json())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35cc0ba6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "{ 'cache_creation_input_token_cost': 3.75e-06,\n",
       "  'cache_creation_input_token_cost_above_200k_tokens': 7.5e-06,\n",
       "  'cache_read_input_token_cost': 3e-07,\n",
       "  'cache_read_input_token_cost_above_200k_tokens': 6e-07,\n",
       "  'input_cost_per_token': 3e-06,\n",
       "  'input_cost_per_token_above_200k_tokens': 6e-06,\n",
       "  'litellm_provider': 'anthropic',\n",
       "  'max_input_tokens': 200000,\n",
       "  'max_output_tokens': 64000,\n",
       "  'max_tokens': 64000,\n",
       "  'mode': 'chat',\n",
       "  'output_cost_per_token': 1.5e-05,\n",
       "  'output_cost_per_token_above_200k_tokens': 2.25e-05,\n",
       "  'search_context_cost_per_query': { 'search_context_size_high': 0.01,\n",
       "                                     'search_context_size_low': 0.01,\n",
       "                                     'search_context_size_medium': 0.01},\n",
       "  'supports_assistant_prefill': True,\n",
       "  'supports_computer_use': True,\n",
       "  'supports_function_calling': True,\n",
       "  'supports_pdf_input': True,\n",
       "  'supports_prompt_caching': True,\n",
       "  'supports_reasoning': True,\n",
       "  'supports_response_schema': True,\n",
       "  'supports_tool_choice': True,\n",
       "  'supports_vision': True,\n",
       "  'tool_use_system_prompt_tokens': 346}\n",
       "```"
      ],
      "text/plain": [
       "{'cache_creation_input_token_cost': 3.75e-06,\n",
       " 'cache_read_input_token_cost': 3e-07,\n",
       " 'input_cost_per_token': 3e-06,\n",
       " 'input_cost_per_token_above_200k_tokens': 6e-06,\n",
       " 'output_cost_per_token_above_200k_tokens': 2.25e-05,\n",
       " 'cache_creation_input_token_cost_above_200k_tokens': 7.5e-06,\n",
       " 'cache_read_input_token_cost_above_200k_tokens': 6e-07,\n",
       " 'litellm_provider': 'anthropic',\n",
       " 'max_input_tokens': 200000,\n",
       " 'max_output_tokens': 64000,\n",
       " 'max_tokens': 64000,\n",
       " 'mode': 'chat',\n",
       " 'output_cost_per_token': 1.5e-05,\n",
       " 'search_context_cost_per_query': {'search_context_size_high': 0.01,\n",
       "  'search_context_size_low': 0.01,\n",
       "  'search_context_size_medium': 0.01},\n",
       " 'supports_assistant_prefill': True,\n",
       " 'supports_computer_use': True,\n",
       " 'supports_function_calling': True,\n",
       " 'supports_pdf_input': True,\n",
       " 'supports_prompt_caching': True,\n",
       " 'supports_reasoning': True,\n",
       " 'supports_response_schema': True,\n",
       " 'supports_tool_choice': True,\n",
       " 'supports_vision': True,\n",
       " 'tool_use_system_prompt_tokens': 346}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pricing['claude-sonnet-4-5']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19ff68bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```python\n",
       "{ 'cache_creation_input_token_cost_above_200k_tokens': 2.5e-07,\n",
       "  'cache_read_input_token_cost': 2e-07,\n",
       "  'cache_read_input_token_cost_above_200k_tokens': 4e-07,\n",
       "  'input_cost_per_token': 2e-06,\n",
       "  'input_cost_per_token_above_200k_tokens': 4e-06,\n",
       "  'input_cost_per_token_batches': 1e-06,\n",
       "  'litellm_provider': 'vertex_ai-language-models',\n",
       "  'max_audio_length_hours': 8.4,\n",
       "  'max_audio_per_prompt': 1,\n",
       "  'max_images_per_prompt': 3000,\n",
       "  'max_input_tokens': 1048576,\n",
       "  'max_output_tokens': 65535,\n",
       "  'max_pdf_size_mb': 30,\n",
       "  'max_tokens': 65535,\n",
       "  'max_video_length': 1,\n",
       "  'max_videos_per_prompt': 10,\n",
       "  'mode': 'chat',\n",
       "  'output_cost_per_token': 1.2e-05,\n",
       "  'output_cost_per_token_above_200k_tokens': 1.8e-05,\n",
       "  'output_cost_per_token_batches': 6e-06,\n",
       "  'source': 'https://cloud.google.com/vertex-ai/generative-ai/pricing',\n",
       "  'supported_endpoints': ['/v1/chat/completions', '/v1/completions', '/v1/batch'],\n",
       "  'supported_modalities': ['text', 'image', 'audio', 'video'],\n",
       "  'supported_output_modalities': ['text'],\n",
       "  'supports_audio_input': True,\n",
       "  'supports_function_calling': True,\n",
       "  'supports_pdf_input': True,\n",
       "  'supports_prompt_caching': True,\n",
       "  'supports_reasoning': True,\n",
       "  'supports_response_schema': True,\n",
       "  'supports_system_messages': True,\n",
       "  'supports_tool_choice': True,\n",
       "  'supports_video_input': True,\n",
       "  'supports_vision': True,\n",
       "  'supports_web_search': True}\n",
       "```"
      ],
      "text/plain": [
       "{'cache_read_input_token_cost': 2e-07,\n",
       " 'cache_read_input_token_cost_above_200k_tokens': 4e-07,\n",
       " 'cache_creation_input_token_cost_above_200k_tokens': 2.5e-07,\n",
       " 'input_cost_per_token': 2e-06,\n",
       " 'input_cost_per_token_above_200k_tokens': 4e-06,\n",
       " 'input_cost_per_token_batches': 1e-06,\n",
       " 'litellm_provider': 'vertex_ai-language-models',\n",
       " 'max_audio_length_hours': 8.4,\n",
       " 'max_audio_per_prompt': 1,\n",
       " 'max_images_per_prompt': 3000,\n",
       " 'max_input_tokens': 1048576,\n",
       " 'max_output_tokens': 65535,\n",
       " 'max_pdf_size_mb': 30,\n",
       " 'max_tokens': 65535,\n",
       " 'max_video_length': 1,\n",
       " 'max_videos_per_prompt': 10,\n",
       " 'mode': 'chat',\n",
       " 'output_cost_per_token': 1.2e-05,\n",
       " 'output_cost_per_token_above_200k_tokens': 1.8e-05,\n",
       " 'output_cost_per_token_batches': 6e-06,\n",
       " 'source': 'https://cloud.google.com/vertex-ai/generative-ai/pricing',\n",
       " 'supported_endpoints': (#3) ['/v1/chat/completions','/v1/completions','/v1/batch'],\n",
       " 'supported_modalities': (#4) ['text','image','audio','video'],\n",
       " 'supported_output_modalities': (#1) ['text'],\n",
       " 'supports_audio_input': True,\n",
       " 'supports_function_calling': True,\n",
       " 'supports_pdf_input': True,\n",
       " 'supports_prompt_caching': True,\n",
       " 'supports_reasoning': True,\n",
       " 'supports_response_schema': True,\n",
       " 'supports_system_messages': True,\n",
       " 'supports_tool_choice': True,\n",
       " 'supports_video_input': True,\n",
       " 'supports_vision': True,\n",
       " 'supports_web_search': True}"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_pricing['gemini-3-pro-preview']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fd2987b",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cf6ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfp = Path('.lisette/litellm-usage.db')\n",
    "dbfp.parent.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a50ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "@patch\n",
    "def user_id_fn(self:LisetteUsageLogger): return 'user-123'\n",
    "logger = LisetteUsageLogger(dbfp)\n",
    "litellm.callbacks = [logger]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5842bb0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "slc = ','.join('id model user_id prompt_tokens completion_tokens total_tokens cached_tokens cache_creation_tokens cache_read_tokens web_search_requests response_cost'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac32ac47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# litellm.set_verbose = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d0af81a",
   "metadata": {},
   "source": [
    "A simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9215558",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "2+2 = 4\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=11, prompt_tokens=14, total_tokens=25, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='2+2 = 4', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=11, prompt_tokens=14, total_tokens=25, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat('claude-sonnet-4-5-20250929')\n",
    "chat(\"What is 2+2?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b82ed4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=1, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=14, completion_tokens=11, total_tokens=25, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=None, response_cost=0.000207)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3) # wait for callback db write\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39087125",
   "metadata": {},
   "source": [
    "Our calculated cost matches litellm's `response_cost`. In some cases it might be better to use the custom calculation as we'll see in the remaining of this notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367cb32f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55735017",
   "metadata": {},
   "source": [
    "Now, let's test with streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58b8cde6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat('claude-sonnet-4-5')\n",
    "res = chat(\"Count from 1 to 5\", stream=True)\n",
    "for o in res: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e8eb30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=2, timestamp=UNSET, model='claude-sonnet-4-5', user_id='user-123', prompt_tokens=15, completion_tokens=17, total_tokens=32, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=None, response_cost=0.00030000000000000003)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13fadb74",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9bb4b",
   "metadata": {},
   "source": [
    "Streaming logged successfully. Let's also verify async chat calls are logged properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5270a8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "3 + 3 = 6\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='3 + 3 = 6', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}))], usage=Usage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_async = AsyncChat('claude-sonnet-4-5-20250929')\n",
    "await chat_async(\"What is 3+3?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a75d42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=2, timestamp=UNSET, model='claude-sonnet-4-5', user_id='user-123', prompt_tokens=15, completion_tokens=17, total_tokens=32, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=None, response_cost=0.00030000000000000003)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1916085a",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f82d440",
   "metadata": {},
   "source": [
    "Finally, let's test async streaming to ensure all API patterns are covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7791bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='10, 11, 12, 13, 14, 15', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=20, prompt_tokens=38, total_tokens=58, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None))\n"
     ]
    }
   ],
   "source": [
    "res = await chat_async(\"Count from 10 to 15\", stream=True)\n",
    "async for o in res: pass\n",
    "print(o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd6b744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=4, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=38, completion_tokens=20, total_tokens=58, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=None, response_cost=0.00041400000000000003)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ea9299",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc60ec86",
   "metadata": {},
   "source": [
    "Now let's run a prompt with web search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d76d3c46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Based on the current weather information for New York City:\n",
       "\n",
       "**Today (December 2, 2025):**\n",
       "Rain, mainly after 7am, with a high near 43째F. Chance of precipitation is 100% with new precipitation amounts between three quarters and one inch possible. North wind 5 to 13 mph.\n",
       "\n",
       "**Tonight:**\n",
       "A 20 percent chance of rain before 10pm, then cloudy during the early evening with gradual clearing and a low around 32째F.\n",
       "\n",
       "**Air Quality:**\n",
       "The air has reached a high level of pollution and is unhealthy for sensitive groups.\n",
       "\n",
       "The weather is rainy and cool today with temperatures in the low 40s, and it will clear up tonight with temperatures dropping to freezing.\n",
       "\n",
       "<details>\n",
       "\n",
       "- id: `chatcmpl-xxx`\n",
       "- model: `claude-sonnet-4-5-20250929`\n",
       "- finish_reason: `stop`\n",
       "- usage: `Usage(completion_tokens=318, prompt_tokens=12012, total_tokens=12330, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0)`\n",
       "\n",
       "</details>"
      ],
      "text/plain": [
       "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='Based on the current weather information for New York City:\\n\\n**Today (December 2, 2025):**\\nRain, mainly after 7am, with a high near 43째F. Chance of precipitation is 100% with new precipitation amounts between three quarters and one inch possible. North wind 5 to 13 mph.\\n\\n**Tonight:**\\nA 20 percent chance of rain before 10pm, then cloudy during the early evening with gradual clearing and a low around 32째F.\\n\\n**Air Quality:**\\nThe air has reached a high level of pollution and is unhealthy for sensitive groups.\\n\\nThe weather is rainy and cool today with temperatures in the low 40s, and it will clear up tonight with temperatures dropping to freezing.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': [[{'type': 'web_search_result_location', 'cited_text': 'Rain, mainly after 7am. High near 43. ', 'url': 'https://forecast.weather.gov/MapClick.php?lon=-73.99419&lat=40.71593', 'title': 'National Weather Service', 'encrypted_index': 'EpEBCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDPut3lyE5unJezaGqxoMLq+4uYjJON8nKg1nIjA7lcAEX2A6D3a/veHjegcDsT0KulT7jKeP4y/2XC1DaHuFcvF7po/tURoRYaVmVSsqFXMRP85K6T1OnbQETLrgzILBi+j6VRgE'}], [{'type': 'web_search_result_location', 'cited_text': 'Chance of precipitation is 100%. New precipitation amounts between three quarters and one inch possible. ', 'url': 'https://forecast.weather.gov/MapClick.php?lon=-73.99419&lat=40.71593', 'title': 'National Weather Service', 'encrypted_index': 'EpIBCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDBPyfYnD1KvAYxpJ7xoMQMIdjxheRZpRkKv9IjBIdn3trXl8W6Xl86I3+kdQCHC8I4Pu1zqxAGo6MpY99o1mgW/UwykEfsBJqf+nsHMqFgH22UUcNMVqgS72HaSINLjarvBKDpIYBA=='}], [{'type': 'web_search_result_location', 'cited_text': 'North wind 5 to 13 mph. ', 'url': 'https://forecast.weather.gov/MapClick.php?lon=-73.99419&lat=40.71593', 'title': 'National Weather Service', 'encrypted_index': 'Eo8BCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDBttfpOIWWVA2INwUhoMnDpaPw9Qh4aSA5B1IjBt5Bg0tYYbwgOgPOSE9m0Mei7eKzvppWhJMe/OC/7l4xd6qBljbYA0KF7MVUpVUsMqE4kHtHeW8lFPOgtkPK4D/c9pPSIYBA=='}], [{'type': 'web_search_result_location', 'cited_text': '... A 20 percent chance of rain before 10pm. Cloudy during the early evening, then gradual clearing, with a low around 32. ', 'url': 'https://forecast.weather.gov/MapClick.php?lon=-73.99419&lat=40.71593', 'title': 'National Weather Service', 'encrypted_index': 'EpMBCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDMQgy3vA/OPch/mlshoMkb0uiMHEj2IsSDADIjBs1ZmSgXe4vvNkuIJTrfiqMssxNE7WI2BsH6TOkP3Ju5I2aQcLQe0i2neuN/f3np4qF5Nfez6AxWSnoHdggd26t8ahsiGx+X20GAQ='}], [{'type': 'web_search_result_location', 'cited_text': 'The air has reached a high level of pollution and is unhealthy for sensitive groups. ', 'url': 'https://www.accuweather.com/en/us/new-york/10021/weather-forecast/14-349727_1_al', 'title': 'New York City, NY Weather Forecast | AccuWeather', 'encrypted_index': 'EpABCioIChgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDGIn8NEKIH9Cs42ihBoMvrQe3BWWn1KfZUgiIjCD3kmwLyr+q0mUxgWBvQupYVn/WbLKqgsPKXiBo1UQUIciAUdF24B5UOMfMq8ES+cqFL2z+0v8U0GhAknxzr8NnuPslNllGAQ='}]], 'thinking_blocks': None}))], usage=Usage(completion_tokens=318, prompt_tokens=12012, total_tokens=12330, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0))"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat = Chat('claude-sonnet-4-5-20250929')\n",
    "chat(\"What is the weather like in NYC? Search web.\", search=\"m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb86f247",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=5, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=12012, completion_tokens=318, total_tokens=12330, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=1, response_cost=0.040805999999999995)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7d58e3",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "Litellm's `response_cost` doesn't take search request cost into account!\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a35480",
   "metadata": {},
   "source": [
    "Now, this is a case where using the custom calculations is better as it will also include the web search request cost:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc09ac23",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(u.cost, u.response_cost + model_prices[u.model]['web_search_prc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8929bae",
   "metadata": {},
   "source": [
    "Web search with streaming:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff66e5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat = Chat('claude-sonnet-4-5')\n",
    "res = chat(\"What is the weather like in NYC? Search web.\", search=\"m\", stream=True)\n",
    "for o in res: pass\n",
    "# print(o)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee0c219",
   "metadata": {},
   "source": [
    "::: {.callout-important}\n",
    "Web search requests are not included in usage when `stream=True`. Here is an open [Issue](https://github.com/BerriAI/litellm/issues/16631)\n",
    ":::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cf0c2c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Usage(id=6, timestamp=UNSET, model='claude-sonnet-4-5', user_id='user-123', prompt_tokens=12012, completion_tokens=318, total_tokens=12330, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=None, response_cost=None)"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.sleep(0.3)\n",
    "u = logger.usage(select=slc)[-1]; u"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ce4d9b",
   "metadata": {},
   "source": [
    "Once this [PR](https://github.com/BerriAI/litellm/pull/16826) is merged `web_search_requests` will be included with `stream=True`, and the following test should pass:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f51ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_eq(u.cost, u.response_cost + model_prices[u.model]['web_search_prc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13257134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# u.cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc36e5e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_eq(len(logger.usage()), 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "126e3064",
   "metadata": {},
   "source": [
    "Let's implement a utility to get the total cost including web search:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef8ac82",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "@patch\n",
    "def total_cost(self:Usage, sc=0.01): return self.response_cost + sc * ifnone(self.web_search_requests, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "661b19b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_close((L(logger.usage()).map(lambda o:o.total_cost(sc=0.01)).sum()), 0.086, 1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3577f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dbfp.parent.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e335da",
   "metadata": {},
   "source": [
    "# Export -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59b8b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d620e45d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
