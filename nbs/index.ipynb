{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e48818",
   "metadata": {},
   "source": [
    "# Meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fb4246",
   "metadata": {
    "hide_input": true
   },
   "source": [
    "I work at AnswerDotAI Jeremy Howard (from Fastai fame)'s company. Here are some tips he gave me before.\n",
    "```\n",
    "Documentation Tips from Jeremy Howard\n",
    "You built something great. You made it public.\n",
    "Your marketing worked. People are here. \n",
    "Don't lose them with bad docs.\n",
    "\n",
    "Core Principles\n",
    "Empathize with users who don't know what your software does and why it's valuable.\n",
    "Blame yourself, not users, when they struggle with your documentation.\n",
    "Simplify the underlying process rather than documenting complex procedures.\n",
    "Study documentation you personally find helpful and identify what makes it effective.\n",
    "\n",
    "Structure & Flow\n",
    "At each stage, focus on getting the reader to take the next step.\n",
    "Start with what your software does and why someone should care about it.\n",
    "Show how to install your software immediately after explaining what it is.\n",
    "Create a quick start that demonstrates complete working examples of what users can accomplish.\n",
    "Structure tutorials as a progression from quick starts, with more detailed explanations.\n",
    "Keep API reference documentation as the final layer, not the main documentation.\n",
    " \n",
    "Content Focus\n",
    "Show real-world, practical, end-to-end examples of solving complete problems.\n",
    "Explain concepts after demonstrating their real-world value.\n",
    "Target documentation to different experience levels (beginners, intermediate, advanced users).\n",
    "\n",
    "Technical Tips\n",
    "Use hyperlinks liberally so users can find more information when they need it.\n",
    "Create good object representations (dunder reprs) as they become part of your documentation.\n",
    "Read nbdev: Notebook Best Practices for more tips\n",
    "\n",
    "LLM Considerations\n",
    "Use LLMs to help identify gaps in your documentation.\n",
    "Test your documentation by asking an LLM to write tutorials using your library.\n",
    "When writing LLM-friendly docs, include more code examples and less explanatory text.\n",
    "Ensure code examples show the exact patterns you want LLMs to replicate.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96b00e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/rensdimmendaal/git/repos/lisette\n"
     ]
    }
   ],
   "source": [
    "%cd ~/git/repos/lisette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9be894c",
   "metadata": {
    "hide_input": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<documents>\r\n",
      "<document index=\"1\">\r\n",
      "<source>00_core.ipynb</source>\r\n",
      "<document_content>\r\n",
      "# Core\r\n",
      "\r\n",
      "&gt; Lisette Core\r\n",
      "\r\n",
      "```python\r\n",
      "#| default_exp core\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| hide\r\n",
      "from cachy import enable_cachy\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| hide\r\n",
      "enable_cachy()\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "import asyncio, base64, json, litellm, mimetypes\r\n",
      "from typing import Optional\r\n",
      "from html import escape\r\n",
      "from litellm import (acompletion, completion, stream_chunk_builder, Message,\r\n",
      "                     ModelResponse, ModelResponseStream, get_model_info)\r\n",
      "from litellm.utils import function_to_dict, StreamingChoices, Delta\r\n",
      "from toolslm.funccall import mk_ns, call_func, call_func_async, get_schema\r\n",
      "from fastcore.utils import *\r\n",
      "from fastcore import imghdr\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| hide\r\n",
      "from litellm.types.utils import ModelResponseBase\r\n",
      "from fastcore.test import *\r\n",
      "from IPython.display import Image, Markdown\r\n",
      "from fastcore.xtras import SaveReturn\r\n",
      "from fastcore.test import *\r\n",
      "```\r\n",
      "\r\n",
      "# LiteLLM\r\n",
      "\r\n",
      "::: {.content-hidden}\r\n",
      "## Deterministic responses\r\n",
      "\r\n",
      "LiteLLM `ModelResponse` objects have `id` and `created_at` fields that are generated dynamically. Even when we use [`cachy`](https://github.com/answerdotai/cachy) to cache the LLM response these dynamic fields create diffs which makes code review more challenging. The patches below ensure that `id` and `created_at` fields are fixed and won't generate diffs.\r\n",
      ":::\r\n",
      "\r\n",
      "```python\r\n",
      "#| hide\r\n",
      "@patch\r\n",
      "def __init__(self: ModelResponseBase, id=None, created=None, *args, **kwargs): \r\n",
      "    self._orig___init__(id='chatcmpl-xxx', created=1000000000, *args, **kwargs)\r\n",
      "\r\n",
      "@patch\r\n",
      "def __setattr__(self: ModelResponseBase, name, value):\r\n",
      "    if name == 'id': value = 'chatcmpl-xxx'\r\n",
      "    elif name == 'created': value = 1000000000\r\n",
      "    self._orig___setattr__(name, value)\r\n",
      "```\r\n",
      "\r\n",
      "## Completion -\r\n",
      "\r\n",
      "LiteLLM provides an convenient unified interface for most big LLM providers. Because it's so useful to be able to switch LLM providers with just one argument. We want to make it even easier to by adding some more convenience functions and classes. \r\n",
      "\r\n",
      "This is very similar to our other wrapper libraries for popular AI providers: [claudette](https://claudette.answer.ai/) (Anthropic), [gaspard](https://github.com/AnswerDotAI/gaspard) (Gemini), [cosette](https://answerdotai.github.io/cosette/) (OpenAI).\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "@patch\r\n",
      "def _repr_markdown_(self: litellm.ModelResponse):\r\n",
      "    message = self.choices[0].message\r\n",
      "    content = ''\r\n",
      "    if mc:=message.content: content += mc[0]['text'] if isinstance(mc,list) else mc\r\n",
      "    if message.tool_calls:\r\n",
      "        tool_calls = [f\"\\n\\nðŸ”§ {nested_idx(tc,'function','name')}({nested_idx(tc,'function','arguments')})\\n\" for tc in message.tool_calls]\r\n",
      "        content += \"\\n\".join(tool_calls)\r\n",
      "    if not content: content = str(message)\r\n",
      "    details = [\r\n",
      "        f\"id: `{self.id}`\",\r\n",
      "        f\"model: `{self.model}`\",\r\n",
      "        f\"finish_reason: `{self.choices[0].finish_reason}`\"\r\n",
      "    ]\r\n",
      "    if hasattr(self, 'usage') and self.usage: details.append(f\"usage: `{self.usage}`\")\r\n",
      "    det_str = '\\n- '.join(details)\r\n",
      "    \r\n",
      "    return f\"\"\"{content}\r\n",
      "\r\n",
      "&lt;details&gt;\r\n",
      "\r\n",
      "- {det_str}\r\n",
      "\r\n",
      "&lt;/details&gt;\"\"\"\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "ms = [\"gemini/gemini-2.5-flash\", \"claude-sonnet-4-20250514\", \"openai/gpt-4.1\"]\r\n",
      "msg = [{'role':'user','content':'Hey there!', 'cache_control': {'type': 'ephemeral'}}]\r\n",
      "for m in ms:\r\n",
      "    display(Markdown(f'**{m}:**'))\r\n",
      "    display(completion(m,msg))\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "&lt;IPython.core.display.Markdown object&gt;\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gemini-2.5-flash', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(conten ... (truncated)\r\n",
      "&lt;IPython.core.display.Markdown object&gt;\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "&lt;IPython.core.display.Markdown object&gt;\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_daf5fcc80a', choices=[Choices(finish_reason='stop', index=0, message=M ... (truncated)\r\n",
      "\r\n",
      "## Messages formatting\r\n",
      "\r\n",
      "Let's start with making it easier to pass messages into litellm's `completion` function (including images).\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "def _mk_img(data:bytes)-&gt;tuple:\r\n",
      "    \"Convert image bytes to a base64 encoded image\"\r\n",
      "    img = base64.b64encode(data).decode(\"utf-8\")\r\n",
      "    mtype = mimetypes.types_map[\".\"+imghdr.what(None, h=data)]\r\n",
      "    return img, mtype\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "def _is_img(data): \r\n",
      "    return isinstance(data, bytes) and bool(imghdr.what(None, data))\r\n",
      "\r\n",
      "def _add_cache_control(msg,          # LiteLLM formatted msg\r\n",
      "                       cache=False,  # Enable Anthropic caching\r\n",
      "                       ttl=None):    # Cache TTL: '5m' (default) or '1h'\r\n",
      "    \"cache `msg` with default time-to-live (ttl) of 5minutes ('5m'), but can be set to '1h'.\"\r\n",
      "    if not cache: return msg\r\n",
      "    if isinstance(msg[\"content\"], str): \r\n",
      "        msg[\"content\"] = [{\"type\": \"text\", \"text\": msg[\"content\"]}]\r\n",
      "    cache_control = {\"type\": \"ephemeral\"}\r\n",
      "    if ttl is not None: cache_control[\"ttl\"] = ttl\r\n",
      "    if isinstance(msg[\"content\"], list) and msg[\"content\"]:\r\n",
      "        msg[\"content\"][-1][\"cache_control\"] = cache_control\r\n",
      "    return msg\r\n",
      "\r\n",
      "def _remove_cache_ckpts(msg):\r\n",
      "    \"remove unnecessary cache checkpoints.\"\r\n",
      "    if isinstance(msg[\"content\"], list) and msg[\"content\"]:\r\n",
      "        msg[\"content\"][-1].pop('cache_control', None)\r\n",
      "    return msg\r\n",
      "\r\n",
      "def _mk_content(o):\r\n",
      "    if isinstance(o, str): return {'type':'text','text':o.strip() or '.'}\r\n",
      "    if _is_img(o): \r\n",
      "        img, mtype = _mk_img(o)\r\n",
      "        return {\"type\": \"image_url\", \"image_url\": f\"data:{mtype};base64,{img}\"}\r\n",
      "    return o\r\n",
      "\r\n",
      "def mk_msg(content,      # Content: str, bytes (image), list of mixed content, or dict w 'role' and 'content' fields\r\n",
      "           role=\"user\",  # Message role if content isn't already a dict/Message\r\n",
      "           cache=False,  # Enable Anthropic caching\r\n",
      "           ttl=None):    # Cache TTL: '5m' (default) or '1h'\r\n",
      "    \"Create a LiteLLM compatible message.\"\r\n",
      "    if isinstance(content, dict) or isinstance(content, Message): return content\r\n",
      "    if isinstance(content, list) and len(content) == 1 and isinstance(content[0], str): c = content[0]\r\n",
      "    elif isinstance(content, list): c = [_mk_content(o) for o in content]\r\n",
      "    else: c = content\r\n",
      "    return _add_cache_control({\"role\": role, \"content\": c}, cache=cache, ttl=ttl)\r\n",
      "```\r\n",
      "\r\n",
      "Now we can use mk_msg to create different types of messages:\r\n",
      "\r\n",
      "Simple text:\r\n",
      "\r\n",
      "```python\r\n",
      "msg = mk_msg(\"hey\")\r\n",
      "msg\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "{'role': 'user', 'content': 'hey'}\r\n",
      "\r\n",
      "Lists w just one string element are flattened for conciseness:\r\n",
      "\r\n",
      "```python\r\n",
      "test_eq(mk_msg(\"hey\"), mk_msg([\"hey\"]))\r\n",
      "```\r\n",
      "\r\n",
      "With Anthropic caching:\r\n",
      "\r\n",
      "```python\r\n",
      "msg = mk_msg(\"hey I'm Rens. Please repeat it in all caps w a fun greeting\",cache=True)\r\n",
      "msg\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "{'role': 'user',\r\n",
      " 'content': [{'type': 'text',\r\n",
      "   'text': \"hey I'm Rens. Please repeat it in all caps w a fun greeting\",\r\n",
      "   'cache_control': {'type': 'ephemeral'}}]}\r\n",
      "\r\n",
      "(LiteLLM ignores these fields when sent to other providers)\r\n",
      "\r\n",
      "Text and images:\r\n",
      "\r\n",
      "```python\r\n",
      "fn = Path('samples/puppy.jpg')\r\n",
      "Image(filename=fn, width=200)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "&lt;IPython.core.display.Image object&gt;\r\n",
      "\r\n",
      "```python\r\n",
      "msg = mk_msg(['hey what in this image?',fn.read_bytes()])\r\n",
      "print(json.dumps(msg,indent=1)[:200]+\"...\")\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "{\r\n",
      " \"role\": \"user\",\r\n",
      " \"content\": [\r\n",
      "  {\r\n",
      "   \"type\": \"text\",\r\n",
      "   \"text\": \"hey what in this image?\"\r\n",
      "  },\r\n",
      "  {\r\n",
      "   \"type\": \"image_url\",\r\n",
      "   \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gxUSU ... (truncated)\r\n",
      "\r\n",
      "Which can be passed to litellm's `completion` function like this:\r\n",
      "\r\n",
      "```python\r\n",
      "model = ms[1]\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "completion(model,[msg])\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "Now lets make it easy to provide entire conversations:\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "def mk_msgs(msgs,                       # List of messages (each: str, bytes, list, or dict w 'role' and 'content' fields)\r\n",
      "            cache=False,                # Enable Anthropic caching\r\n",
      "            ttl=None,                   # Cache TTL: '5m' (default) or '1h'\r\n",
      "            cache_last_ckpt_only=True   # Only cache the last message\r\n",
      "           ):\r\n",
      "    \"Create a list of LiteLLM compatible messages.\"\r\n",
      "    if not msgs: return []\r\n",
      "    if not isinstance(msgs, list): msgs = [msgs]\r\n",
      "    res,role = [],'user'\r\n",
      "    for m in msgs:\r\n",
      "        res.append(msg:=mk_msg(m, role=role,cache=cache))\r\n",
      "        role = 'assistant' if msg['role'] in ('user','function', 'tool') else 'user'\r\n",
      "    if cache_last_ckpt_only: res = [_remove_cache_ckpts(m) for m in res]\r\n",
      "    if res and cache: res[-1] = _add_cache_control(res[-1], cache=cache, ttl=ttl)\r\n",
      "    return res\r\n",
      "```\r\n",
      "\r\n",
      "With `mk_msgs` you can easily provide a whole conversation:\r\n",
      "\r\n",
      "```python\r\n",
      "msgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"])\r\n",
      "msgs\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "[{'role': 'user', 'content': 'Hey!'},\r\n",
      " {'role': 'assistant', 'content': 'Hi there!'},\r\n",
      " {'role': 'user', 'content': 'How are you?'},\r\n",
      " {'role': 'assistant', 'content': \"I'm doing fine and you?\"}]\r\n",
      "\r\n",
      "Who's speaking at when is automatically inferred.\r\n",
      "Even when there are multiple tools being called in parallel (which LiteLLM supports!).\r\n",
      "\r\n",
      "```python\r\n",
      "msgs = mk_msgs(['Tell me the weather in Paris and Rome',\r\n",
      "                'Assistant calls weather tool two times',\r\n",
      "                {'role':'tool','content':'Weather in Paris is ...'},\r\n",
      "                {'role':'tool','content':'Weather in Rome is ...'},\r\n",
      "                'Assistant returns weather',\r\n",
      "                'Thanks!'])\r\n",
      "msgs\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "[{'role': 'user', 'content': 'Tell me the weather in Paris and Rome'},\r\n",
      " {'role': 'assistant', 'content': 'Assistant calls weather tool two times'},\r\n",
      " {'role': 'tool', 'content': 'Weather in Paris is .. ... (truncated)\r\n",
      "\r\n",
      "```python\r\n",
      "#| hide\r\n",
      "test_eq([m['role'] for m in msgs],['user','assistant','tool','tool','assistant','user'])\r\n",
      "```\r\n",
      "\r\n",
      "For ease of use, if `msgs` is not already in a `list`, it will automatically be wrapped inside one. This way you can pass a single prompt into `mk_msgs` and get back a LiteLLM compatible msg history.\r\n",
      "\r\n",
      "```python\r\n",
      "msgs = mk_msgs(\"Hey\")\r\n",
      "msgs\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "[{'role': 'user', 'content': 'Hey'}]\r\n",
      "\r\n",
      "```python\r\n",
      "#| hide\r\n",
      "msgs = mk_msgs({'role':'tool','content':'fake tool result'})\r\n",
      "msgs\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "[{'role': 'tool', 'content': 'fake tool result'}]\r\n",
      "\r\n",
      "```python\r\n",
      "msgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm fine, you?\"])\r\n",
      "msgs\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "[{'role': 'user', 'content': 'Hey!'},\r\n",
      " {'role': 'assistant', 'content': 'Hi there!'},\r\n",
      " {'role': 'user', 'content': 'How are you?'},\r\n",
      " {'role': 'assistant', 'content': \"I'm fine, you?\"}]\r\n",
      "\r\n",
      "However, beware that if you use `mk_msgs` for a single message, consisting of multiple parts.\r\n",
      "Then you should be explicit, and make sure to wrap those multiple messages in two lists:\r\n",
      "\r\n",
      "1. One list to show that they belong together in one message (the inner list).\r\n",
      "2. Another, because mk_msgs expects a list of multiple messages (the outer list).\r\n",
      "\r\n",
      "This is common when working with images for example:\r\n",
      "\r\n",
      "```python\r\n",
      "msgs = mk_msgs([['Whats in this img?',fn.read_bytes()]])\r\n",
      "print(json.dumps(msgs,indent=1)[:200]+\"...\")\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "[\r\n",
      " {\r\n",
      "  \"role\": \"user\",\r\n",
      "  \"content\": [\r\n",
      "   {\r\n",
      "    \"type\": \"text\",\r\n",
      "    \"text\": \"Whats in this img?\"\r\n",
      "   },\r\n",
      "   {\r\n",
      "    \"type\": \"image_url\",\r\n",
      "    \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD ... (truncated)\r\n",
      "\r\n",
      "## Streaming\r\n",
      "\r\n",
      "LiteLLM supports streaming responses. That's really useful if you want to show intermediate results, instead of having to wait until the whole response is finished.\r\n",
      "\r\n",
      "We create this helper function that returns the entire response at the end of the stream. This is useful when you want to store the whole response somewhere after having displayed the intermediate results.\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "def stream_with_complete(gen, postproc=noop):\r\n",
      "    \"Extend streaming response chunks with the complete response\"\r\n",
      "    chunks = []\r\n",
      "    for chunk in gen:\r\n",
      "        chunks.append(chunk)\r\n",
      "        yield chunk\r\n",
      "    postproc(chunks)\r\n",
      "    return stream_chunk_builder(chunks)\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "r = completion(model, mk_msgs(\"Hey!\"), stream=True)\r\n",
      "r2 = SaveReturn(stream_with_complete(r))\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "for o in r2:\r\n",
      "    cts = o.choices[0].delta.content\r\n",
      "    if cts: print(cts, end='')\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "Hello! How are you doing today? Is there anything I can help you with?\r\n",
      "\r\n",
      "```python\r\n",
      "r2.value\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "## Tools\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "def lite_mk_func(f):\r\n",
      "    if isinstance(f, dict): return f\r\n",
      "    return {'type':'function', 'function':get_schema(f, pname='parameters')}\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "def simple_add(\r\n",
      "    a: int,   # first operand\r\n",
      "    b: int=0  # second operand\r\n",
      ") -&gt; int:\r\n",
      "    \"Add two numbers together\"\r\n",
      "    return a + b\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "toolsc = lite_mk_func(simple_add)\r\n",
      "toolsc\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "{'type': 'function',\r\n",
      " 'function': {'name': 'simple_add',\r\n",
      "  'description': 'Add two numbers together\\n\\nReturns:\\n- type: integer',\r\n",
      "  'parameters': {'type': 'object',\r\n",
      "   'properties': {'a': {'type': 'i ... (truncated)\r\n",
      "\r\n",
      "```python\r\n",
      "tmsg = mk_msg(\"What is 5478954793+547982745? How about 5479749754+9875438979? Always use tools for calculations, and describe what you'll do before using a tool. Where multiple tool calls are required, do them in a single response where possible.\")\r\n",
      "r = completion(model, [tmsg], tools=[toolsc])\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "display(r)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message= ... (truncated)\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "def _lite_call_func(tc,ns,raise_on_err=True):\r\n",
      "    try: fargs = json.loads(tc.function.arguments)\r\n",
      "    except Exception as e: raise ValueError(f\"Failed to parse function arguments: {tc.function.arguments}\") from e\r\n",
      "    res = call_func(tc.function.name, fargs,ns=ns)\r\n",
      "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": str(res)}\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "tcs = [_lite_call_func(o, ns=globals()) for o in r.choices[0].message.tool_calls]\r\n",
      "tcs\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "[{'tool_call_id': 'toolu_013MZMqJL4fBRGTsMtAGJjMk',\r\n",
      "  'role': 'tool',\r\n",
      "  'name': 'simple_add',\r\n",
      "  'content': '6026937538'},\r\n",
      " {'tool_call_id': 'toolu_01HkbM4zwAb38n4rH7SNvi75',\r\n",
      "  'role': 'tool',\r\n",
      "  'name' ... (truncated)\r\n",
      "\r\n",
      "```python\r\n",
      "def delta_text(msg):\r\n",
      "    \"Extract printable content from streaming delta, return None if nothing to print\"\r\n",
      "    c = msg.choices[0]\r\n",
      "    if not c: return c\r\n",
      "    if not hasattr(c,'delta'): return None #f'{c}'\r\n",
      "    delta = c.delta\r\n",
      "    if delta.content: return delta.content\r\n",
      "    if delta.tool_calls:\r\n",
      "        res = ''.join(f\"ðŸ”§ {tc.function.name}\" for tc in delta.tool_calls if tc.id and tc.function.name)\r\n",
      "        if res: return f'\\n{res}\\n'\r\n",
      "    if hasattr(delta,'reasoning_content'): return 'ðŸ§ ' if delta.reasoning_content else '\\n\\n'\r\n",
      "    return None\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "r = completion(messages=[tmsg], model=model, stream=True, tools=[toolsc])\r\n",
      "r2 = SaveReturn(stream_with_complete(r))\r\n",
      "for o in r2: print(delta_text(o) or '', end='')\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "I'll help you calculate both of those sums using the addition tool. Let me perform both calculations for you:\r\n",
      "\r\n",
      "1. First, I'll calculate 5478954793 + 547982745\r\n",
      "2. Then, I'll calculate 5479749754 + 9875 ... (truncated)\r\n",
      "\r\n",
      "```python\r\n",
      "r2.value\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message= ... (truncated)\r\n",
      "\r\n",
      "```python\r\n",
      "msg = mk_msg(\"Solve this complex math problem: What is the derivative of x^3 + 2x^2 - 5x + 1?\")\r\n",
      "r = completion(messages=[msg], model=model, stream=True, reasoning_effort=\"low\")\r\n",
      "r2 = SaveReturn(stream_with_complete(r))\r\n",
      "for o in r2: print(delta_text(o) or '', end='')\r\n",
      "\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ ðŸ§ \r\n",
      "\r\n",
      "I'll find the derivative of f(x) = xÂ³ + 2xÂ² - 5x + 1 using the power rule.\r\n",
      "\r\n",
      "The power rule states that for any term ax^n, the derivative is nax^(n-1).\r\n",
      "\r\n",
      "Taking the derivative of eac ... (truncated)\r\n",
      "\r\n",
      "```python\r\n",
      "r2.value\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "## Search\r\n",
      "\r\n",
      "LiteLLM provides search, not via tools, but via the special `web_search_options` param.\r\n",
      "\r\n",
      "**Note:** Not all models support web search. LiteLLM's `supports_web_search` field should indicate this, but it's unreliable for some models like `claude-sonnet-4-20250514`. Checking both `supports_web_search` and `search_context_cost_per_query` provides more accurate detection.\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "def _has_search(m):\r\n",
      "    i = get_model_info(m)\r\n",
      "    return bool(i['search_context_cost_per_query'] or i['supports_web_search'])\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "for m in ms: print(m, _has_search(m))\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "gemini/gemini-2.5-flash True\r\n",
      "claude-sonnet-4-20250514 True\r\n",
      "openai/gpt-4.1 False\r\n",
      "\r\n",
      "\r\n",
      "When search is supported it can be used like this:\r\n",
      "\r\n",
      "```python\r\n",
      "smsg = mk_msg(\"Search the web and tell me very briefly about otters\")\r\n",
      "r = completion(model, [smsg], web_search_options={\"search_context_size\": \"low\"})  # or 'medium' / 'high'\r\n",
      "r\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "## Citations\r\n",
      "\r\n",
      "Next, lets handle Anthropic's search citations.\r\n",
      "\r\n",
      "When not using streaming, all citations are placed in a separate key in the response:\r\n",
      "\r\n",
      "```python\r\n",
      "r.choices[0].message.provider_specific_fields['citations'][0]\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "[{'type': 'web_search_result_location',\r\n",
      "  'cited_text': 'The charismatic otter, a member of the weasel family, is found on every continent except Australia and Antarctica. ',\r\n",
      "  'url': 'https://www.nat ... (truncated)\r\n",
      "\r\n",
      "However, when streaming the results are not captured this way.\r\n",
      "Instead, we provide this helper function that adds the citation to the `content` field in markdown format:\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "def cite_footnote(msg):\r\n",
      "    if not (delta:=nested_idx(msg, 'choices', 0, 'delta')): return\r\n",
      "    if citation:= nested_idx(delta, 'provider_specific_fields', 'citation'):\r\n",
      "        title = citation['title'].replace('\"', '\\\\\"')\r\n",
      "        delta.content = f'[*]({citation[\"url\"]} \"{title}\") '\r\n",
      "        \r\n",
      "def cite_footnotes(stream_list):\r\n",
      "    \"Add markdown footnote citations to stream deltas\"\r\n",
      "    for msg in stream_list: cite_footnote(msg)\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "r = list(completion(model, [smsg], stream=True, web_search_options={\"search_context_size\": \"low\"}))\r\n",
      "cite_footnotes(r)\r\n",
      "stream_chunk_builder(r)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "# Chat\r\n",
      "\r\n",
      "LiteLLM is pretty bare bones. It doesnt keep track of conversation history or what tools have been added in the conversation so far.\r\n",
      "\r\n",
      "So lets make a Claudette style wrapper so we can do streaming, toolcalling, and toolloops without problems.\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "effort = AttrDict({o[0]:o for o in ('low','medium','high')})\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "def _mk_prefill(pf): return ModelResponseStream([StreamingChoices(delta=Delta(content=pf,role='assistant'))])\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "class Chat:\r\n",
      "    def __init__(\r\n",
      "        self,\r\n",
      "        model:str,                # LiteLLM compatible model name \r\n",
      "        sp='',                    # System prompt\r\n",
      "        temp=0,                   # Temperature\r\n",
      "        search=False,             # Search (l,m,h), if model supports it\r\n",
      "        tools:list=None,          # Add tools\r\n",
      "        hist:list=None,           # Chat history\r\n",
      "        ns:Optional[dict]=None,   # Custom namespace for tool calling \r\n",
      "        cache=False               # Anthropic prompt caching\r\n",
      "    ):\r\n",
      "        \"LiteLLM chat client.\"\r\n",
      "        self.model = model\r\n",
      "        hist,tools = mk_msgs(hist),listify(tools)\r\n",
      "        if ns is None and tools: ns = mk_ns(tools)\r\n",
      "        elif ns is None: ns = globals()\r\n",
      "        self.tool_schemas = [lite_mk_func(t) for t in tools] if tools else None\r\n",
      "        store_attr()\r\n",
      "    \r\n",
      "    def _prep_msgs(self, msgs=None, prefill=None):\r\n",
      "        \"Prepare the messages list for the API call\"\r\n",
      "        sp = [{\"role\": \"system\", \"content\": self.sp}] if self.sp else []\r\n",
      "        if msgs: self.hist = mk_msgs(self.hist+(msgs if isinstance(msgs,list) else [msgs]), cache=self.cache)\r\n",
      "        pf = [{\"role\":\"assistant\",\"content\":prefill}] if prefill else []\r\n",
      "        return sp + self.hist + pf\r\n",
      "\r\n",
      "    def _call(self, msgs=None, prefill=None, temp=None, think=None, search=None, stream=False, max_steps=2, step=1, final_prompt=None, tool_choice=None, **kwargs):\r\n",
      "        \"Internal method that always yields responses\"\r\n",
      "        if step&gt;max_steps: return\r\n",
      "        if not get_model_info(self.model)[\"supports_assistant_prefill\"]: prefill=None\r\n",
      "        if _has_search(self.model) and (s:=ifnone(search,self.search)): kwargs['web_search_options'] = {\"search_context_size\": effort[s]}\r\n",
      "        else: _=kwargs.pop('web_search_options',None)\r\n",
      "        res = completion(model=self.model, messages=self._prep_msgs(msgs, prefill), stream=stream, \r\n",
      "                         tools=self.tool_schemas, reasoning_effort = effort.get(think), tool_choice=tool_choice,\r\n",
      "                         # temperature is not supported when reasoning\r\n",
      "                         temperature=None if think else ifnone(temp,self.temp),\r\n",
      "                         **kwargs)\r\n",
      "        if stream:\r\n",
      "            if prefill: yield _mk_prefill(prefill)\r\n",
      "            res = yield from stream_with_complete(res,postproc=cite_footnotes)\r\n",
      "        m = res.choices[0].message\r\n",
      "        if prefill: m.content = prefill + m.content\r\n",
      "        self.hist.append(m)\r\n",
      "        yield res\r\n",
      "\r\n",
      "        if tcs := m.tool_calls:\r\n",
      "            tool_results = [_lite_call_func(tc, ns=self.ns) for tc in tcs]\r\n",
      "            for r in tool_results: yield r\r\n",
      "            if step&gt;=max_steps-1:\r\n",
      "                tool_results += ([{\"role\": \"user\", \"content\": final_prompt}] if final_prompt else [])\r\n",
      "                tool_choice,search = 'none',False\r\n",
      "            yield from self._call(\r\n",
      "                tool_results, prefill, temp, think, search, stream, max_steps, step+1,\r\n",
      "                final_prompt, tool_choice, **kwargs)\r\n",
      "    \r\n",
      "    def __call__(self,\r\n",
      "                 msgs=None,         # List of messages, or single str if only one prompt\r\n",
      "                 prefill=None,      # Prefill AI response if model supports it\r\n",
      "                 temp=None,         # Override temp set on chat initialization\r\n",
      "                 think=None,        # Thinking (l,m,h)\r\n",
      "                 search=None,       # Override search set on chat initialization (l,m,h)\r\n",
      "                 stream=False,      # Stream results\r\n",
      "                 max_steps=1, # Maximum number of tool calls\r\n",
      "                 final_prompt=None, # Final prompt when tool calls have ran out \r\n",
      "                 return_all=False,  # Returns all intermediate ModelResponses if not streaming and has tool calls\r\n",
      "                 **kwargs):\r\n",
      "        \"Main call method - handles streaming vs non-streaming\"\r\n",
      "        result_gen = self._call(msgs, prefill, temp, think, search, stream, max_steps, 1, final_prompt, **kwargs)     \r\n",
      "        if stream: return result_gen              # streaming\r\n",
      "        elif return_all: return list(result_gen)  # toolloop behavior\r\n",
      "        else: return last(result_gen)             # normal chat behavior\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "@patch(as_prop=True)\r\n",
      "def cost(self: Chat):\r\n",
      "    \"Total cost of all responses in conversation history\"\r\n",
      "    return sum(getattr(r, '_hidden_params', {}).get('response_cost')  or 0\r\n",
      "               for r in self.h if hasattr(r, 'choices'))\r\n",
      "```\r\n",
      "\r\n",
      "## Examples\r\n",
      "\r\n",
      "### History tracking\r\n",
      "\r\n",
      "```python\r\n",
      "chat = Chat(model)\r\n",
      "res = chat(\"Hey my name is Rens\")\r\n",
      "res\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "```python\r\n",
      "chat(\"Whats my name\")\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "See now we keep track of history!\r\n",
      "\r\n",
      "### Prefill\r\n",
      "\r\n",
      "Prefill works as expected:\r\n",
      "\r\n",
      "```python\r\n",
      "chat(\"Spell my name\",prefill=\"Your name is R E\")\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "And the entire message is stored in the history, not just the generated part:\r\n",
      "\r\n",
      "```python\r\n",
      "chat.hist[-1]\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "Message(content='Your name is R E N S.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})\r\n",
      "\r\n",
      "### Streaming\r\n",
      "\r\n",
      "```python\r\n",
      "from time import sleep\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "chat = Chat(model)\r\n",
      "stream_gen = chat(\"Count to 5\", stream=True)\r\n",
      "for chunk in stream_gen:\r\n",
      "    if isinstance(chunk, ModelResponse): display(chunk)\r\n",
      "    else: print(delta_text(chunk) or '',end='')\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n",
      "5\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "Lets try prefill with streaming too:\r\n",
      "\r\n",
      "```python\r\n",
      "stream_gen = chat(\"Continue counting to 10\",\"Okay! 6, 7\",stream=True)\r\n",
      "for chunk in stream_gen:\r\n",
      "    if isinstance(chunk, ModelResponse): display(chunk)\r\n",
      "    else: print(delta_text(chunk) or '',end='')\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "Okay! 6, 7, 8, 9, 10.\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "### Tool use\r\n",
      "\r\n",
      "Ok now lets test tool use\r\n",
      "\r\n",
      "```python\r\n",
      "for m in ms:\r\n",
      "    display(Markdown(f'**{m}:**'))\r\n",
      "    chat = Chat(m, tools=[simple_add])\r\n",
      "    res = chat(\"What's 5 + 3? Use the `simple_add` tool. Explain.\")\r\n",
      "    display(res)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "&lt;IPython.core.display.Markdown object&gt;\r\n",
      "{'tool_call_id': 'call_258912f94671431283c59d9ea96f',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'simple_add',\r\n",
      " 'content': '8'}\r\n",
      "&lt;IPython.core.display.Markdown object&gt;\r\n",
      "{'tool_call_id': 'toolu_01V3fMovcMH9s8K1UijEz7LR',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'simple_add',\r\n",
      " 'content': '8'}\r\n",
      "&lt;IPython.core.display.Markdown object&gt;\r\n",
      "{'tool_call_id': 'call_ayQF75Sk1sUCs7ipHHJlgZYC',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'simple_add',\r\n",
      " 'content': '8'}\r\n",
      "\r\n",
      "### Thinking w tool use\r\n",
      "\r\n",
      "```python\r\n",
      "chat = Chat(model, tools=[simple_add])\r\n",
      "res = chat(\"What's 5 + 3?\",think='l',return_all=True)\r\n",
      "display(*res)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message= ... (truncated)\r\n",
      "{'tool_call_id': 'toolu_01QBLkGLtd85Yj8eEiFVBu4v',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'simple_add',\r\n",
      " 'content': '8'}\r\n",
      "\r\n",
      "### Search\r\n",
      "\r\n",
      "```python\r\n",
      "chat = Chat(model)\r\n",
      "res = chat(\"Search the web and tell me very briefly about otters\", search='l', stream=True)\r\n",
      "for o in res:\r\n",
      "    if isinstance(o, ModelResponse): sleep(0.01); display(o)\r\n",
      "    else: print(delta_text(o) or '',end='')\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "Otters are charismatic members of the weasel family, found on every continent except Australia and Antarctica. There are 13-14 species in total, ranging from the Asian small-clawed otter (smallest) to ... (truncated)\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "### Multi tool calling\r\n",
      "\r\n",
      "We can let the model call multiple tools in sequence using the `max_steps` parameter.\r\n",
      "\r\n",
      "```python\r\n",
      "chat = Chat(model, tools=[simple_add])\r\n",
      "res = chat(\"What's ((5 + 3)+7)+11? Work step by step\", return_all=True, max_steps=5)\r\n",
      "for r in res: display(r)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message= ... (truncated)\r\n",
      "{'tool_call_id': 'toolu_01PbJNpks9G6o9UYjRs5nd4b',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'simple_add',\r\n",
      " 'content': '8'}\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message= ... (truncated)\r\n",
      "{'tool_call_id': 'toolu_01PAuvDwaBNgM2JbEwJwxov5',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'simple_add',\r\n",
      " 'content': '15'}\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message= ... (truncated)\r\n",
      "{'tool_call_id': 'toolu_01KeELPbBSpCePrk4c8EzbyB',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'simple_add',\r\n",
      " 'content': '26'}\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "Some models support parallel tool calling. I.e. sending multiple tool call requests in one conversation step.\r\n",
      "\r\n",
      "```python\r\n",
      "def multiply(a: int, b: int) -&gt; int:\r\n",
      "    \"Multiply two numbers\"\r\n",
      "    return a * b\r\n",
      "\r\n",
      "chat = Chat('openai/gpt-4.1', tools=[simple_add, multiply])\r\n",
      "res = chat(\"Calculate (5 + 3) * (7 + 2)\", max_steps=5, return_all=True)\r\n",
      "for r in res: display(r)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_3502f4eb73', choices=[Choices(finish_reason='tool_calls', index=0, mes ... (truncated)\r\n",
      "{'tool_call_id': 'call_8ZH9yZH3N9TEmaIOmV6H81mU',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'simple_add',\r\n",
      " 'content': '8'}\r\n",
      "{'tool_call_id': 'call_uDxT76wyUI8WY9ugRPwCnlb9',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'simple_add',\r\n",
      " 'content': '9'}\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_3502f4eb73', choices=[Choices(finish_reason='tool_calls', index=0, mes ... (truncated)\r\n",
      "{'tool_call_id': 'call_Dm0jjJPQf2elRy4z7yBhOMf5',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'multiply',\r\n",
      " 'content': '72'}\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='gpt-4.1-2025-04-14', object='chat.completion', system_fingerprint='fp_3502f4eb73', choices=[Choices(finish_reason='stop', index=0, message=M ... (truncated)\r\n",
      "\r\n",
      "See it did the additions in one go!\r\n",
      "\r\n",
      "We don't want the model to keep running tools indefinitely. Lets showcase how we can force thee model to stop after our specified number of toolcall rounds:\r\n",
      "\r\n",
      "```python\r\n",
      "def divide(a: int, b: int) -&gt; float:\r\n",
      "    \"Divide two numbers\"\r\n",
      "    return a / b\r\n",
      "\r\n",
      "chat = Chat(model, tools=[simple_add, multiply, divide])\r\n",
      "res = chat(\"Calculate ((10 + 5) * 3) / (2 + 1) step by step.\", \r\n",
      "           max_steps=3, return_all=True,\r\n",
      "           final_prompt=\"Please wrap-up for now and summarize how far we got.\")\r\n",
      "for r in res: display(r)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message= ... (truncated)\r\n",
      "{'tool_call_id': 'toolu_013mAGGxvDDy3ftqSS3u2qPh',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'simple_add',\r\n",
      " 'content': '15'}\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message= ... (truncated)\r\n",
      "{'tool_call_id': 'toolu_019KZ7aFBcAoHieWEFX6hYwA',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'simple_add',\r\n",
      " 'content': '3'}\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "```python\r\n",
      "#| hide\r\n",
      "test_eq(len([o for o in res if isinstance(o,ModelResponse)]),3)\r\n",
      "```\r\n",
      "\r\n",
      "# Async\r\n",
      "\r\n",
      "If you want to use LiteLLM in a webapp you probably want to use their async function `acompletion`.\r\n",
      "To make that easier we will implement our version of `AsyncChat` to complement it. It follows the same implementation as Chat as much as possible:\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "async def _alite_call_func(tc, ns, raise_on_err=True):\r\n",
      "    try: fargs = json.loads(tc.function.arguments)\r\n",
      "    except Exception as e: raise ValueError(f\"Failed to parse function arguments: {tc.function.arguments}\") from e\r\n",
      "    res = await call_func_async(tc.function.name, fargs, ns=ns)\r\n",
      "    return {\"tool_call_id\": tc.id, \"role\": \"tool\", \"name\": tc.function.name, \"content\": str(res)}\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| hide export\r\n",
      "@asave_iter\r\n",
      "async def astream_with_complete(self, agen, postproc=noop):\r\n",
      "    chunks = []\r\n",
      "    async for chunk in agen:\r\n",
      "        chunks.append(chunk)\r\n",
      "        postproc(chunk)\r\n",
      "        yield chunk\r\n",
      "    self.value = stream_chunk_builder(chunks)\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "class AsyncChat(Chat):\r\n",
      "    async def _call(self, msgs=None, prefill=None, temp=None, think=None, search=None, stream=False, max_steps=1, step=1, final_prompt=None, tool_choice=None, **kwargs):\r\n",
      "        if step&gt;max_steps+1: return\r\n",
      "        if not get_model_info(self.model)[\"supports_assistant_prefill\"]: prefill=None\r\n",
      "        if _has_search(self.model) and (s:=ifnone(search,self.search)): kwargs['web_search_options'] = {\"search_context_size\": effort[s]}\r\n",
      "        else: _=kwargs.pop('web_search_options',None)\r\n",
      "        res = await acompletion(model=self.model, messages=self._prep_msgs(msgs, prefill), stream=stream,\r\n",
      "                         tools=self.tool_schemas, reasoning_effort=effort.get(think), tool_choice=tool_choice,\r\n",
      "                         # temperature is not supported when reasoning\r\n",
      "                         temperature=None if think else ifnone(temp,self.temp), \r\n",
      "                         **kwargs)\r\n",
      "        if stream:\r\n",
      "            if prefill: yield _mk_prefill(prefill)\r\n",
      "            res = astream_with_complete(res,postproc=cite_footnote)\r\n",
      "            async for chunk in res: yield chunk\r\n",
      "            res = res.value\r\n",
      "        m=res.choices[0].message\r\n",
      "        if prefill: m.content = prefill + m.content\r",
      "\r\n",
      "        yield res\r\n",
      "        self.hist.append(m)\r\n",
      "\r\n",
      "        if tcs := m.tool_calls:\r\n",
      "            tool_results = []\r\n",
      "            for tc in tcs:\r\n",
      "                result = await _alite_call_func(tc, ns=self.ns)\r\n",
      "                tool_results.append(result)\r\n",
      "                yield result\r\n",
      "            \r\n",
      "            if step&gt;=max_steps-1:\r\n",
      "                tool_results += ([{\"role\": \"user\", \"content\": final_prompt}] if final_prompt else [])\r\n",
      "                tool_choice,search = 'none',False\r\n",
      "            \r\n",
      "            async for result in self._call(\r\n",
      "                tool_results, prefill, temp, think, search, stream, max_steps, step+1,\r\n",
      "                final_prompt, tool_choice=tool_choice, **kwargs):\r\n",
      "                    yield result\r\n",
      "    \r\n",
      "    async def __call__(self,\r\n",
      "                       msgs=None,         # List of messages, or single str if only one prompt\r\n",
      "                       prefill=None,      # Prefill AI response if model supports it\r\n",
      "                       temp=None,         # Override temp set on chat initialization\r\n",
      "                       think=None,        # Thinking (l,m,h)\r\n",
      "                       search=None,       # Override search set on chat initialization (l,m,h)\r\n",
      "                       stream=False,      # Stream results\r\n",
      "                       max_steps=1, # Maximum number of tool calls\r\n",
      "                       final_prompt=None, # Final prompt when tool calls have ran out \r\n",
      "                       return_all=False,  # Returns all intermediate ModelResponses if not streaming and has tool calls\r\n",
      "                       **kwargs):\r\n",
      "        result_gen = self._call(msgs, prefill, temp, think, search, stream, max_steps, 1, final_prompt, **kwargs)\r\n",
      "        if stream or return_all: return result_gen\r\n",
      "        async for res in result_gen: pass\r\n",
      "        return res # normal chat behavior only return last msg\r\n",
      "```\r\n",
      "\r\n",
      "## Examples\r\n",
      "\r\n",
      "Basic example\r\n",
      "\r\n",
      "```python\r\n",
      "chat = AsyncChat(model)\r\n",
      "await chat(\"What is 2+2?\")\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "With tool calls\r\n",
      "\r\n",
      "```python\r\n",
      "async def async_add(a: int, b: int) -&gt; int:\r\n",
      "    \"Add two numbers asynchronously\"\r\n",
      "    await asyncio.sleep(0.1)\r\n",
      "    return a + b\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "chat_with_tools = AsyncChat(model, tools=[async_add])\r\n",
      "res = await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\", return_all=True)\r\n",
      "async for r in res: display(r)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message= ... (truncated)\r\n",
      "{'tool_call_id': 'toolu_01X1GDcGawNLq77xytxEGDBw',\r\n",
      " 'role': 'tool',\r\n",
      " 'name': 'async_add',\r\n",
      " 'content': '12'}\r\n",
      "ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-20250514', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Messag ... (truncated)\r\n",
      "\r\n",
      "## Async Streaming Display\r\n",
      "\r\n",
      "This is what our outputs look like with streaming results:\r\n",
      "\r\n",
      "```python\r\n",
      "chat_with_tools = AsyncChat(model, tools=[async_add])\r\n",
      "res = await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\r\n",
      "async for o in res: \r\n",
      "    if isinstance(o,ModelResponseStream): print(delta_text(o) or '',end='')\r\n",
      "    elif isinstance(o,dict): print(o)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "I'll use the async_add tool to calculate 5 + 7 for you.\r\n",
      "ðŸ”§ async_add\r\n",
      "{'tool_call_id': 'toolu_01NTrwnCZYd6K7ZeNQtUyx9k', 'role': 'tool', 'name': 'async_add', 'content': '12'}\r\n",
      "The result of 5 + 7 is 12.\r\n",
      "\r\n",
      "We use this one quite a bit so we want to provide some utilities to better format these outputs:\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "def _clean_str(text):\r\n",
      "    \"Clean content to prevent breaking surrounding markdown formatting.\"\r\n",
      "    return escape(str(text)).replace('`', '').replace('\\n', ' ').replace('|', ' ')\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "def _trunc_str(s, mx=2000, replace=\"â€¦\"):\r\n",
      "    \"Truncate `s` to `mx` chars max, adding `replace` if truncated\"\r\n",
      "    s = str(s).strip()\r\n",
      "    return s[:mx]+replace if len(s)&gt;mx else s\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "async def aformat_stream(rs):\r\n",
      "    \"Format the response stream for markdown display.\"\r\n",
      "    think = False\r\n",
      "    async for o in rs:\r\n",
      "        if isinstance(o, ModelResponseStream):\r\n",
      "            d = o.choices[0].delta\r\n",
      "            if nested_idx(d, 'reasoning_content'): \r\n",
      "                think = True\r\n",
      "                yield 'ðŸ§ '\r\n",
      "            elif think:\r\n",
      "                think = False\r\n",
      "                yield '\\n\\n'\r\n",
      "            if c := d.content: yield c\r\n",
      "        elif isinstance(o, ModelResponse) and (c := getattr(o.choices[0].message, 'tool_calls', None)):\r\n",
      "            fn = first(c).function\r\n",
      "            yield f\"\\n&lt;details class='tool-usage-details'&gt;\\n\\n `{fn.name}({_trunc_str(fn.arguments)})`\\n\"\r\n",
      "        elif isinstance(o, dict) and 'tool_call_id' in o: \r\n",
      "            yield f\"  - `{_trunc_str(_clean_str(o.get('content')))}`\\n\\n&lt;/details&gt;\\n\\n\"\r\n",
      "```\r\n",
      "\r\n",
      "```python\r\n",
      "#| export\r\n",
      "async def adisplay_stream(rs):\r\n",
      "    \"Use IPython.display to markdown display the response stream.\"\r\n",
      "    md = ''\r\n",
      "    async for o in aformat_stream(rs): \r\n",
      "        md+=o\r\n",
      "        display(Markdown(md),clear=True)\r\n",
      "```\r\n",
      "\r\n",
      "## Streaming examples\r\n",
      "\r\n",
      "Now we can demonstrate `AsynChat` with `stream=True`!\r\n",
      "\r\n",
      "### Tool call\r\n",
      "\r\n",
      "```python\r\n",
      "chat = AsyncChat(model, tools=[async_add])\r\n",
      "res = await chat(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\r\n",
      "await adisplay_stream(res)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "&lt;IPython.core.display.Markdown object&gt;\r\n",
      "\r\n",
      "### Thinking tool call\r\n",
      "\r\n",
      "```python\r\n",
      "chat = AsyncChat(model)\r\n",
      "res = await chat(\"Briefly, what's the most efficient way to sort a list of 1000 random integers?\", think='l',stream=True)\r\n",
      "await adisplay_stream(res)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "&lt;IPython.core.display.Markdown object&gt;\r\n",
      "\r\n",
      "### Multiple tool calls\r\n",
      "\r\n",
      "```python\r\n",
      "#| hide\r\n",
      "chat = AsyncChat(model, tools=[simple_add, multiply, divide])\r\n",
      "res = await chat(\"Calculate ((10 + 5) * 3) / (2 + 1) Explain step by step.\", \r\n",
      "           max_steps=3, stream=True,\r\n",
      "           final_prompt=\"Please wrap-up for now and summarize how far we got.\")\r\n",
      "await adisplay_stream(res)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "&lt;IPython.core.display.Markdown object&gt;\r\n",
      "\r\n",
      "### Search\r\n",
      "\r\n",
      "```python\r\n",
      "chat_stream_tools = AsyncChat(model, search='l')\r\n",
      "res = await chat_stream_tools(\"Search the web and tell me very briefly about otters\", stream=True)\r\n",
      "await adisplay_stream(res)\r\n",
      "```\r\n",
      "\r\n",
      "**Outputs:**\r\n",
      "&lt;IPython.core.display.Markdown object&gt;\r\n",
      "\r\n",
      "# Export -\r\n",
      "\r\n",
      "```python\r\n",
      "#| hide\r\n",
      "import nbdev; nbdev.nbdev_export()\r\n",
      "```\r\n",
      "</document_content>\r\n",
      "</document>\r\n",
      "</documents>"
     ]
    }
   ],
   "source": [
    "# XML dump of the core notebook (which is exported into the core lisette module)\n",
    "!cat _justme/_core/outp.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6eb57a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "19c02966",
   "metadata": {},
   "source": [
    "# README"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce0b522",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from lisette.core import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3853fe13",
   "metadata": {},
   "source": [
    "# lisette\n",
    "\n",
    "> litellm helper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db746464",
   "metadata": {},
   "source": [
    "This file will become your README and also the index of your documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d7f43b5",
   "metadata": {},
   "source": [
    "## Developer Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9babfbe9",
   "metadata": {},
   "source": [
    "If you are new to using `nbdev` here are some useful pointers to get you started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5a24f5",
   "metadata": {},
   "source": [
    "### Install lisette in Development mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc5cb2d",
   "metadata": {},
   "source": [
    "```sh\n",
    "# make sure lisette package is installed in development mode\n",
    "$ pip install -e .\n",
    "\n",
    "# make changes under nbs/ directory\n",
    "# ...\n",
    "\n",
    "# compile to have changes apply to lisette\n",
    "$ nbdev_prepare\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb03ee8f",
   "metadata": {},
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd061cf",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4351d4",
   "metadata": {},
   "source": [
    "Install latest from the GitHub [repository][repo]:\n",
    "\n",
    "```sh\n",
    "$ pip install git+https://github.com/AnswerDotAI/lisette.git\n",
    "```\n",
    "\n",
    "or from [conda][conda]\n",
    "\n",
    "```sh\n",
    "$ conda install -c AnswerDotAI lisette\n",
    "```\n",
    "\n",
    "or from [pypi][pypi]\n",
    "\n",
    "\n",
    "```sh\n",
    "$ pip install lisette\n",
    "```\n",
    "\n",
    "\n",
    "[repo]: https://github.com/AnswerDotAI/lisette\n",
    "[docs]: https://AnswerDotAI.github.io/lisette/\n",
    "[pypi]: https://pypi.org/project/lisette/\n",
    "[conda]: https://anaconda.org/AnswerDotAI/lisette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ee36bb",
   "metadata": {},
   "source": [
    "### Documentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b3696a8",
   "metadata": {},
   "source": [
    "Documentation can be found hosted on this GitHub [repository][repo]'s [pages][docs]. Additionally you can find package manager specific guidelines on [conda][conda] and [pypi][pypi] respectively.\n",
    "\n",
    "[repo]: https://github.com/AnswerDotAI/lisette\n",
    "[docs]: https://AnswerDotAI.github.io/lisette/\n",
    "[pypi]: https://pypi.org/project/lisette/\n",
    "[conda]: https://anaconda.org/AnswerDotAI/lisette"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5819d5",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77984c3d",
   "metadata": {},
   "source": [
    "Fill me in please! Don't forget code examples:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aef1ec63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f1757ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
