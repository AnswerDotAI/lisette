{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd7861e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from cachy import enable_cachy,disable_cachy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f7e5b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "enable_cachy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82380377",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio, base64, json, litellm, mimetypes, random, string, ast\n",
    "from typing import Optional,Callable\n",
    "from html import escape\n",
    "from litellm import (acompletion, completion, stream_chunk_builder, Message,\n",
    "                     ModelResponse, ModelResponseStream, get_model_info, register_model, Usage)\n",
    "from litellm.utils import function_to_dict, StreamingChoices, Delta, ChatCompletionMessageToolCall, Function, Choices\n",
    "from toolslm.funccall import mk_ns, call_func, call_func_async, get_schema\n",
    "from fastcore.utils import *\n",
    "from fastcore.meta import delegates\n",
    "from fastcore import imghdr\n",
    "from dataclasses import dataclass\n",
    "from litellm.exceptions import ContextWindowExceededError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a801c2a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "from IPython.display import Markdown, Image, Audio, Video\n",
    "import httpx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa1fb99",
   "metadata": {},
   "source": [
    "### Caching"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eaf8f0d",
   "metadata": {},
   "source": [
    "#### Anthropic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e071882",
   "metadata": {},
   "source": [
    "We use explicit caching via cache control checkpoints. Anthropic requires exact match with cached tokens and even a small change results in cache invalidation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64ed32f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "disable_cachy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32340dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| notest\n",
    "a,b = random.randint(0,100), random.randint(0,100)\n",
    "hist = [[f\"What is {a}+{b}?\\n\" * 250], f\"It's {a+b}\", ['hi'], \"Hello\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd0c6f1c",
   "metadata": {},
   "source": [
    "In this first api call we will see cache creation until the last user msg:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6160c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| notest\n",
    "# # TODO: flaky\n",
    "# sleep(5)\n",
    "# chat = AsyncChat(ms[3], cache=True, hist=hist)\n",
    "# rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n",
    "# async for o in rs: \n",
    "#     if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0b68f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| notest\n",
    "# # TODO: flaky\n",
    "# test_eq(o.usage.cache_creation_input_tokens > 1000, True)\n",
    "# test_eq(o.usage.cache_read_input_tokens, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4a63ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| notest\n",
    "# # TODO: flaky\n",
    "# hist.extend([['hi again'], 'how may i help you?'])\n",
    "# chat = AsyncChat(ms[3], cache=True, hist=hist)\n",
    "# rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n",
    "# async for o in rs:\n",
    "#     if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "568ebb7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| notest\n",
    "# # TODO: flaky\n",
    "# test_eq(o.usage.cache_read_input_tokens > 1000, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2096b34",
   "metadata": {},
   "source": [
    "The subsequent call should re-use the existing cache:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d469b523",
   "metadata": {},
   "source": [
    "#### Gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118670c9",
   "metadata": {},
   "source": [
    "Gemini implicit caching supports partial token matches. The usage metadata only shows cache hits with the `cached_tokens` field. So, to view them we need to run completions at least twice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfdad46",
   "metadata": {},
   "source": [
    "Testing with `gemini-2.5-flash` until `gemini-3-pro-preview` is more reliable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6215649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| notest\n",
    "# # TODO: flaky\n",
    "# chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "# rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n",
    "# async for o in rs: \n",
    "#     if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd85cb9",
   "metadata": {},
   "source": [
    "Running the same completion again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa84445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| notest\n",
    "# # TODO: flaky\n",
    "# sleep(5) # it takes a while for cached tokens to be avail.\n",
    "# chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "# rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n",
    "# async for o in rs: \n",
    "#     if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbff9056",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| notest\n",
    "# # TODO: flaky\n",
    "# test_eq(o.usage.prompt_tokens_details.cached_tokens > 1800, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78a1d56",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| notest\n",
    "# # TODO: flaky\n",
    "# hist.extend([['hi again'], 'how may i help you?'])\n",
    "# chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "# rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n",
    "# async for o in rs:\n",
    "#     if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "024f5e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| notest\n",
    "# # TODO: flaky\n",
    "# test_eq(o.usage.prompt_tokens_details.cached_tokens > 1800, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0806d459",
   "metadata": {},
   "source": [
    "Let's modify the cached content and see that partial matching works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b88eb842",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| notest\n",
    "# # TODO: flaky\n",
    "# c = hist[0][0]\n",
    "# hist[0][0] = c[:int(len(c)*0.75)] + \" Some extra text\"\n",
    "# hist.extend([['hi again'], 'how may i help you?'])\n",
    "# chat = AsyncChat(ms[2], cache=True, hist=hist)\n",
    "# rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n",
    "# async for o in rs:\n",
    "#     if isinstance(o, ModelResponse): print(o.usage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f82575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #| notest\n",
    "# # # TODO: flaky\n",
    "# test_eq(o.usage.prompt_tokens_details.cached_tokens > 900, True)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
