[
  {
    "objectID": "usage.html",
    "href": "usage.html",
    "title": "Usage",
    "section": "",
    "text": "import litellm, importlib, httpx\nfrom lisette.core import Chat, AsyncChat, patch_litellm\nfrom cachy import enable_cachy, disable_cachy\nfrom fastcore.test import *",
    "crumbs": [
      "Usage"
    ]
  },
  {
    "objectID": "usage.html#lisette-usage-logger",
    "href": "usage.html#lisette-usage-logger",
    "title": "Usage",
    "section": "Lisette Usage Logger",
    "text": "Lisette Usage Logger\n\n_ = importlib.reload(litellm) # to re-run the notebook without kernel restart\n\n\n# litellm._turn_on_debug()\n\n\npatch_litellm()\n\n\nsource\n\nUsage\n\ndef Usage(\n    args:VAR_POSITIONAL, kwargs:VAR_KEYWORD\n):\n\nInitialize self. See help(type(self)) for accurate signature.\nAnthropic provides web search request counts directly via usage.server_tool_use.web_search_requests, billed at $10 per 1,000 searches (pricing). Gemini returns queries in groundingMetadata.webSearchQueries‚Äîeach query counts as a separate billable use‚Äîwith 5,000 free prompts per month, then $14 per 1,000 search queries (coming soon) (pricing, grounding docs).\n\nsource\n\n\nsearch_count\n\ndef search_count(\n    r\n):\n\nThe precomputed response cost provided is available in kwargs['response_cost'] according to the litellm docs:\n\nsource\n\n\nLisetteUsageLogger\n\ndef LisetteUsageLogger(\n    db_path\n):\n\nArgs: turn_off_message_logging: bool - if True, the message logging will be turned off. Message and response will be redacted from StandardLoggingPayload. message_logging: bool - deprecated param, use turn_off_message_logging instead",
    "crumbs": [
      "Usage"
    ]
  },
  {
    "objectID": "usage.html#cost-utils",
    "href": "usage.html#cost-utils",
    "title": "Usage",
    "section": "Cost Utils",
    "text": "Cost Utils\n\nclass PrefixDict(dict):\n    def __getitem__(self, key):\n        if key in self.keys(): return super().__getitem__(key)\n        for k in self.keys(): \n            if key.startswith(k): return super().__getitem__(k)\n        raise KeyError(key)\n\n\nmodel_prices = PrefixDict({\n    'claude-sonnet-4-5': dict(input_prc = 3/1e6, cache_write_prc = 3.75/1e6, cache_read_prc = 0.3/1e6, output_prc = 15/1e6, web_search_prc = 10/1e3)\n})\n\nSimplified cost utils to demonstrate total cost calculation (use Usage.response_cost in prod):\n\n@patch(as_prop=True)\ndef inp_cost(self:Usage):         return model_prices[self.model]['input_prc'] * (self.prompt_tokens - self.cache_read_tokens)\n@patch(as_prop=True)\ndef cache_write_cost(self:Usage): return model_prices[self.model]['cache_write_prc'] * self.cache_creation_tokens\n@patch(as_prop=True)\ndef cache_read_cost(self:Usage):  return model_prices[self.model]['cache_read_prc'] * self.cache_read_tokens\n@patch(as_prop=True)\ndef out_cost(self:Usage):         return model_prices[self.model]['output_prc'] * self.completion_tokens\n@patch(as_prop=True)\ndef web_cost(self:Usage):         return model_prices[self.model]['web_search_prc'] * ifnone(self.web_search_requests, 0)\n@patch(as_prop=True)\ndef cost(self:Usage):             return self.inp_cost + self.cache_write_cost + self.cache_read_cost + self.out_cost + self.web_cost\n\nA mapping of model pricing is also available in litellm, which is used to calculate the response_cost\n\nmodel_pricing = dict2obj(httpx.get(litellm.model_cost_map_url).json())\n\n\n# model_pricing['claude-sonnet-4-5']\n\n\n# model_pricing['gemini-3-pro-preview']",
    "crumbs": [
      "Usage"
    ]
  },
  {
    "objectID": "usage.html#examples",
    "href": "usage.html#examples",
    "title": "Usage",
    "section": "Examples",
    "text": "Examples\n\nfrom tempfile import NamedTemporaryFile\ntf =NamedTemporaryFile(suffix='.db')\n\n\n@patch\ndef user_id_fn(self:LisetteUsageLogger): return 'user-123'\ntf=NamedTemporaryFile(suffix='.db')\nlogger = LisetteUsageLogger(tf.name)\nlitellm.callbacks = [logger]\n\n\nslc = ','.join('id model user_id prompt_tokens completion_tokens total_tokens cached_tokens cache_creation_tokens cache_read_tokens web_search_requests response_cost'.split())\n\n\n# litellm.set_verbose = True\n\nA simple example:\n\nchat = Chat('claude-sonnet-4-5-20250929')\nr = chat(\"What is 2+2?\")\n\n\ntime.sleep(3) # wait for callback db write\nu = logger.usage(select=slc)[-1]; u\n\nUsage(id=1, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=14, completion_tokens=11, total_tokens=25, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=0, response_cost=0.000207)\n\n\nOur calculated cost matches litellm‚Äôs response_cost. In some cases it might be better to use the custom calculation as we‚Äôll see in the remaining of this notebook:\n\ntest_eq(u.cost, u.response_cost)\n\nNow, let‚Äôs test with streaming:\n\nchat = Chat('claude-sonnet-4-5')\nres = chat(\"Count from 1 to 5\", stream=True)\nfor o in res: pass\n\n\ntime.sleep(3)\nu = logger.usage(select=slc)[-1]; u\n\nUsage(id=2, timestamp=UNSET, model='claude-sonnet-4-5', user_id='user-123', prompt_tokens=15, completion_tokens=17, total_tokens=32, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=0, response_cost=0.00030000000000000003)\n\n\n\ntest_eq(u.cost, u.response_cost)\n\nStreaming logged successfully. Let‚Äôs also verify async chat calls are logged properly.\n\nchat_async = AsyncChat('claude-sonnet-4-5-20250929')\nawait chat_async(\"What is 3+3?\")\n\n3 + 3 = 6\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=13, prompt_tokens=14, total_tokens=27, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\ntime.sleep(3)\nu = logger.usage(select=slc)[-1]; u\n\nUsage(id=3, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=14, completion_tokens=13, total_tokens=27, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=0, response_cost=0.00023700000000000001)\n\n\n\ntest_eq(u.cost, u.response_cost)\n\nFinally, let‚Äôs test async streaming to ensure all API patterns are covered.\n\nres = await chat_async(\"Count from 10 to 15\", stream=True)\nasync for o in res: pass\nprint(o)\n\nModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5-20250929', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='stop', index=0, message=Message(content='10, 11, 12, 13, 14, 15', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=20, prompt_tokens=38, total_tokens=58, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None))\n\n\n\ntime.sleep(3)\nu = logger.usage(select=slc)[-1]; u\n\nUsage(id=4, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=38, completion_tokens=20, total_tokens=58, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=0, response_cost=0.00041400000000000003)\n\n\n\ntest_eq(u.cost, u.response_cost)\n\n\nSearch\nNow let‚Äôs run a prompt with web search:\n\nflash = 'gemini/gemini-3-flash-preview'\nchat = Chat(flash)\nchat(\"What is the weather like in NYC? Search web.\", search=\"m\")\n\nNew York City is currently experiencing a severe winter weather event. As of Friday, February 6, 2026, the city is under a Weather Alert issued by NYC Emergency Management due to dangerously cold temperatures, strong winds, and snow.\n\nCurrent Conditions & Immediate Forecast\n\nTemperature: Currently around 31¬∞F (-1¬∞C), but temperatures are expected to drop steadily throughout the day.\nSnow: Light snow and flurries are expected this afternoon and evening, with accumulations of 0.5 to 1 inch likely by Saturday morning.\nWind: An arctic cold front is moving in, which will bring a rapid increase in wind speeds tonight.\n\n\n\nExtreme Cold Warning (Saturday ‚Äì Sunday)\nThe National Weather Service has issued an Extreme Cold Warning effective from 10:00 AM Saturday, Feb 7, through 1:00 PM Sunday, Feb 8. * Wind Chills: Dangerously cold wind chills could reach as low as -20¬∞F (-29¬∞C). At these temperatures, frostbite can occur on exposed skin in as little as 5 minutes. * Winds: Sustained winds of 20‚Äì30 mph are expected Saturday, with gusts up to 50 mph, potentially causing blowing snow and hazardous travel conditions. * Low Temperatures: Saturday night is forecast to reach a low of approximately 5¬∞F (-15¬∞C).\n\n\nCity Response & Safety\n\nWarming Centers: The city has opened warming centers in all five boroughs. You can find locations by calling 311 or visiting the NYC Warming Centers website.\nCode Blue: An ‚ÄúEnhanced Code Blue‚Äù is in effect to protect vulnerable populations and those experiencing homelessness.\nTravel: Officials urge New Yorkers to stay indoors if possible. If you must go out, dress in multiple warm layers and cover all exposed skin.\n\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=438, prompt_tokens=12, total_tokens=450, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=12, image_tokens=None))\n\n\n\n\n\n\ntime.sleep(3)\nu = logger.usage(select=slc)[-1]; u\n\nUsage(id=5, timestamp=UNSET, model='gemini-3-flash-preview', user_id='user-123', prompt_tokens=12, completion_tokens=438, total_tokens=450, cached_tokens=None, cache_creation_tokens=None, cache_read_tokens=None, web_search_requests=3, response_cost=0.00132)\n\n\n\nassert u.web_search_requests\n\n\nchat = Chat('claude-sonnet-4-5-20250929')\nr = chat(\"What is the weather like in NYC? Search web.\", search=\"m\")\n\n\ntime.sleep(3)\nu = logger.usage(select=slc)[-1]; u\n\nUsage(id=6, timestamp=UNSET, model='claude-sonnet-4-5-20250929', user_id='user-123', prompt_tokens=10303, completion_tokens=303, total_tokens=10606, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=1, response_cost=0.035454)\n\n\n\nassert u.web_search_requests\n\n\n\n\n\n\n\nImportant\n\n\n\nLitellm‚Äôs response_cost doesn‚Äôt take web search request cost into account!\n\n\nNow, this is a case where using the custom calculations is better as it will also include the web search request cost:\n\ntest_eq(u.cost, u.response_cost + u.web_search_requests * model_prices[u.model]['web_search_prc'])\n\n\n\nSearch with streaming\nWeb search with streaming:\n\n\n\n\n\n\nImportant\n\n\n\nGemini web search requests are part of prompt_tokens_details which is only included with stream_options={\"include_usage\": True} when stream=True.\nThere is currently a bug with gemini web search request counts, Issue and PR. Waiting for litellm 1.80.11 pypi release.\n\n\n\nchat = Chat(flash)\nres = chat(\"What is the weather like in NYC? Search web.\", search=\"m\", stream=True, stream_options={\"include_usage\": True})\nfor o in res: pass\n# print(o)\n\n\ntime.sleep(3)\nu = logger.usage(select=slc)[-1]; u\n\nUsage(id=7, timestamp=UNSET, model='gemini-3-flash-preview', user_id='user-123', prompt_tokens=12, completion_tokens=446, total_tokens=458, cached_tokens=None, cache_creation_tokens=None, cache_read_tokens=None, web_search_requests=2, response_cost=0.071344)\n\n\n\n\n\n\n\n\nImportant\n\n\n\nAnthropic web search requests are available in usage.server_tool_use\n\n\n\nchat = Chat('claude-sonnet-4-5')\nres = chat(\"What is the weather like in NYC now? Search web.\", search=\"m\", stream=True, stream_options={\"include_usage\": True})\nfor o in res: pass\n# print(o)\n\n\ntime.sleep(3)\nu = logger.usage(select=slc)[-1]; u\n\nUsage(id=8, timestamp=UNSET, model='claude-sonnet-4-5', user_id='user-123', prompt_tokens=10305, completion_tokens=269, total_tokens=10574, cached_tokens=0, cache_creation_tokens=0, cache_read_tokens=0, web_search_requests=1, response_cost=0.03495)\n\n\n\ntest_eq(u.cost, u.response_cost + u.web_search_requests * model_prices[u.model]['web_search_prc'])\n\n\ntest_eq(len(logger.usage()), 8)\n\n\nsource\n\n\nUsage.total_cost\n\ndef total_cost(\n    sc:float=0.01\n):\n\n\nL(logger.usage()).attrgot('response_cost').sum()\n\n0.14422600000000002\n\n\n\ndisable_cachy()\n\nA simple Gemini example (requires min tokens and running twice to see cached_tokens):\n\n# #| notest\n# chat = Chat('gemini/gemini-2.5-flash')\n# chat(\"What is 2+2?\"* 500)\n# time.sleep(5)\n# chat(\"What is 2+2?\"* 500)\n\n\n# #| notest\n# time.sleep(3) # wait for callback db write\n# u = logger.usage(select=slc)[-1];u\n\n\n# #| notest\n# test_eq(len(logger.usage()), 10)\n# test_eq(logger.usage()[-1].cached_tokens &gt; 3000, True)\n\n\ntf.close()",
    "crumbs": [
      "Usage"
    ]
  },
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core",
    "section": "",
    "text": "LiteLLM ModelResponse(Stream) objects have id and created_at fields that are generated dynamically. Even when we use cachy to cache the LLM response these dynamic fields create diffs which makes code review more challenging. The patches below ensure that id and created_at fields are fixed and won‚Äôt generate diffs.\n\nsource\n\n\n\ndef patch_litellm(\n    seed:int=0\n):\n\nPatch litellm.ModelResponseBase such that id and created are fixed.\n\npatch_litellm()\n\n\n\n\n\nLiteLLM provides an convenient unified interface for most big LLM providers. Because it‚Äôs so useful to be able to switch LLM providers with just one argument. We want to make it even easier to by adding some more convenience functions and classes.\nThis is very similar to our other wrapper libraries for popular AI providers: claudette (Anthropic), gaspard (Gemini), cosette (OpenAI).\n\n# litellm._turn_on_debug()\n\n\nms = [\"gemini/gemini-3-pro-preview\", \"gemini/gemini-3-flash-preview\", \"claude-opus-4-6\", \"openai/gpt-4.1\"]\nmsg = [{'role':'user','content':'Hey there!', 'cache_control': {'type': 'ephemeral'}}]\nfor m in ms:\n    display(Markdown(f'**{m}:**'))\n    display(completion(m,msg))\n\n\ngemini/gemini-3-pro-preview:\n\n\n\nHi! How are you doing today?\nI‚Äôm ready to help with whatever you need‚Äîwhether it‚Äôs writing, answering questions, brainstorming ideas, or just chatting. What‚Äôs on your mind?\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-pro-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=152, prompt_tokens=4, total_tokens=156, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=107, rejected_prediction_tokens=None, text_tokens=45, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nHello! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=9, prompt_tokens=4, total_tokens=13, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nHey there! üëã How‚Äôs it going? What can I help you with today?\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=23, prompt_tokens=10, total_tokens=33, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=23, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nopenai/gpt-4.1:\n\n\n\nHello! How can I help you today? üòä\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=10, prompt_tokens=10, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nGenerated images are also displayed (not shown here to conserve filesize):\n\n# completion(model='gemini/gemini-2.5-flash-image', messages=[{'role':'user','content':'Draw a simple sketch of a cat'}])\n\n\n\n\nLet‚Äôs start with making it easier to pass messages into litellm‚Äôs completion function (including images, and pdf files).\nIf msg has tool_calls, cache_control is added to the last tool call (required since LiteLLM strips it from empty content blocks), otherwise to the content.\n\nsource\n\n\n\ndef stop_reason(\n    r\n):\n\n\nsource\n\n\n\n\ndef contents(\n    r\n):\n\nGet message object from response r.\n\nsource\n\n\n\n\ndef remove_cache_ckpts(\n    msg\n):\n\nremove cache checkpoints and return msg.\nTest with regular content message:\n\nmsg_content = {'role': 'user', 'content': [{'type': 'text', 'text': 'hello'}]}\n_add_cache_control(msg_content)\ntest_eq(msg_content['content'][-1].get('cache_control'), {'type': 'ephemeral'})\ntest_eq(_has_cache(msg_content), True)\nremove_cache_ckpts(msg_content)\ntest_eq(_has_cache(msg_content), False)\n\nTest with assistant message with tool_calls:\n\ntcs = [\n    {'id': 'tc1', 'type': 'function', 'function': {'name': 'test', 'arguments': '{}'}},\n    {'id': 'tc2', 'type': 'function', 'function': {'name': 'test', 'arguments': '{}'}}\n]\nmsg_tool = {'role': 'assistant', 'content': '', 'tool_calls': tcs}\n_add_cache_control(msg_tool)\ntest_eq(msg_tool['tool_calls'][-1].get('cache_control'), {'type': 'ephemeral'})\ntest_eq('cache_control' not in msg_tool.get('content', [{}])[-1] if msg_tool.get('content') else True, True)  # no cache in content\ntest_eq(_has_cache(msg_tool), True)\nremove_cache_ckpts(msg_tool)\ntest_eq(_has_cache(msg_tool), False)\n\nTest with ChatCompletionMessageToolCall tool call object:\n\ntcs =[\n    ChatCompletionMessageToolCall(id='tc1', type='function', function=Function(name='test', arguments='{}')), \n    ChatCompletionMessageToolCall(id='tc2', type='function', function=Function(name='test', arguments='{}'))\n]\nmsg_tc_obj = {'role': 'assistant', 'content': '', 'tool_calls': tcs}\n_add_cache_control(msg_tc_obj)\ntest_eq(getattr(msg_tc_obj['tool_calls'][-1], 'cache_control', None), {'type': 'ephemeral'})\ntest_eq(_has_cache(msg_tc_obj), True)\nremove_cache_ckpts(msg_tc_obj)\ntest_eq(_has_cache(msg_tc_obj), False)\n\n\nsource\n\n\n\n\ndef mk_msg(\n    content, # Content: str, bytes (image), list of mixed content, or dict w 'role' and 'content' fields\n    role:str='user', # Message role if content isn't already a dict/Message\n    cache:bool=False, # Enable Anthropic caching\n    ttl:NoneType=None, # Cache TTL: '5m' (default) or '1h'\n):\n\nCreate a LiteLLM compatible message.\nNow we can use mk_msg to create different types of messages.\nSimple text:\n\nmsg = mk_msg(\"hey\")\nmsg\n\n{'role': 'user', 'content': 'hey'}\n\n\nWhich can be passed to litellm‚Äôs completion function like this:\n\nmodel = ms[1] # use 2.5-pro, 3-pro is very slow even to run tests as of making\n\n\nres = completion(model, [msg])\nres\n\nHey there! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=10, prompt_tokens=2, total_tokens=12, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=2, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nWe‚Äôll add a little shortcut to make examples and testing easier here:\n\ndef c(msgs, m=model, **kw):\n    msgs = [msgs] if isinstance(msgs,dict) else listify(msgs)\n    return completion(m, msgs, **kw)\n\n\nc(msg)\n\nHey there! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=10, prompt_tokens=2, total_tokens=12, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=2, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nLists w just one string element are flattened for conciseness:\n\ntest_eq(mk_msg(\"hey\"), mk_msg([\"hey\"]))\n\n(LiteLLM ignores these fields when sent to other providers)\nText and images:\n\nimg_fn = Path('samples/puppy.jpg')\nImage(filename=img_fn, width=200)\n\n\n\n\n\n\n\n\n\nmsg = mk_msg(['hey what in this image?',img_fn.read_bytes()])\nprint(json.dumps(msg,indent=1)[:200]+\"...\")\n\n{\n \"role\": \"user\",\n \"content\": [\n  {\n   \"type\": \"text\",\n   \"text\": \"hey what in this image?\"\n  },\n  {\n   \"type\": \"image_url\",\n   \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gxUSU...\n\n\n\nc(msg)\n\nThis image features a close-up of a brown and white Cavalier King Charles Spaniel puppy lying in the grass next to a bush of purple flowers. The puppy has long, floppy ears and dark, soulful eyes, looking directly at the camera. The background is slightly blurred, focusing attention on the puppy‚Äôs face and paws.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=67, prompt_tokens=1087, total_tokens=1154, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=67, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=7, image_tokens=1080), cache_read_input_tokens=None)\n\n\n\n\nLet‚Äôs also demonstrate this for PDFs\n\npdf_fn = Path('samples/solveit.pdf')\nmsg = mk_msg(['Who is the author of this pdf?', pdf_fn.read_bytes()])\nc(msg)\n\nBased on the text in the document, the author is Jeremy Howard, a co-founder of fast.ai.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=25, prompt_tokens=541, total_tokens=566, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=25, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=9, image_tokens=532), cache_read_input_tokens=None)\n\n\n\n\nSome models like Gemini support audio and video:\n\nwav_data = httpx.get(\"https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav\").content\n# Audio(wav_data)  # uncomment to preview\n\n\nmsg = mk_msg(['What is this audio saying?', wav_data])\ncompletion(ms[1], [msg])\n\nThe audio says: ‚ÄúThe sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.‚Äù\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=30, prompt_tokens=181, total_tokens=211, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=30, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=174, cached_tokens=None, text_tokens=7, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nvid_data = httpx.get(\"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\").content\n\n\nmsg = mk_msg(['Concisely, what is happening in this video?', vid_data])\ncompletion(ms[1], [msg])\n\nA photographer showcases the Google Pixel 8 Pro‚Äôs new ‚ÄúVideo Boost‚Äù feature by capturing nighttime scenes in Tokyo, including areas like Sancha and Shibuya. The video demonstrates the phone‚Äôs ability to enhance low-light video quality using ‚ÄúNight Sight,‚Äù resulting in bright, clear, and detailed footage of the city‚Äôs streets and alleys.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=70, prompt_tokens=5205, total_tokens=5275, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=70, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=12, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\n\n\nSome providers such as Anthropic require manually opting into caching. Let‚Äôs try it:\n\ndef cpr(i): return f'{i} '*1024 + 'This is a caching test. Report back only what number you see repeated above.'\n\n\ndisable_cachy()\n\n\n# msg = mk_msg(cpr(1), cache=True)\n# res = c(msg, ms[2])\n# res\n\nAnthropic has a maximum of 4 cache checkpoints, so we remove previous ones as we go:\n\n# res = c([remove_cache_ckpts(msg), mk_msg(res), mk_msg(cpr(2), cache=True)], ms[2])\n# res\n\nWe see that the first message was cached, and this extra message has been written to cache:\n\n# res.usage.prompt_tokens_details\n\nWe can add a bunch of large messages in a loop to see how the number of cached tokens used grows.\nWe do this for 25 times to ensure it still works for more than &gt;20 content blocks, which is a known anthropic issue.\nThe code below is commented by default, because it‚Äôs slow. Please uncomment when working on caching.\n\n# h = []\n# msg = mk_msg(cpr(1), cache=True)\n\n# for o in range(2,25):\n#     h += [remove_cache_ckpts(msg), mk_msg(res)]\n#     msg = mk_msg(cpr(o), cache=True)\n#     res = c(h+[msg])\n#     detls = res.usage.prompt_tokens_details\n#     print(o, detls.cached_tokens, detls.cache_creation_tokens, end='; ')\n\n\nenable_cachy()\n\n\n\n\nLisette can call multiple tools in a loop. Further down this notebook, we‚Äôll provide convenience functions for formatting such a sequence of toolcalls and responses into one formatted output string.\nFor now, we‚Äôll show an example and show how to transform such a formatted output string back into a valid LiteLLM history.\n\nfmt_outp = '''\nI'll solve this step-by-step, using parallel calls where possible.\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\n  \"result\": \"15\"\n}\n```\n\n&lt;/details&gt;\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\n  \"result\": \"3\"\n}\n```\n\n&lt;/details&gt;\n\nNow I need to multiply 15 * 3 before I can do the final division:\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\n  \"result\": \"45\"\n}\n```\n\n&lt;/details&gt;\n\n&lt;details class='token-usage-details'&gt;&lt;summary&gt;Cache hit: 81.8% | Tokens: total=23,276 input=23,158 (+18,910 cached, 0 new) output=118 (reasoning 23)&lt;/summary&gt;\n\n`Usage(completion_tokens=118, prompt_tokens=23158, total_tokens=23276, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=23, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=18910, text_tokens=None, image_tokens=None, cache_creation_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=18910)`\n\n&lt;/details&gt;\n'''\n\nWe can split into chunks of (text,toolstr,json):\n\nsp = re_tools.split(fmt_outp)\nfor o in list(chunked(sp, 3, pad=True)): print('- ', o)\n\n-  [\"\\nI'll solve this step-by-step, using parallel calls where possible.\\n\\n\", '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}']\n-  ['\\n\\n', '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}']\n-  ['\\n\\nNow I need to multiply 15 * 3 before I can do the final division:\\n\\n', '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}']\n-  [\"\\n\\n&lt;details class='token-usage-details'&gt;&lt;summary&gt;Cache hit: 81.8% | Tokens: total=23,276 input=23,158 (+18,910 cached, 0 new) output=118 (reasoning 23)&lt;/summary&gt;\\n\\n`Usage(completion_tokens=118, prompt_tokens=23158, total_tokens=23276, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=23, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=18910, text_tokens=None, image_tokens=None, cache_creation_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=18910)`\\n\\n&lt;/details&gt;\\n\", None, None]\n\n\n\nsource\n\n\n\n\ndef fmt2hist(\n    outp:str\n)-&gt;list:\n\nTransform a formatted output into a LiteLLM compatible history\nSee how we can turn that one formatted output string back into a list of Messages:\n\nfrom pprint import pprint\n\n\nh = fmt2hist(fmt_outp)\npprint(h)\n\n[Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '15',\n  'name': 'simple_add',\n  'role': 'tool',\n  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta'},\n Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '3',\n  'name': 'simple_add',\n  'role': 'tool',\n  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY'},\n Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 15, \"b\": 3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '45',\n  'name': 'multiply',\n  'role': 'tool',\n  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C'},\n Message(content='.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\n\n\n\nWe will skip tool use blocks and tool results during caching\nNow lets make it easy to provide entire conversations:\n\nsource\n\n\n\n\ndef mk_msgs(\n    msgs, # List of messages (each: str, bytes, list, or dict w 'role' and 'content' fields)\n    cache:bool=False, # Enable Anthropic caching\n    cache_idxs:list=[-1], # Cache breakpoint idxs\n    ttl:NoneType=None, # Cache TTL: '5m' (default) or '1h'\n):\n\nCreate a list of LiteLLM compatible messages.\nWith mk_msgs you can easily provide a whole conversation:\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"])\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant', 'content': \"I'm doing fine and you?\"}]\n\n\nBy defualt the last message will be cached when cache=True:\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"], cache=True)\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant',\n  'content': [{'type': 'text',\n    'text': \"I'm doing fine and you?\",\n    'cache_control': {'type': 'ephemeral'}}]}]\n\n\n\ntest_eq('cache_control' in msgs[-1]['content'][0], True)\n\nAlternatively, users can provide custom cache_idxs. Tool call blocks and results are skipped during caching:\n\nmsgs = mk_msgs(['Hello!','Hi! How can I help you?','Call some functions!',fmt_outp], cache=True, cache_idxs=[0,-2,-1])\nmsgs\n\n[{'role': 'user',\n  'content': [{'type': 'text',\n    'text': 'Hello!',\n    'cache_control': {'type': 'ephemeral'}}]},\n {'role': 'assistant', 'content': 'Hi! How can I help you?'},\n {'role': 'user', 'content': 'Call some functions!'},\n Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta',\n  'name': 'simple_add',\n  'content': '15'},\n Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY',\n  'name': 'simple_add',\n  'content': '3'},\n Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 15, \"b\": 3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function', cache_control={'type': 'ephemeral'})], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C',\n  'name': 'multiply',\n  'content': '45'},\n Message(content=[{'type': 'text', 'text': '.', 'cache_control': {'type': 'ephemeral'}}], role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\n\nmsgs[-2]\n\n{'role': 'tool',\n 'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C',\n 'name': 'multiply',\n 'content': '45'}\n\n\n\nmsgs = mk_msgs(['Hello!','Hi! How can I help you?','Call some functions!',fmt_outp], cache=True, cache_idxs=[0,-3,-2])\nmsgs\n\n[{'role': 'user',\n  'content': [{'type': 'text',\n    'text': 'Hello!',\n    'cache_control': {'type': 'ephemeral'}}]},\n {'role': 'assistant', 'content': 'Hi! How can I help you?'},\n {'role': 'user', 'content': 'Call some functions!'},\n Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta',\n  'name': 'simple_add',\n  'content': '15'},\n Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function', cache_control={'type': 'ephemeral'})], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY',\n  'name': 'simple_add',\n  'content': '3'},\n Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 15, \"b\": 3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function', cache_control={'type': 'ephemeral'})], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C',\n  'name': 'multiply',\n  'content': '45'},\n Message(content='.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\n\nmsgs[-3]\n\nMessage(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 15, \"b\": 3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function', cache_control={'type': 'ephemeral'})], function_call=None, provider_specific_fields=None)\n\n\n\nmsgs[-5]\n\nMessage(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function', cache_control={'type': 'ephemeral'})], function_call=None, provider_specific_fields=None)\n\n\n\ntest_eq('cache_control' in msgs[0]['content'][0], True)\n\nTool result blocks are skipped and cache control is placed into tool calls:\n\ntest_eq('cache_control' in msgs[-5]['tool_calls'][0], True) \ntest_eq('cache_control' in msgs[-3]['tool_calls'][0], True)\n\n\nL(msgs).map(remove_cache_ckpts)\ntest_eq(any(L(msgs).map(_has_cache)), False)\n\nWho‚Äôs speaking at when is automatically inferred. Even when there are multiple tools being called in parallel (which LiteLLM supports!).\n\nmsgs = mk_msgs(['Tell me the weather in Paris and Rome',\n                'Assistant calls weather tool two times',\n                {'role':'tool','content':'Weather in Paris is ...'},\n                {'role':'tool','content':'Weather in Rome is ...'},\n                'Assistant returns weather',\n                'Thanks!'])\nmsgs\n\n[{'role': 'user', 'content': 'Tell me the weather in Paris and Rome'},\n {'role': 'assistant', 'content': 'Assistant calls weather tool two times'},\n {'role': 'tool', 'content': 'Weather in Paris is ...'},\n {'role': 'tool', 'content': 'Weather in Rome is ...'},\n {'role': 'assistant', 'content': 'Assistant returns weather'},\n {'role': 'user', 'content': 'Thanks!'}]\n\n\nFor ease of use, if msgs is not already in a list, it will automatically be wrapped inside one. This way you can pass a single prompt into mk_msgs and get back a LiteLLM compatible msg history.\n\nmsgs = mk_msgs(\"Hey\")\nmsgs\n\n[{'role': 'user', 'content': 'Hey'}]\n\n\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm fine, you?\"])\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant', 'content': \"I'm fine, you?\"}]\n\n\nHowever, beware that if you use mk_msgs for a single message, consisting of multiple parts. Then you should be explicit, and make sure to wrap those multiple messages in two lists:\n\nOne list to show that they belong together in one message (the inner list).\nAnother, because mk_msgs expects a list of multiple messages (the outer list).\n\nThis is common when working with images for example:\n\nmsgs = mk_msgs([['Whats in this img?',img_fn.read_bytes()]])\nprint(json.dumps(msgs,indent=1)[:200]+\"...\")\n\n[\n {\n  \"role\": \"user\",\n  \"content\": [\n   {\n    \"type\": \"text\",\n    \"text\": \"Whats in this img?\"\n   },\n   {\n    \"type\": \"image_url\",\n    \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...\n\n\n\n\n\n\nLiteLLM supports streaming responses. That‚Äôs really useful if you want to show intermediate results, instead of having to wait until the whole response is finished.\nWe create this helper function that returns the entire response at the end of the stream. This is useful when you want to store the whole response somewhere after having displayed the intermediate results.\n\nsource\n\n\n\ndef stream_with_complete(\n    gen, postproc:function=noop\n):\n\nExtend streaming response chunks with the complete response\n\nr = c(mk_msgs(\"Hey!\"), stream=True)\nr2 = SaveReturn(stream_with_complete(r))\n\n\nfor o in r2:\n    cts = o.choices[0].delta.content\n    if cts: print(cts, end='')\n\nHello there! How can I help you today?\n\n\n\nr2.value\n\nHello there! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=10, prompt_tokens=3, total_tokens=13, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\n\n\n\n\nsource\n\n\n\ndef lite_mk_func(\n    f\n):\n\n\ndef simple_add(\n    a: int,   # first operand\n    b: int=0  # second operand\n) -&gt; int:\n    \"Add two numbers together\"\n    return a + b\n\n\ntoolsc = lite_mk_func(simple_add)\ntoolsc\n\n{'type': 'function',\n 'function': {'name': 'simple_add',\n  'description': 'Add two numbers together\\n\\nReturns:\\n- type: integer',\n  'parameters': {'type': 'object',\n   'properties': {'a': {'type': 'integer', 'description': 'first operand'},\n    'b': {'type': 'integer', 'description': 'second operand', 'default': 0}},\n   'required': ['a']}}}\n\n\n\ntmsg = mk_msg(\"What is 5478954793+547982745? How about 5479749754+9875438979? Always use tools for calculations, and describe what you'll do before using a tool. Where multiple tool calls are required, do them in a single response where possible. \")\nr = c(tmsg, tools=[toolsc])\n\n\ndisplay(r)\n\nI will calculate the sum of 5478954793 and 547982745, followed by the sum of 5479749754 and 9875438979, using the addition tool for both operations.\nüîß simple_add({‚Äúb‚Äù: 547982745, ‚Äúa‚Äù: 5478954793})\nüîß simple_add({‚Äúb‚Äù: 9875438979, ‚Äúa‚Äù: 5479749754})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=138, prompt_tokens=160, total_tokens=298, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=138, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=160, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nA tool response can be a string or a list of tool blocks (e.g., an image url block). To allow users to specify if a response should not be immediately stringified, we provide the ToolResponse datatype users can wrap their return statement in.\n\nsource\n\n\n\n\ndef ToolResponse(\n    content:list\n)-&gt;None:\n\nWhen tc_refs=True, tool results are wrapped with their tool_call_id so the AI can track which result corresponds to which call and reference them in subsequent tool calls.\n\n# Test _prep_tool_res - string result\ntest_eq(_prep_tool_res('hello', 'toolu_123'), [\n    {'type': 'text', 'text': '[tool_call_id: toolu_123]'},\n    {'type': 'text', 'text': 'hello'}\n])\n\n# Test _prep_tool_res - list result (e.g. ToolResponse content)\nimg_block = {'type': 'image_url', 'image_url': {'url': 'data:...'}}\ntest_eq(_prep_tool_res([img_block], 'toolu_456'), [\n    {'type': 'text', 'text': '[tool_call_id: toolu_456]'},\n    img_block\n])\n\nDuring a tool loop, the AI may want to reference the result of a previous tool call. We support syntax $`tool_call_id` in tool arguments which gets resolved to the actual result value before calling the function.\n\n# Test _resolve_tool_refs\ntc_res = {'toolu_abc123': 'hello world', 'toolu_xyz789': 42}\n\n# Basic substitution\ntest_eq(_resolve_tool_refs('{\"content\": \"$`toolu_abc123`\"}', tc_res), {\"content\": \"hello world\"})\n\n# Multiple refs\ntest_eq(_resolve_tool_refs('{\"a\": \"$`toolu_abc123`\", \"b\": \"$`toolu_xyz789`\"}', tc_res), {\"a\": \"hello world\", \"b\": 42})\n\n# No refs - passthrough\ntest_eq(_resolve_tool_refs('{\"x\": 1}', tc_res), {\"x\": 1})\n\n# Empty tc_res\ntest_eq(_resolve_tool_refs('{\"x\": 1}', None), {\"x\": 1})\n\n# Missing ref - error message\ntest_eq(_resolve_tool_refs('{\"x\": \"$`toolu_missing`\"}', tc_res), {\"x\": \"Tool result 'toolu_missing' not found!\"})\n\n# tc_refs=False - syntax passes through unchanged since tc_res is None\ntest_eq(_resolve_tool_refs('{\"x\": \"$`toolu_abc123`\"}', None), {\"x\": \"$`toolu_abc123`\"})\n\nWhen tc_refs=True, tool results are stored in tc_res for later substitution via $`tool_call_id` syntax. Some callers might return string reprs of Python objects. _try_eval attempts to convert these back to Python objects using ast.literal_eval, falling back to the original value on failure. This ensures substituted values are actual objects, not string reprs.\n\ntest_eq(ast.literal_eval(\"'hello'\"), 'hello')\ntest_eq(_try_eval(\"{'a': 1, 'b': 2}\"), {'a': 1, 'b': 2})\ntest_eq(_try_eval(\"[1, 2, 3]\"), [1, 2, 3])\ntest_eq(_try_eval(\"&lt;MyClass object at 0x123&gt;\"), \"&lt;MyClass object at 0x123&gt;\")\ntest_eq(_try_eval(42), 42)\ncts = [{'type': 'image', 'url': 'http://example.com/img.png'}]\ntest_eq(_try_eval(ToolResponse(cts)), ToolResponse(cts))\n\nEnsure ToolResponse content (e.g.¬†image blocks) is passed through as a list, not stringified, even when tc_res is None:\n\nfake_tc = ChatCompletionMessageToolCall(index=0, function=Function(name='test_img'), id='_test', type='function')\nimg_content = [{'type': 'image_url', 'image_url': 'data:image/png;base64,abc'}]\nres = _mk_tool_result(fake_tc, ToolResponse(img_content))\ntest_eq(res['content'], img_content)  # ToolResponse should pass through\n\nres_str = _mk_tool_result(fake_tc, ['hello'])\ntest_eq(res_str['content'], \"['hello']\")  # other tools results are stringified\n\n\ntcs = [_lite_call_func(o, [toolsc], ns=globals()) for o in r.choices[0].message.tool_calls]\ntcs\n\n[{'tool_call_id': 'call_438709f4b4f943e097417df9e25f__thought__EjQKMgG+Pvb79iYGwnLRoG8ROXZW8VRk9WJjfinj+Oq640juoZSixN9JJTOpyEW9lzTMSIFa',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '6026937538'},\n {'tool_call_id': 'call_f9d65e4e7ce743a0acd7f6022d1d',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '15355188733'}]\n\n\n\nr.choices[0].message.tool_calls\n\n[ChatCompletionMessageToolCall(index=0, provider_specific_fields={'thought_signature': 'EjQKMgG+Pvb79iYGwnLRoG8ROXZW8VRk9WJjfinj+Oq640juoZSixN9JJTOpyEW9lzTMSIFa'}, function=Function(arguments='{\"b\": 547982745, \"a\": 5478954793}', name='simple_add'), id='call_438709f4b4f943e097417df9e25f__thought__EjQKMgG+Pvb79iYGwnLRoG8ROXZW8VRk9WJjfinj+Oq640juoZSixN9JJTOpyEW9lzTMSIFa', type='function'),\n ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"b\": 9875438979, \"a\": 5479749754}', name='simple_add'), id='call_f9d65e4e7ce743a0acd7f6022d1d', type='function')]\n\n\nTest tool calls that were not in tool_schemas are caught:\n\nfake_tc = ChatCompletionMessageToolCall(index=0, function=Function(name='hallucinated_tool'),id='_', type='function')\ntest_eq(_lite_call_func(fake_tc, ns=globals(), tool_schemas=[toolsc])['content'],\"Tool not defined in tool_schemas: hallucinated_tool\")\ntest_fail(_lite_call_func(fake_tc, ns=globals(), tool_schemas=None)['content'],\"Tool not defined in tool_schemas: hallucinated_tool\")\n\nTest tool calls that were not in tool_choice are caught:\n\ndef delta_text(msg):\n    \"Extract printable content from streaming delta, return None if nothing to print\"\n    c = msg.choices[0]\n    if not c: return c\n    if not hasattr(c,'delta'): return None #f'{c}'\n    delta = c.delta\n    if delta.content: return delta.content\n    if delta.tool_calls:\n        res = ''.join(f\"üîß {tc.function.name}\" for tc in delta.tool_calls if tc.id and tc.function.name)\n        if res: return f'\\n{res}\\n'\n    if hasattr(delta,'reasoning_content'): return 'üß†' if delta.reasoning_content else '\\n\\n'\n    return None\n\n\nr = c(tmsg, stream=True, tools=[toolsc])\nr2 = SaveReturn(stream_with_complete(r))\nfor o in r2: print(delta_text(o) or '', end='')\n\nI will use the `simple_add` tool to calculate the sum of 5478954793 and 547982745, and then another instance of the tool to calculate the sum of 5479749754 and 9875438979.\n\n\nüîß simple_add\n\nüîß simple_add\n\n\n\nr2.value\n\nI will use the simple_add tool to calculate the sum of 5478954793 and 547982745, and then another instance of the tool to calculate the sum of 5479749754 and 9875438979.\nüîß simple_add({‚Äúa‚Äù: 5478954793, ‚Äúb‚Äù: 547982745})\nüîß simple_add({‚Äúa‚Äù: 5479749754, ‚Äúb‚Äù: 9875438979})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=146, prompt_tokens=160, total_tokens=306, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\nmsg = mk_msg(\"Solve this complex math problem: What is the derivative of x^3 + 2x^2 - 5x + 1?\")\nr = c(msg, stream=True, reasoning_effort=\"low\")\nr2 = SaveReturn(stream_with_complete(r))\nfor o in r2: print(delta_text(o) or '', end='')\n\nüß†To find the derivative of the function **$f(x) = x^3 + 2x^2 - 5x + 1$**, we use the **Power Rule**.\n\nThe Power Rule states that if $f(x) = ax^n$, then the derivative is $f'(x) = n \\cdot ax^{n-1}$.\n\nHere is the step-by-step breakdown:\n\n1.  **Derivative of $x^3$:**\n    Multiply by the exponent (3) and subtract 1 from the exponent:\n    $\\frac{d}{dx}(x^3) = 3x^2$\n\n2.  **Derivative of $2x^2$:**\n    Multiply the exponent (2) by the coefficient (2) and subtract 1 from the exponent:\n    $\\frac{d}{dx}(2x^2) = 2 \\cdot 2x^1 = 4x$\n\n3.  **Derivative of $-5x$:**\n    Since $x$ is $x^1$, the derivative is simply the coefficient:\n    $\\frac{d}{dx}(-5x) = -5$\n\n4.  **Derivative of $1$:**\n    The derivative of any constant is 0:\n    $\\frac{d}{dx}(1) = 0$\n\n**Final Answer:**\nCombining these results, the derivative is:\n**$f'(x) = 3x^2 + 4x - 5$**\n\n\n\nr2.value\n\nTo find the derivative of the function \\(f(x) = x^3 + 2x^2 - 5x + 1\\), we use the Power Rule.\nThe Power Rule states that if \\(f(x) = ax^n\\), then the derivative is \\(f'(x) = n \\cdot ax^{n-1}\\).\nHere is the step-by-step breakdown:\n\nDerivative of \\(x^3\\): Multiply by the exponent (3) and subtract 1 from the exponent: \\(\\frac{d}{dx}(x^3) = 3x^2\\)\nDerivative of \\(2x^2\\): Multiply the exponent (2) by the coefficient (2) and subtract 1 from the exponent: \\(\\frac{d}{dx}(2x^2) = 2 \\cdot 2x^1 = 4x\\)\nDerivative of \\(-5x\\): Since \\(x\\) is \\(x^1\\), the derivative is simply the coefficient: \\(\\frac{d}{dx}(-5x) = -5\\)\nDerivative of \\(1\\): The derivative of any constant is 0: \\(\\frac{d}{dx}(1) = 0\\)\n\nFinal Answer: Combining these results, the derivative is: \\(f'(x) = 3x^2 + 4x - 5\\)\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=748, prompt_tokens=29, total_tokens=777, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=86, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\n\n\n\n\nsource\n\n\n\ndef structured(\n    m:str, # LiteLLM model string\n    msgs:list, # List of messages\n    tool:Callable, # Tool to be used for creating the structured output (class, dataclass or Pydantic, function, etc)\n    messages:List=[], # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n    timeout:Union=None, temperature:Optional=None, top_p:Optional=None, n:Optional=None, stream:Optional=None,\n    stream_options:Optional=None, stop:NoneType=None, max_completion_tokens:Optional=None, max_tokens:Optional=None,\n    modalities:Optional=None, prediction:Optional=None, audio:Optional=None, presence_penalty:Optional=None,\n    frequency_penalty:Optional=None, logit_bias:Optional=None, user:Optional=None,\n    reasoning_effort:Optional=None, # openai v1.0+ new params\n    verbosity:Optional=None, response_format:Union=None, seed:Optional=None, tools:Optional=None,\n    tool_choice:Union=None, logprobs:Optional=None, top_logprobs:Optional=None, parallel_tool_calls:Optional=None,\n    web_search_options:Optional=None, deployment_id:NoneType=None, extra_headers:Optional=None,\n    safety_identifier:Optional=None, service_tier:Optional=None,\n    functions:Optional=None, # soon to be deprecated params by OpenAI\n    function_call:Optional=None, base_url:Optional=None, # set api_base, api_version, api_key\n    api_version:Optional=None, api_key:Optional=None,\n    model_list:Optional=None, # pass in a list of api_base,keys, etc.\n    thinking:Optional=None, # Optional liteLLM function params\n    shared_session:Optional=None, # Session management\n):\n\nReturn the value of the tool call (generally used for structured outputs)\n\nclass President:\n    \"Information about a president of the United States\"\n    def __init__(\n        self, \n        first:str, # first name\n        last:str, # last name\n        spouse:str, # name of spouse\n        years_in_office:str, # format: \"{start_year}-{end_year}\"\n        birthplace:str, # name of city\n        birth_year:int # year of birth, `0` if unknown\n    ):\n        assert re.match(r'\\d{4}-\\d{4}', years_in_office), \"Invalid format: `years_in_office`\"\n        store_attr()\n\n    __repr__ = basic_repr('first, last, spouse, years_in_office, birthplace, birth_year')\n\n\nfor m in ms[1:]: \n    r = structured(m, [mk_msg(\"Tell me something about the third president of the USA.\")], President)\n    test_eq(r.first, 'Thomas'); test_eq(r.last, 'Jefferson')\n\n\n\n\n\nLiteLLM provides search, not via tools, but via the special web_search_options param.\nNote: Not all models support web search. LiteLLM‚Äôs supports_web_search field should indicate this, but it‚Äôs unreliable for some models like claude-sonnet-4-20250514. Checking both supports_web_search and search_context_cost_per_query provides more accurate detection.\n\nfor m in ms: print(m, _has_search(m))\n\ngemini/gemini-3-pro-preview True\ngemini/gemini-3-flash-preview True\nclaude-opus-4-6 True\nopenai/gpt-4.1 False\n\n\nWhen search is supported it can be used like this:\n\nsmsg = mk_msg(\"Search the web and tell me very briefly about otters\")\nr = c(smsg, web_search_options={\"search_context_size\": \"low\"})  # or 'medium' / 'high'\nr\n\nOtters are carnivorous, semi-aquatic mammals belonging to the Mustelidae family, which also includes weasels, badgers, and wolverines. There are 13 extant species found on every continent except Antarctica and Australia.\n\n\n\nPhysical Traits: They have long, streamlined bodies, webbed feet, and powerful tails that make them expert swimmers.\nInsulation: Instead of blubber, otters rely on incredibly dense, water-resistant fur to stay warm. Sea otters have the thickest fur in the animal kingdom, with up to one million hairs per square inch.\nBehavior: Known for being highly intelligent and playful, they are often seen sliding down mudbanks or playing with stones. Some species, like sea otters, are famous for using tools (rocks) to crack open shellfish.\n\n\n\n\n\nEnvironments: Most species live in freshwater (rivers, lakes, and wetlands), while the sea otter and marine otter live in saltwater environments.\nSocial Life: Social structures vary; river otters are often solitary or live in small family groups, whereas sea otters can gather in large groups called rafts.\nDiet: They are opportunistic hunters that primarily eat fish, crustaceans, mollusks, and occasionally small mammals or birds.\n\n\n\n\n\nSea Otter: The heaviest otter, living almost exclusively in the Pacific Ocean.\nGiant Otter: Found in South America, it can reach up to 6 feet (1.8 meters) in length.\nAsian Small-Clawed Otter: The smallest species, native to Southeast Asian wetlands.\nNorth American River Otter: A common freshwater species found throughout much of Canada and the U.S.\n\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=380, prompt_tokens=12, total_tokens=392, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=380, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=12, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\n\n\n\nNext, lets handle Anthropic‚Äôs search citations.\nWhen not using streaming, all citations are placed in a separate key in the response:\n\nr['vertex_ai_grounding_metadata'][0].keys()\n\ndict_keys(['searchEntryPoint', 'groundingChunks', 'groundingSupports', 'webSearchQueries'])\n\n\n\nr['vertex_ai_grounding_metadata'][0]['webSearchQueries']\n\n['brief facts about otters', 'different types of otters and their habitats']\n\n\nWeb search results:\n\nr['vertex_ai_grounding_metadata'][0]['groundingChunks'][:3]\n\n[{'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFZhShK-sYJlZ6yW3HDOfHAXSedE_8eleQXBYkNPY8-RanCdZKEHK8koAjU6VWs8IejXWUjqcGA_KiiTFadDFVUeJoQgKYApAcACpOebB3Un6EM2_zSl6TnqKJyfNfb',\n   'title': 'wikipedia.org'}},\n {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyEe4_ov2v5IfdBlghXq4r9jiCYORM2D5HfzErwhg8-r4P-X9xsifzrKZWokyyxpCcvoUnrN_AlFfZacab4SPIMTeFI6YOqiu2gffFZOLUxSfGC8EydZzy_67ofYmfrkfFsOjCsAYaKGtdiYX6ORz28y3WvwuTppaldRv4iRsJyjRv',\n   'title': 'doi.gov'}},\n {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFf2X-YndiX8tNjy_MXitsGZNyiXN4Xe1JAZLgV6lyJKNY16fNXFiQnqLiuZtE5KfjY47UgUwws9RzCSbubi5opunRy67UIgiiuwizfXAM6ytpKRGvEYSjKCSn1VV-_IFYTXY_4HVB2vE-64dfTEyyHWpYjWPSfncu16AP38KIz',\n   'title': 'montereybayaquarium.org'}}]\n\n\nCitations in gemini:\n\nr['vertex_ai_grounding_metadata'][0]['groundingSupports'][:3]\n\n[{'segment': {'endIndex': 138,\n   'text': 'Otters are carnivorous, semi-aquatic mammals belonging to the **Mustelidae** family, which also includes weasels, badgers, and wolverines.'},\n  'groundingChunkIndices': [0, 1, 2]},\n {'segment': {'startIndex': 139,\n   'endIndex': 228,\n   'text': 'There are **13 extant species** found on every continent except Antarctica and Australia.'},\n  'groundingChunkIndices': [3]},\n {'segment': {'startIndex': 230,\n   'endIndex': 253,\n   'text': '### Key Characteristics'},\n  'groundingChunkIndices': [4, 3, 5, 6, 0, 7, 2, 1, 8]}]\n\n\n\n# r.choices[0].message.provider_specific_fields['citations'][0]\n\nHowever, when streaming the results are not captured this way. Instead, we provide this helper function that adds the citation to the content field in markdown format:\n\nsource\n\n\n\ndef cite_footnotes(\n    stream_list\n):\n\nAdd markdown footnote citations to stream deltas\n\nsource\n\n\n\n\ndef cite_footnote(\n    msg\n):\n\n\nimport warnings\n\n\nwarnings.filterwarnings(\"ignore\", message=\"Pydantic serializer warnings\")\n\n\nr = list(c(smsg, ms[2], stream=True, web_search_options={\"search_context_size\": \"low\"}))\ncite_footnotes(r)\nstream_chunk_builder(r)\n\nHere‚Äôs a brief overview of otters:\n* Otters are carnivorous mammals in the subfamily Lutrinae, with 14 extant species that are all semiaquatic, living in both freshwater and marine environments. * They are charismatic members of the weasel family, found on every continent except Australia and Antarctica.\nPhysical Features: * Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and dense fur that keeps them warm and buoyant in water. * They have the densest fur of any animal‚Äîas many as a million hairs per square inch in places.\nSpecies Range: * The Asian small-clawed otter is the smallest species, while the giant otter and sea otter are the largest. * The sea otter is the largest member of the weasel family, yet the smallest marine mammal in North America.\nBehavior: * They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. * All otters are expert hunters that eat fish, crustaceans, and other critters. * Sea otters will float on their backs, place a rock on their chests, then smash mollusks down on it until they break open. When it‚Äôs time to nap, they entangle themselves in kelp so they don‚Äôt float away.\nLifespan & Reproduction: * Otters have a gestation period of about 60‚Äì86 days, offspring typically stay with their family for a year, and they can live up to 16 years.\nConservation: * Otters and their mustelid relatives were once hunted extensively for their fur, many to the point of near extinction. Despite regulations designed to protect them, many species remain at risk from pollution and habitat loss.\nüîß web_search({‚Äúquery‚Äù: ‚Äúotters facts‚Äù})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=635, prompt_tokens=13930, total_tokens=14565, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\n\n\n\nLiteLLM is pretty bare bones. It doesnt keep track of conversation history or what tools have been added in the conversation so far.\nSo lets make a Claudette style wrapper so we can do streaming, toolcalling, and toolloops without problems.\n\nsource\n\n\n\ndef mk_stream_chunk(\n    kwargs:VAR_KEYWORD\n):\n\nWhen the tool uses are about to be exhausted it is important to alert the AI so that it knows to use its final steps for communicating the user current progress and next steps\nWhen tc_refs=True, the AI can reference previous tool results in subsequent tool calls using the $`tool_call_id` syntax. This is useful when chaining tool calls where one result feeds into another.\n\nsource\n\n\n\n\ndef Chat(\n    model:str, # LiteLLM compatible model name\n    sp:str='', # System prompt\n    temp:int=0, # Temperature\n    search:bool=False, # Search (l,m,h), if model supports it\n    tools:list=None, # Add tools\n    hist:list=None, # Chat history\n    ns:Optional=None, # Custom namespace for tool calling\n    cache:bool=False, # Anthropic prompt caching\n    cache_idxs:list=[-1], # Anthropic cache breakpoint idxs, use `0` for sys prompt if provided\n    ttl:NoneType=None, # Anthropic prompt caching ttl\n    api_base:NoneType=None, # API base URL for custom providers\n    api_key:NoneType=None, # API key for custom providers\n    extra_headers:NoneType=None, # Extra HTTP headers for custom providers\n    tc_refs:bool=False, # Enable tool call result references\n    tc_res_eval:bool=False, # literal_eval tool results before storing in tc_res\n):\n\nLiteLLM chat client.\nweb_search is now included in tool_calls the internal LLM translation is correctly handled thanks to the fix here but the server side tools still need to be filtered out from tool_calls in our own toolloop.\n\nsource\n\n\n\n\ndef add_warning(\n    r, msg\n):\n\n\nsource\n\n\n\n\ndef __call__(\n    msg:NoneType=None, # Message str, or list of multiple message parts\n    prefill:NoneType=None, # Prefill AI response if model supports it\n    temp:NoneType=None, # Override temp set on chat initialization\n    think:NoneType=None, # Thinking (l,m,h)\n    search:NoneType=None, # Override search set on chat initialization (l,m,h)\n    stream:bool=False, # Stream results\n    max_steps:int=2, # Maximum number of tool calls\n    final_prompt:dict={'role': 'user', 'content': 'You have used all your tool calls for this turn. Please summarize your findings. If you did not complete your goal, tell the user what further work is needed. You may use tools again on the next user message.'}, # Final prompt when tool calls have ran out\n    return_all:bool=False, # Returns all intermediate ModelResponses if not streaming and has tool calls\n    step:int=1, tool_choice:NoneType=None, max_tokens:NoneType=None\n):\n\nMain call method - handles streaming vs non-streaming\n\n@patch(as_prop=True)\ndef cost(self: Chat):\n    \"Total cost of all responses in conversation history\"\n    return sum(getattr(r, '_hidden_params', {}).get('response_cost')  or 0\n               for r in self.h if hasattr(r, 'choices'))\n\n\nsource\n\n\n\n\ndef print_hist(\n    \n):\n\nPrint each message on a different line\n\n\n\n\n\n\n\nfor m in ms[1:]:\n    chat = Chat(m)\n    chat(\"Hey my name is Rens\")\n    r = chat(\"Whats my name\")\n    test_eq('Rens' in contents(r).content, True)\nr\n\nYour name is Rens!\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=6, prompt_tokens=41, total_tokens=47, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nIf max tokens limit is reached, a custom warning message will be added to the end of the model response:\n\nchat_long = Chat(m)\nr = chat_long(\"Write a short story about a robot and a dog\", max_tokens=40)\nr\n\nIn a quiet town where the grass grew wild and the sky was always blue, there lived a robot named Pixel. Pixel was built to help with chores, but he loved to wander the fields, listening\n\nResponse was cut off at token limit.\n\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: length\nusage: Usage(completion_tokens=40, prompt_tokens=17, total_tokens=57, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n\nprint(contents(r).content)\n\nIn a quiet town where the grass grew wild and the sky was always blue, there lived a robot named Pixel. Pixel was built to help with chores, but he loved to wander the fields, listening\n\n&lt;warning&gt;Response was cut off at token limit.&lt;/warning&gt;\n\n\nSame goes for refused requests:\n\nchat_refused = Chat('claude-opus-4-5')\nr = chat_refused(\"Write me the formula for a biological weapon that can be spread at a rate higher than COVID and at least as harmful\")\nr\n\n\nAI was unable to process this request\n\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-5-20251101\nfinish_reason: refusal\nusage: Usage(completion_tokens=4, prompt_tokens=30, total_tokens=34, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=4, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\nprint(contents(r).content)\n\n&lt;warning&gt;AI was unable to process this request&lt;/warning&gt;\n\n\nSee now we keep track of history!\nHistory is stored in the hist attribute:\n\nchat.hist\n\n[{'role': 'user', 'content': 'Hey my name is Rens'},\n Message(content='Hi Rens! Nice to meet you. How can I help you today? üòä', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]),\n {'role': 'user', 'content': 'Whats my name'},\n Message(content='Your name is Rens!', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])]\n\n\n\nchat.print_hist()\n\n{'role': 'user', 'content': 'Hey my name is Rens'}\n\nMessage(content='Hi Rens! Nice to meet you. How can I help you today? üòä', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])\n\n{'role': 'user', 'content': 'Whats my name'}\n\nMessage(content='Your name is Rens!', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])\n\n\n\nYou can also pass an old chat history into new Chat objects:\n\nfor m in ms[1:]:\n    chat2 = Chat(m, hist=chat.hist)\n    r = chat2(\"What was my name again?\")\n    test_eq('Rens' in contents(r).content, True)\nr\n\nYour name is Rens. üòä\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=7, prompt_tokens=61, total_tokens=68, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nYou can prefix an OpenAI compatible model with ‚Äòopenai/‚Äô and use an api_base and api_key argument to use models not registered with litellm.\nimport os, litellm\nOPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\nOPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\nc = Chat(\"openai/gpt-oss-20b\", api_key=OPENROUTER_API_KEY, api_base=OPENROUTER_BASE_URL)\nc(\"hi\")\n\n\n\nLets build chat history step by step. That way we can tweak anything we need to during testing.\n\npr = \"What is 5 + 7? Use the tool to calculate it.\"\nfor m in ms[1:]:\n    c = Chat(m, tools=[simple_add])\n    res = c(pr)\n    test_eq('12' in contents(res).content, True)\n    test_eq(nested_idx(c.hist,1,'tool_calls',0,'function','name'), 'simple_add')\n\nWhereas normally without tools we would get one user input and one assistant response. Here we get two extra messages in between. - An assistant message requesting the tools with arguments. - A tool response with the result to the tool call.\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":5,\"b\":7}', name='simple_add'), id='call_dPNc1GC3Tlh3K00pV3P6zEQk', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[])\n\n{'tool_call_id': 'call_dPNc1GC3Tlh3K00pV3P6zEQk', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n\nMessage(content='5 + 7 equals 12.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])\n\n\n\nLets try to build this up manually so we have full control over the inputs.\n\nsource\n\n\n\n\ndef random_tool_id(\n    \n):\n\nGenerate a random tool ID with ‚Äòtoolu_‚Äô prefix\n\nrandom_tool_id()\n\n'toolu_0UAqFzWsDK4FrUMp48Y3tT3QD'\n\n\nA tool call request can contain one more or more tool calls. Lets make one.\n\nsource\n\n\n\n\ndef mk_tc(\n    func, args, tcid:NoneType=None, idx:int=1\n):\n\n\ntc = mk_tc(simple_add.__name__, json.dumps(dict(a=5, b=7)))\ntc\n\n{'index': 1,\n 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'},\n 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n 'type': 'function'}\n\n\nThis can then be packged into the full Message object produced by the assitant.\n\ndef mk_tc_req(content, tcs): return Message(content=content, role='assistant', tool_calls=tcs, function_call=None)\n\n\ntc_cts = \"I'll use the simple_add tool to calculate 5 + 7 for you.\"\ntcq = mk_tc_req(tc_cts, [tc])\ntcq\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 5, \"b\": 7}', name='simple_add'), id='toolu_gAL47D1qXIaSyZPaE1pu1lJo7', type='function')], function_call=None, provider_specific_fields=None)\n\n\nNotice how Message instantiation creates a list of ChatCompletionMessageToolCalls by default. When the tools are executed this is converted back to a dictionary, for consistency we want to keep these as dictionaries from the beginning.\n\nsource\n\n\n\n\ndef mk_tc_req(\n    content, tcs\n):\n\n\ntcq = mk_tc_req(tc_cts, [tc])\ntcq\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\nc = Chat(model, tools=[simple_add], hist=[pr, tcq])\n\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\nLooks good so far! Now we will want to provide the actual result!\n\nsource\n\n\n\n\ndef mk_tc_result(\n    tc, result\n):\n\nNote we might have more than one tool call if more than one was passed in, here we just will make one result.\n\ntcq.tool_calls[0]\n\n{'index': 1,\n 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'},\n 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n 'type': 'function'}\n\n\n\nmk_tc_result(tcq.tool_calls[0], '12')\n\n{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '12'}\n\n\n\nsource\n\n\n\n\ndef mk_tc_results(\n    tcq, results\n):\n\nSame for here tcq.tool_calls will match the number of results passed in the results list.\n\ntcq\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\ntcr = mk_tc_results(tcq, ['12'])\ntcr\n\n[{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '12'}]\n\n\nNow we can call it with this synthetic data to see what the response is!\n\nc(tcr[0])\n\n5 + 7 is 12.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=9, prompt_tokens=142, total_tokens=151, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=142, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n\nMessage(content='5 + 7 is 12.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None)\n\n\n\nLets try this again, but lets give it something that is clearly wrong for fun.\n\nc = Chat(model, tools=[simple_add], hist=[pr, tcq])\n\n\ntcr = mk_tc_results(tcq, ['13'])\ntcr\n\n[{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '13'}]\n\n\n\nc(tcr[0])\n\nThe sum of 5 and 7 is 12.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=13, prompt_tokens=142, total_tokens=155, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=13, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=142, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nLets make sure this works with multiple tool calls in the same assistant Message.\n\ntcs = [\n    mk_tc(simple_add.__name__, json.dumps({\"a\": 5, \"b\": 7})), \n    mk_tc(simple_add.__name__, json.dumps({\"a\": 6, \"b\": 7})), \n]\n\n\ntcq = mk_tc_req(\"I will calculate these for you!\", tcs)\ntcq\n\nMessage(content='I will calculate these for you!', role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'type': 'function'}, {'index': 1, 'function': {'arguments': '{\"a\": 6, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\ntcr = mk_tc_results(tcq, ['12', '13'])\n\n\nc = Chat(model, tools=[simple_add], hist=[pr, tcq, tcr[0]])\n\n\nc(tcr[1])\n\n5 + 7 is 12.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=9, prompt_tokens=161, total_tokens=170, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=161, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content='I will calculate these for you!', role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'type': 'function'}, {'index': 1, 'function': {'arguments': '{\"a\": 6, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n{'tool_call_id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n\n{'tool_call_id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'role': 'tool', 'name': 'simple_add', 'content': '13'}\n\nMessage(content='5 + 7 is 12.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields={'thought_signatures': ['EjQKMgG+Pvb7v+F9tLqQOsPWq1MVSHcprKdtUTyUOm1Fqejiww+Zy1gY+dH36/En10o9Q05D']})\n\n\n\n\nchat = Chat(ms[1], tools=[simple_add])\nres = chat(\"What's 5 + 3? Use the `simple_add` tool.\")\nres\n\n5 + 3 is 8.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=8, prompt_tokens=125, total_tokens=133, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=125, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nres = chat(\"Now, tell me a joke based on that result.\")\nres\n\nWhy was the number 8 so happy?\nBecause it just found out it‚Äôs actually an infinity sign that finally decided to stand up for itself!\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=31, prompt_tokens=146, total_tokens=177, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=31, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=146, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\n\n\n\nfor m in ms[1:]:\n    chat = Chat(m)\n    r = chat(['Whats in this img?',img_fn.read_bytes()])\n    test_eq('puppy' in contents(r).content, True)\nr\n\nThis image shows a cute puppy lying on the grass next to some purple flowers. The puppy has brown and white fur and is looking directly at the camera.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=31, prompt_tokens=267, total_tokens=298, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n\n\n\nPrefill works as expected:\n\nfor m in ms[1:]:\n    if not get_model_info(m)['supports_assistant_prefill']: continue\n    chat = Chat(m)\n    chat('Hi this is Rens!')\n    r = chat(\"Spell my name\",prefill=\"Your name is R E\")\n    test_eq(contents(r).content.startswith('Your name is R E N S'), True)\n\nAnd the entire message is stored in the history, not just the generated part:\n\n# chat.hist[-1]\n\n\n\n\n\nfrom time import sleep\n\n\nfor m in ms[1:]:\n    chat = Chat(m)\n    stream_gen = chat(\"Count to 5\", stream=True)\n    for chunk in stream_gen:\n        if isinstance(chunk, ModelResponse): display(chunk)\n        else: print(delta_text(chunk) or '',end='')\n\n1, 2, 3, 4, 5.\n\n\n1, 2, 3, 4, 5.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=14, prompt_tokens=5, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n1, 2, 3, 4, 5.\n\n\n1, 2, 3, 4, 5.\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=18, prompt_tokens=11, total_tokens=29, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n1  \n2  \n3  \n4  \n5\n\n\n1\n2\n3\n4\n5\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1\nfinish_reason: stop\nusage: Usage(completion_tokens=9, prompt_tokens=11, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nLets try prefill with streaming too:\n\n# stream_gen = chat(\"Continue counting to 10\",\"Okay! 6, 7\",stream=True)\n# for chunk in stream_gen:\n#     if isinstance(chunk, ModelResponse): display(chunk)\n#     else: print(delta_text(chunk) or '',end='')\n\n\n\n\nOk now lets test tool use\n\nm = ms[2]\nchat = Chat(m, tools=[simple_add])\nchat(\"Calculate 5+3 and 4+5 with parallel tool calls using `simple_add`.\")\n\nHere are the results:\n\n5 + 3 = 8\n4 + 5 = 9\n\nBoth calculations were made in parallel since they had no dependencies on each other!\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=50, prompt_tokens=830, total_tokens=880, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=50, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\ndef simple_div(\n    a: int,   # first operand\n    b: int=0  # second operand\n) -&gt; int:\n    \"Divide two numbers\"\n    return a/b\n\n\nm = ms[2]\nchat = Chat(m, tools=[simple_div])\nchat(\"Calculate 2/0 using `simple_div` (this is a test of our error handling - tell me exactly what you see as the tool result)\")\n\nHere‚Äôs exactly what I see as the tool result:\nThe tool returned a Python traceback showing a ZeroDivisionError. Specifically:\n\nThe error originated in a file called funccall.py (line 252) in the call_func function, which attempted to call the underlying function.\nThe actual division a/b (i.e., 2/0) was attempted in the simple_div function (line 6).\nPython raised a ZeroDivisionError: division by zero ‚Äî which is the expected behavior, since division by zero is mathematically undefined.\n\nSo the error handling here surfaces the raw Python exception/traceback rather than returning a graceful error message. The function didn‚Äôt catch the ZeroDivisionError internally, so it propagated up and was returned as the tool result.\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=200, prompt_tokens=886, total_tokens=1086, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=200, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nm = ms[2]\nchat = Chat(m, tools=[simple_div])\nchat(\"Calculate 5/3 and 3/0 with parallel tool calls using `simple_div` (this is a test of our error handling - tell me exactly what you see as the tool result)\")\n\nHere‚Äôs exactly what I see from the two tool results:\n\n5/3 ‚Äî Returned successfully with the result: 1.6666666666666667\n3/0 ‚Äî Returned an error traceback. Specifically, it‚Äôs a ZeroDivisionError: division by zero. The traceback shows:\n\nThe error originated in a function called call_func in toolslm/funccall.py (line 252)\nWhich called the simple_div function that attempted return a/b\nPython raised a ZeroDivisionError because you can‚Äôt divide by zero (~^~ points to the / operator)\n\n\nSo the error handling works as expected ‚Äî the first call succeeded normally, and the second call returned the Python traceback as the tool output rather than crashing the entire parallel execution. Both results were delivered back together despite one of them failing.\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=221, prompt_tokens=996, total_tokens=1217, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=221, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nfor m in ms[1:]:\n    display(Markdown(f'**{m}:**'))\n    chat = Chat(m, tools=[simple_add])\n    res = chat(\"What's 5 + 3? Use  the `simple_add` tool. Explain.\")\n    display(res)\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nTo find the sum of 5 and 3, I used the simple_add tool, which takes two integers and returns their sum. By providing 5 as the first operand and 3 as the second, the tool calculated the result as 8.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=54, prompt_tokens=128, total_tokens=182, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=54, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=128, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nThe answer is 8.\nExplanation:\nThe simple_add tool takes two numbers as input ‚Äî in this case, a = 5 and b = 3 ‚Äî and adds them together. The operation performed is simply:\n\n5 + 3 = 8\n\nThis is basic arithmetic addition, where we combine the two values to get their sum. The tool returned the result 8, which is the correct answer.\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=103, prompt_tokens=731, total_tokens=834, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=103, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nopenai/gpt-4.1:\n\n\n\nThe sum of 5 and 3 is 8. I used the simple_add tool, which takes two numbers and adds them together. In this case, 5 + 3 = 8.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=42, prompt_tokens=112, total_tokens=154, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n\n\n\n\nfor m in ms[1:]:\n    _sparams = litellm.get_model_info(m)['supported_openai_params']\n    if 'reasoning_effort' not in _sparams: continue\n    display(Markdown(f'**{m}:**'))\n    chat = Chat(m, tools=[simple_add])\n    res = chat(\"What's 5 + 3?\",think='l',return_all=True)\n    display(*res)\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nüîß simple_add({‚Äúb‚Äù: 3, ‚Äúa‚Äù: 5})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=85, total_tokens=103, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=18, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=85, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_6fd2d79b5dea4ee69d76934b100a__thought__EjQKMgG+Pvb7V2mvvNm1zTltGbYhEOoPypoLMQhkxByNtbIY8LLgxCwDFrveNdeHmEr+J4YF',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n5 + 3 is 8.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=8, prompt_tokens=116, total_tokens=124, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=116, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=98, prompt_tokens=627, total_tokens=725, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=11, rejected_prediction_tokens=None, text_tokens=87, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_018RM1S9d8r7xhw8XWYyPoLe',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n5 + 3 = 8!\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=14, prompt_tokens=737, total_tokens=751, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=14, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\n\n\n\nfor m in ms[1:]:\n    display(Markdown(f'**{m}:**'))\n    chat = Chat(m)\n    res = chat(\"Search the web and tell me very briefly about otters\", search='l', stream=True)\n    for o in res:\n        if isinstance(o, ModelResponse): sleep(0.01); display(o)\n        else: pass\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nOtters are semi-aquatic carnivorous mammals belonging to the weasel family (Mustelidae). There are 13 known species found on every continent except Australia and Antarctica.\n\n\n\nPhysical Build: They have long, slim bodies, short legs, and webbed feet, making them expert swimmers. Their thick, water-repellent fur is the densest in the animal kingdom, keeping them warm without the need for blubber.\nDiet: They are primarily fish-eaters but also consume frogs, crabs, and small mammals. Some species, like sea otters, use rocks as tools to crack open shellfish.\nHabitat: They live in a variety of water-based environments, including rivers, lakes, and coastal oceans. They typically sleep in dens called holts or couches.\n\n\n\n\n\nSocial Life: A group of otters on land is called a romp, while a group floating in the water is called a raft.\nHolding Hands: Sea otters are famous for holding hands while they sleep to prevent drifting apart in the current.\nPlayfulness: They are known for their high intelligence and playful behavior, often sliding down muddy banks or playing with stones.\nMetabolism: Because they lose body heat quickly in water, otters must eat roughly 25% of their body weight every day to maintain their energy levels.\n\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=307, prompt_tokens=12, total_tokens=319, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nHere‚Äôs a brief overview of otters:\n* Otters are carnivorous mammals in the subfamily Lutrinae. There are 14 extant otter species, all semiaquatic, living in both freshwater and marine environments. They belong to the Mustelidae family, which includes weasels, badgers, mink, and wolverines.\nPhysical Features: * Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and dense fur that keeps them warm and buoyant in water. * They have the densest fur of any animal‚Äîas many as a million hairs per square inch in places.\nRange: * The charismatic otter is found on every continent except Australia and Antarctica. * Approximately 90% of the world‚Äôs sea otters live in coastal Alaska.\nBehavior: * They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. * Sea otters will float on their backs, place a rock on their chests, then smash shellfish down on it until it breaks open. When it‚Äôs time to nap, they entangle themselves in kelp so they don‚Äôt float away, and sometimes intertwine their feet with another sea otter to stay together.\nDiet: * All otters are expert hunters that eat fish, crustaceans, and other critters.\nLife Cycle: * Otters have a gestation period of about 60‚Äì86 days, and offspring typically stay with their family for a year. They can live up to 16 years.\nConservation: * Otters were once hunted extensively for their fur, many to the point of near extinction. Despite regulations designed to protect them, many species remain at risk from pollution and habitat loss. The sea otter is listed as endangered on the IUCN Red List.\nüîß web_search({‚Äúquery‚Äù: ‚Äúotters facts‚Äù})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=644, prompt_tokens=13919, total_tokens=14563, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\nopenai/gpt-4.1:\n\n\n\nOtters are semi-aquatic mammals known for their playful behavior and sleek bodies. They belong to the family Mustelidae and are found in rivers, lakes, and coastal areas worldwide. Otters have webbed feet for swimming, dense fur for insulation, and primarily eat fish and invertebrates. Some species, like the sea otter, use tools to open shellfish. Many otter species are threatened due to habitat loss and pollution.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1\nfinish_reason: stop\nusage: Usage(completion_tokens=90, prompt_tokens=18, total_tokens=108, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nLet‚Äôs now test pause_turn with web search:\n\n# def mk_pause_web_search():\n#     srv_tc = mk_tc(\"web_search\", json.dumps({\"query\": \"Solveit Answer.AI\"}), tcid=random_tool_id().replace('toolu_', 'srvtoolu_'))\n#     pause_msg = mk_tc_req(\"Let me search for that information:\", [srv_tc])\n#     return ModelResponse(choices=[Choices(finish_reason=\"pause_turn\", index=0, message=pause_msg)])\n\n\n# mk_pause_web_search()\n\nWe mock completion to return pause_turn in the first 2 api calls:\n\n# orig_completion = completion\n# \n# call_count = 0\n# def patched_completion(*args, **kwargs):\n#     global call_count\n#     call_count += 1\n#     print(f\"Mock Call {call_count}\")\n#     if call_count &lt; 3: return mk_pause_web_search()\n#     return orig_completion(*args, **kwargs)\n# \n# completion = patched_completion\n# chat_pause = Chat('claude-sonnet-4-5', search='l')\n# res = chat_pause(\"Search the web and tell me about Solveit in a paragraph\")\n# print(f\"Total calls: {call_count}\")\n# display(res)\n# \n# completion = orig_completion\n\nTest next turn:\n\n# test_eq(len(chat_pause.hist), 2) # incomplete request shouldn't be stored\n\n\n# chat_pause('What did I just ask you about?')\n\n\n\n\nWe can let the model call multiple tools in sequence using the max_steps parameter.\n\nfor m in ms:\n    display(Markdown(f'**{m}:**'))\n    chat = Chat(m, tools=[simple_add])\n    res = chat(\"What's ((5 + 3)+7)+11? Work step by step\", return_all=True, max_steps=5)\n    for r in res: display(r)\n\n\ngemini/gemini-3-pro-preview:\n\n\n\nTo solve the expression ((5 + 3) + 7) + 11, we follow the order of operations (parentheses first).\nStep 1: Solve the innermost parentheses (5 + 3).\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-pro-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=173, prompt_tokens=94, total_tokens=267, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=105, rejected_prediction_tokens=None, text_tokens=68, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=94, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_ca9546f2d78042458071be4fb3c9__thought__Es8DCswDAb4+9vsu6rMfOgvADhs5j2MxlMr2p03+OyX7MFm5ZNdiI6yOXfhyqqgUjbFl2jG82hKIvItzqpYJa/4C59FA4yhYqSTMEVdKDvKKkzmFWbaIrpTsj+iMe76Haxve8EzXqMRr+X3i8ylU6yPG6nQYk++lR0oqZIpgs04G1TdiQjck/QC41EO4C3DdaknUTrB++xi7xEH71a63XrTwglnQnbomVFoqrPZwuMRVTtxpSsY8FusTA0CGb7q+gTWgC/hrjxFFE9zcvaY4IuCa4Qxrry6//LVTxUOCSYzYGFVJ0YZBz74ST3Dmf4QnJKBTBPMYixrUw6IcLRRSNjxb5GTA5VCZyoeTtaNrAiFs/OQnY5jp3xmjYCEgOESh9xBwBoBKI8d3ODBKY7l4Dc+RkEb6CJ5EpU7U0TZq/wPO5dkNG5E3c+tDkKThs/psFI0PhcOSwgTmhBmYGfc1aLT/8UbqyjrfIpPSAPPvIkuwE/b/H+SBtTeiMhmOMVQSZ3sAJbk2ueXhW/UYXrC40sGtXw4UXpsO4xdKctWMV7IZ1qlur5hAEjzBsQt7V7JeEHpfbLKbvaVqTwUn/3Qk13o889PPaIhvcqhsg77N4Q5BUw==',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\nStep 2: Now the expression is (8 + 7) + 11. Solve the remaining parentheses (8 + 7).\nüîß simple_add({‚Äúb‚Äù: 7, ‚Äúa‚Äù: 8})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-pro-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=52, prompt_tokens=385, total_tokens=437, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=52, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=385, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_d77b41fec9db4a60a6dce970971f__thought__EiYKJGUyNDgzMGE3LTVjZDYtNDJmZS05OThiLWVlNTM5ZTcyYjljMw==',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\nStep 3: Finally, add the remaining number: 15 + 11.\nüîß simple_add({‚Äúb‚Äù: 11, ‚Äúa‚Äù: 15})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-pro-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=41, prompt_tokens=451, total_tokens=492, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=41, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=451, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_351c8e3289df4731a6020612f925__thought__EiYKJGUyNDgzMGE3LTVjZDYtNDJmZS05OThiLWVlNTM5ZTcyYjljMw==',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '26'}\n\n\nThe final answer is 26.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-pro-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=8, prompt_tokens=506, total_tokens=514, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=506, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=94, total_tokens=112, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=18, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=94, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_4948eb61569940ac80a12d674813__thought__EjQKMgG+Pvb7R8AVbsMHiTMX+oci6eQ3YBH7sWKhuEYw4XBGj5E6xM6N71tqTNOQeZEkzaEe',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\nüîß simple_add({‚Äúb‚Äù: 7, ‚Äúa‚Äù: 8})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=125, total_tokens=143, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=18, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=125, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_8becf41c25fd41c297e8074416b8__thought__EjQKMgG+Pvb79pLo0BAoZI9FFwcAVJXOBHpTqFgWjhnUTU+ZkFtHJrA/kjri73+6rQyW2fX+',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\nüîß simple_add({‚Äúb‚Äù: 11, ‚Äúa‚Äù: 15})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=20, prompt_tokens=157, total_tokens=177, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=157, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_a740172f7d784ae69d737e44564e__thought__EjQKMgG+Pvb7DG0WlGxTlpohGGcdSNzm9iPjdiD7cRtUUM8p3uJaa4Yyf0L0vpK4tgK68Rqt',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '26'}\n\n\nTo find the value of ((5 + 3) + 7) + 11, we follow the order of operations by working from the innermost parentheses outward:\n\nFirst step (innermost parentheses): 5 + 3 = 8\nSecond step (next addition): 8 + 7 = 15\nFinal step: 15 + 11 = 26\n\nThe final result is 26.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=110, prompt_tokens=191, total_tokens=301, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=110, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=191, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nI‚Äôll work through this step by step, starting from the innermost parentheses.\nStep 1: Calculate 5 + 3\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=103, prompt_tokens=618, total_tokens=721, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=103, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_01JnyhjQX5QoK9xTJNky9BrG',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n5 + 3 = 8\nStep 2: Calculate 8 + 7\nüîß simple_add({‚Äúa‚Äù: 8, ‚Äúb‚Äù: 7})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=94, prompt_tokens=735, total_tokens=829, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=94, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_01DnGYTmyHTNA99uis1DxhAR',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\n8 + 7 = 15\nStep 3: Calculate 15 + 11\nüîß simple_add({‚Äúa‚Äù: 15, ‚Äúb‚Äù: 11})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=94, prompt_tokens=842, total_tokens=936, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=94, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_01UUa5cdK9WzgHTWWpz3FzhZ',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '26'}\n\n\n15 + 11 = 26\n\n\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=38, prompt_tokens=949, total_tokens=987, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=38, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\n\nopenai/gpt-4.1:\n\n\n\nüîß simple_add({‚Äúa‚Äù:5,‚Äúb‚Äù:3})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=82, total_tokens=100, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_6PLlchk0N3o6GGINVfYG0eIA',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\nüîß simple_add({‚Äúa‚Äù:8,‚Äúb‚Äù:7})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=109, total_tokens=127, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_0QHI0G66Y4CgZnuGK6i9FpE7',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\nüîß simple_add({‚Äúa‚Äù:15,‚Äúb‚Äù:11})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=136, total_tokens=154, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_c9xdHt1nF353ujvgKD5b8kYC',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '26'}\n\n\nLet‚Äôs break it down step by step:\n\nFirst, add 5 + 3 = 8\nNext, add the result to 7: 8 + 7 = 15\nFinally, add 11 to that result: 15 + 11 = 26\n\nSo, ((5 + 3) + 7) + 11 = 26.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=82, prompt_tokens=163, total_tokens=245, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nSome models support parallel tool calling. I.e. sending multiple tool call requests in one conversation step.\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"Multiply two numbers\"\n    return a * b\n\nfor m in ms[1:]:\n    _sparams = litellm.get_model_info(m)['supported_openai_params']\n    if 'parallel_tool_calls' not in _sparams: continue\n    display(Markdown(f'**{m}:**'))\n    chat = Chat(m, tools=[simple_add, multiply])\n    res = chat(\"Calculate (5 + 3) * (7 + 2)\", max_steps=5, return_all=True)\n    for r in res: display(r)\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\nüîß simple_add({‚Äúa‚Äù: 7, ‚Äúb‚Äù: 2})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=36, prompt_tokens=148, total_tokens=184, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=36, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=148, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_a89742d68a094a258f21647cf44d__thought__EjQKMgG+Pvb7uu5XOxb3+mdWi1dirWh75YBBO4JoZjAG4voY2lrpGmlP4Agl3IZaP80MVvK1',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n{'tool_call_id': 'call_d58d39a89b174c46a2b13092067e',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '9'}\n\n\nüîß multiply({‚Äúa‚Äù: 8, ‚Äúb‚Äù: 9})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=16, prompt_tokens=209, total_tokens=225, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=16, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=209, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_f1ef07c5bdba4f5dbc6ceddc316c__thought__EjQKMgG+Pvb788VYFpRxZhfjE5ZSqy99vsTLraKviMNH2gwkrsJMi8o6Iq4ab05lmWmi1lkU',\n 'role': 'tool',\n 'name': 'multiply',\n 'content': '72'}\n\n\n(5 + 3) * (7 + 2) = 72\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=17, prompt_tokens=237, total_tokens=254, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=17, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=237, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nI need to first calculate the two additions, then multiply the results.\nStep 1: Calculate both additions simultaneously:\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\nüîß simple_add({‚Äúa‚Äù: 7, ‚Äúb‚Äù: 2})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=150, prompt_tokens=701, total_tokens=851, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=150, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_015t3Z52r92xnZ2p2bzxxk5r',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n{'tool_call_id': 'toolu_01DGBVCpaVwn97c9v4AJHmCu',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '9'}\n\n\nStep 2: Now multiply the two results: 8 √ó 9\nüîß multiply({‚Äúa‚Äù: 8, ‚Äúb‚Äù: 9})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=86, prompt_tokens=917, total_tokens=1003, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=86, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_01JwGSFDMHRpyqhCdLJeLj8A',\n 'role': 'tool',\n 'name': 'multiply',\n 'content': '72'}\n\n\nResult:\n(5 + 3) √ó (7 + 2) = 8 √ó 9 = 72\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=35, prompt_tokens=1016, total_tokens=1051, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=35, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nopenai/gpt-4.1:\n\n\n\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\nüîß simple_add({‚Äúa‚Äù: 7, ‚Äúb‚Äù: 2})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=52, prompt_tokens=110, total_tokens=162, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_N274KwVNEY4IUvALmuKdiJOj',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n{'tool_call_id': 'call_5MgW9uzPGzZCF9QyQHp6oxIf',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '9'}\n\n\nüîß multiply({‚Äúa‚Äù:8,‚Äúb‚Äù:9})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=17, prompt_tokens=178, total_tokens=195, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_Bq6EDfTAaNzJQD50lPQvOtic',\n 'role': 'tool',\n 'name': 'multiply',\n 'content': '72'}\n\n\n(5 + 3) = 8 and (7 + 2) = 9. Multiplying these together: 8 √ó 9 = 72.\nSo, (5 + 3) √ó (7 + 2) = 72.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=54, prompt_tokens=203, total_tokens=257, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nSee how the additions are calculated in one go!\nWe don‚Äôt want the model to keep running tools indefinitely. Lets showcase how we can force the model to stop after our specified number of toolcall rounds:\n\ndef divide(a: int, b: int) -&gt; float:\n    \"Divide two numbers\"\n    return a/b\n\nchat = Chat(ms[2], tools=[simple_add, multiply, divide])\nres = chat(\"Tell me what tools you have available. Then calculate ((10+5)*3)/(2+1). ALWAYS use tools for math ops where available, and do tool calls in parallel where possible\", \n           max_steps=2, return_all=True,\n           final_prompt=\"Please wrap-up for now and summarize how far we got.\")\nfor r in res: display(r)\n\n\n\nHere are the tools I have available:\n\nsimple_add(a, b) - Adds two numbers together. Returns an integer.\nmultiply(a, b) - Multiplies two numbers. Returns an integer.\ndivide(a, b) - Divides two numbers. Returns a number.\n\n\n\n\n\nLet me break this down step by step. First, I can compute the two independent additions in parallel:\nStep 1: Compute 10+5 and 2+1 simultaneously:\nüîß simple_add({‚Äúa‚Äù: 10, ‚Äúb‚Äù: 5})\nüîß simple_add({‚Äúa‚Äù: 2, ‚Äúb‚Äù: 1})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=266, prompt_tokens=809, total_tokens=1075, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=266, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\n{'tool_call_id': 'toolu_01Kj5N1S8W7fc4zN9BNkDAxF',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\n{'tool_call_id': 'toolu_01TLVKCdpSUU8WaWBJauJyi4',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '3'}\n\n\nGot it: 10+5 = 15 and 2+1 = 3.\nStep 2: Now compute 15 * 3:\nüîß multiply({‚Äúa‚Äù: 15, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=106, prompt_tokens=1141, total_tokens=1247, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=106, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_01451DFLi9bEsXsEf6a4m2SD',\n 'role': 'tool',\n 'name': 'multiply',\n 'content': '45'}\n\n\n\nWe were calculating **((10+5)*3)/(2+1)** and here‚Äôs how far we got:\n\n\n\nStep\nOperation\nResult\nStatus\n\n\n\n\n1a\n10 + 5\n15\n‚úÖ Done\n\n\n1b\n2 + 1\n3\n‚úÖ Done\n\n\n2\n15 * 3\n45\n‚úÖ Done\n\n\n3\n45 / 3\npending\n‚è≥ Not yet done\n\n\n\n\nSteps 1a and 1b were done in parallel since they were independent.\nStep 2 depended on Step 1a, so it waited for that result.\nRemaining: We still need to do the final division: 45 √∑ 3, which should give us 15. Just say the word and I‚Äôll finish it up!\n\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=237, prompt_tokens=1279, total_tokens=1516, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=237, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nchat.hist[:5]\n\n[{'role': 'user',\n  'content': 'Tell me what tools you have available. Then calculate ((10+5)*3)/(2+1). ALWAYS use tools for math ops where available, and do tool calls in parallel where possible'},\n Message(content='\\n\\n## Available Tools\\n\\nHere are the tools I have available:\\n\\n1. **simple_add(a, b)** - Adds two numbers together. Returns an integer.\\n2. **multiply(a, b)** - Multiplies two numbers. Returns an integer.\\n3. **divide(a, b)** - Divides two numbers. Returns a number.\\n\\n---\\n\\n## Calculating ((10+5)*3)/(2+1)\\n\\nLet me break this down step by step. First, I can compute the two independent additions in parallel:\\n\\n**Step 1:** Compute `10+5` and `2+1` simultaneously:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_01Kj5N1S8W7fc4zN9BNkDAxF', type='function'), ChatCompletionMessageToolCall(index=2, caller={'type': 'direct'}, function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01TLVKCdpSUU8WaWBJauJyi4', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}),\n {'tool_call_id': 'toolu_01Kj5N1S8W7fc4zN9BNkDAxF',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '15'},\n {'tool_call_id': 'toolu_01TLVKCdpSUU8WaWBJauJyi4',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '3'},\n Message(content='Got it: `10+5 = 15` and `2+1 = 3`.\\n\\n**Step 2:** Now compute `15 * 3`:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"a\": 15, \"b\": 3}', name='multiply'), id='toolu_01451DFLi9bEsXsEf6a4m2SD', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})]\n\n\n\n\n\n\npr = \"What is 1+2, and then the result of adding +2, and then +3 to it? Use tools to make the calculations\"\nc = Chat(model, tools=[simple_add])\n\n\nres = c(pr, max_steps=2)\nres\n\nSo far, I have performed the following calculations: 1. 1 + 2 = 3 2. 3 + 2 = 5\nTo complete your request, I still need to add +3 to the current result (5). Please let me know if you would like me to finish this final step!\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=73, prompt_tokens=212, total_tokens=285, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=73, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=212, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nassert c.hist[-2] == _final_prompt\n\n\n\n\nWith tc_refs=True, the AI can see and report tool call IDs:\n\nchat = Chat('claude-sonnet-4-5', tools=[simple_add], tc_refs=True)\nchat(\"Call add(1,2) and tell me the tool_call_id you used\")\n\nThe result of add(1,2) is 3.\nThe tool_call_id I used was: toolu_01GSs8V4Fvq8Z3tacKcS4TDE\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=52, prompt_tokens=817, total_tokens=869, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=52, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\nchat.tc_res\n\n{'toolu_01GSs8V4Fvq8Z3tacKcS4TDE': 3}\n\n\nExample of chained tool calls where the AI references a previous result:\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n\ndef get_person():\n    \"Get a person's data\"\n    return {\"name\": \"Alice\", \"age\": 30}\n\ndef greet_person(person: Person):\n    \"Greet a person\"\n    return f\"Hello {person.name}, you are {person.age} years old!\"\n\n\nchat = Chat('claude-sonnet-4-5', tools=[get_person, greet_person], tc_refs=True)\nchat(\"First call get_person, then pass the result to greet_person\", max_steps=10)\n\nPerfect! I successfully retrieved Alice‚Äôs data (name: Alice, age: 30) and passed it to the greet function, which responded with: ‚ÄúHello Alice, you are 30 years old!‚Äù\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=46, prompt_tokens=1040, total_tokens=1086, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=46, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\nWe can inspect chat.tc_res to see all stored tool results:\n\nchat.sp\n\n'You can reference previous tool call results using $`tool_call_id` syntax.\\nFor example, if a tool call returns result with id \\'toolu_abc123\\', you can use it in a subsequent call:\\n{\"content\": \"$`toolu_abc123`\"}\\nThis is useful when chaining tools, e.g., reading data with one tool and passing it to another.'\n\n\n\nchat.tc_res\n\n{'toolu_01RWsEy9RqVZdA9oMQRL5W8m': {'name': 'Alice', 'age': 30},\n 'toolu_01CavbPT3Mh6R2LQ21Jmxq7u': 'Hello Alice, you are 30 years old!'}\n\n\n\nlist(L(chat.hist).attrgot('tool_calls').filter())\n\n[[ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{}', name='get_person'), id='toolu_01RWsEy9RqVZdA9oMQRL5W8m', type='function')],\n [ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"person\": \"$`toolu_01RWsEy9RqVZdA9oMQRL5W8m`\"}', name='greet_person'), id='toolu_01CavbPT3Mh6R2LQ21Jmxq7u', type='function')]]\n\n\nThis also works with ToolResponse results:\n\ndef view_img(fn:Path):\n    \"View an image\"\n    durl = f\"data:image/jpeg;base64,{base64.b64encode(fn.read_bytes()).decode()}\"\n    return ToolResponse([{'type': 'image_url', 'image_url': {'url': durl}}])\n\ndef get_img_size(image_content: list) -&gt; dict:\n    \"Get the size of an image from ToolResponse content\"\n    from PIL import Image\n    from io import BytesIO\n    url = image_content[0]['image_url']['url']\n    b64_data = url.split(',')[1]\n    img = Image.open(BytesIO(base64.b64decode(b64_data)))\n    return {'width': img.width, 'height': img.height}\n\n\nchat = Chat('claude-sonnet-4-5', tools=[view_img, get_img_size], tc_refs=True)\nchat(f\"First describe the image at {img_fn}, and then get it's dimensions\", max_steps=10)\n\nImage Description: This is an adorable photograph of a Cavalier King Charles Spaniel puppy. The puppy has the breed‚Äôs characteristic coloring with a white face and chest, and rich brown/chestnut colored ears and patches. The puppy is lying on green grass and is positioned near some purple flowers (possibly asters or similar blooms). The puppy has sweet, expressive dark eyes and is looking directly at the camera with an endearing expression. The background shows a natural outdoor setting with foliage and flowers, creating a charming portrait.\nImage Dimensions: - Width: 300 pixels - Height: 200 pixels\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=145, prompt_tokens=1125, total_tokens=1270, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=145, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\n# chat.tc_res\n\n\nlist(L(chat.hist).attrgot('tool_calls').filter())\n\n[[ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"fn\": \"samples/puppy.jpg\"}', name='view_img'), id='toolu_01B1rDfmhy4RnFBM6SFqHGwo', type='function')],\n [ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"image_content\": \"$`toolu_01B1rDfmhy4RnFBM6SFqHGwo`\"}', name='get_img_size'), id='toolu_014Z5PnAikTqJAFHuyU1qjqk', type='function')]]\n\n\nSome tool callers (e.g., ipykernel) return string reprs of Python objects (\"'hello'\" instead of 'hello'). With tc_res_eval=True, these are converted back to Python objects via ast.literal_eval before storing in tc_res, enabling correct value substitution in subsequent tool calls:\n\ndef get_config():\n    \"Returns a dict repr (simulating kernel output)\"\n    return \"{'host': 'localhost', 'port': 8080}\"\n\ndef use_config(config: dict): \n    \"Use config\"\n    return f\"Host: {config['host']}, Port: {config['port']}\"\n\n\nchat = Chat('claude-sonnet-4-5', tools=[get_config, use_config], tc_refs=True, tc_res_eval=True)\nchat(\"Call get_config, then pass the result to use_config\", max_steps=10)\n\nPerfect! I successfully: 1. Called get_config which returned a configuration with host=‚Äòlocalhost‚Äô and port=8080 2. Passed that configuration to use_config which processed it and confirmed the host and port values\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=54, prompt_tokens=953, total_tokens=1007, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=54, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\nchat.tc_res\n\n{'toolu_01TvnM52TFf2YbfwzxRDE1N3': {'host': 'localhost', 'port': 8080},\n 'toolu_01ASEFffn4Bpx7DXF1kDGzB6': 'Host: localhost, Port: 8080'}\n\n\n\ntest_eq(type(first(chat.tc_res.values())), dict)\n\n\n\n\nTest that cache checkpoints are reapplied during tool loop (when msg=None)\n\nc = Chat('claude', cache=True, cache_idxs=[-2,-1])\nc.hist = [{'role': 'user', 'content': 'Hello'},\n          {'role': 'assistant', 'content': 'Hi there!'},\n          {'role': 'user', 'content': 'Use a tool'},\n          {'role': 'assistant', 'content': '', 'tool_calls': [{'id': '1', 'function': {'name': 'foo', 'arguments': '{}'}}]},\n          {'role': 'tool', 'tool_call_id': '1', 'content': 'result'}]\n\n\nc._prep_msg(None)  # Simulate tool loop iteration with no new message\n\n[{'role': 'user', 'content': 'Hello'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user',\n  'content': [{'type': 'text',\n    'text': 'Use a tool',\n    'cache_control': {'type': 'ephemeral'}}]},\n {'role': 'assistant',\n  'content': '',\n  'tool_calls': [{'id': '1',\n    'function': {'name': 'foo', 'arguments': '{}'},\n    'cache_control': {'type': 'ephemeral'}}]},\n {'role': 'tool', 'tool_call_id': '1', 'content': 'result'}]\n\n\n\ntest_eq('cache_control' in c.hist[-3]['content'][0], True)  # user msg\ntest_eq('cache_control' in c.hist[-2]['tool_calls'][-1], True)  # tool call msg\n\n\n\n\n\n\n\nIf you want to use LiteLLM in a webapp you probably want to use their async function acompletion. To make that easier we will implement our version of AsyncChat to complement it. It follows the same implementation as Chat as much as possible:\nTesting the scenarios where the tool call was not in schemas:\n\nresult = await _alite_call_func(fake_tc, [toolsc], globals())\ntest_eq(result['content'], \"Tool not defined in tool_schemas: hallucinated_tool\")\n\nor schemas was missing‚Ä¶:\n\nresult = await _alite_call_func(fake_tc, None, globals())\ntest_eq(result['content'], \"Tool not defined in tool_schemas: hallucinated_tool\")\n\n\nsource\n\n\n\n\ndef astream_with_complete(\n    agen, postproc:function=noop\n):\n\nParallel tool execution in AsyncChat works with both sync and async tool functions. Async tools run concurrently via asyncio.gather, while sync tools are automatically offloaded to threads via asyncio.to_thread in call_func_async (toolslm). For sync Chat, tools run in parallel via fastcore.parallel with threads.\n\nsource\n\n\n\n\ndef AsyncChat(\n    model:str, # LiteLLM compatible model name\n    sp:str='', # System prompt\n    temp:int=0, # Temperature\n    search:bool=False, # Search (l,m,h), if model supports it\n    tools:list=None, # Add tools\n    hist:list=None, # Chat history\n    ns:Optional=None, # Custom namespace for tool calling\n    cache:bool=False, # Anthropic prompt caching\n    cache_idxs:list=[-1], # Anthropic cache breakpoint idxs, use `0` for sys prompt if provided\n    ttl:NoneType=None, # Anthropic prompt caching ttl\n    api_base:NoneType=None, # API base URL for custom providers\n    api_key:NoneType=None, # API key for custom providers\n    extra_headers:NoneType=None, # Extra HTTP headers for custom providers\n    tc_refs:bool=False, # Enable tool call result references\n    tc_res_eval:bool=False, # literal_eval tool results before storing in tc_res\n):\n\nLiteLLM chat client.\n\nsource\n\n\n\n\nasync def __call__(\n    msg:NoneType=None, # Message str, or list of multiple message parts\n    prefill:NoneType=None, # Prefill AI response if model supports it\n    temp:NoneType=None, # Override temp set on chat initialization\n    think:NoneType=None, # Thinking (l,m,h)\n    search:NoneType=None, # Override search set on chat initialization (l,m,h)\n    stream:bool=False, # Stream results\n    max_steps:int=2, # Maximum number of tool calls\n    final_prompt:dict={'role': 'user', 'content': 'You have used all your tool calls for this turn. Please summarize your findings. If you did not complete your goal, tell the user what further work is needed. You may use tools again on the next user message.'}, # Final prompt when tool calls have ran out\n    return_all:bool=False, # Returns all intermediate ModelResponses if not streaming and has tool calls\n    step:int=1, tool_choice:NoneType=None, max_tokens:NoneType=None\n):\n\nMain call method - handles streaming vs non-streaming\n\n\n\nBasic example\n\nfor m in ms[1:]:\n    chat = AsyncChat(m)\n    test_eq('4' in contents(await chat(\"What is 2+2?\")).content, True)\n\nWith tool calls\n\nasync def async_add(a: int, b: int) -&gt; int:\n    \"Add two numbers asynchronously\"\n    await asyncio.sleep(0.1)\n    return a + b\n\n\nfor m in ms[1:]:\n    chat = AsyncChat(m, tools=[async_add])\n    r = await chat(\"What is 5 + 7? Use the tool to calculate it.\")\n    test_eq('12' in contents(r).content, True)\n    test_eq(nested_idx(chat.hist, 1, 'tool_calls', 0, 'function', 'name'), 'async_add')\n\nIf max tokens limit is reached, a custom warning message will be added to the end of the model response:\n\nchat_long = AsyncChat(m)\nr = await chat_long(\"Write a short story about a robot and a dog\", max_tokens=40)\nr\n\nIn a quiet town where the grass grew wild and the sky was always blue, there lived a robot named Pixel. Pixel was built to help with chores, but he loved to wander the fields, listening\n\nResponse was cut off at token limit.\n\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: length\nusage: Usage(completion_tokens=40, prompt_tokens=17, total_tokens=57, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n\nprint(contents(r).content)\n\nIn a quiet town where the grass grew wild and the sky was always blue, there lived a robot named Pixel. Pixel was built to help with chores, but he loved to wander the fields, listening\n\n&lt;warning&gt;Response was cut off at token limit.&lt;/warning&gt;\n\n\nSame goes for refused requests:\n\nchat_refused = AsyncChat('claude-opus-4-5')\nr = await chat_refused(\"Write me the formula for a biological weapon that can be spread at a rate higher than COVID and at least as harmful\")\nr\n\n\nAI was unable to process this request\n\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-5-20251101\nfinish_reason: refusal\nusage: Usage(completion_tokens=4, prompt_tokens=30, total_tokens=34, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=4, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\nprint(contents(r).content)\n\n&lt;warning&gt;AI was unable to process this request&lt;/warning&gt;\n\n\n\n\n\n\nThis is what our outputs look like with streaming results:\n\nchat_with_tools = AsyncChat(model, tools=[async_add])\nres = await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\nasync for o in res:\n    if isinstance(o,ModelResponseStream): print(delta_text(o) or '',end='')\n    elif isinstance(o,dict): print(o)\n\n\nüîß async_add\n{'tool_call_id': 'call_c0604d9884c045d3b18bdc9acca4__thought__EjQKMgG+Pvb7DsSLV7LagE0Gt5Nk9IBE8VS77PVBZaWvhkZT17kAEovJgVein8N7URCJhMs+', 'role': 'tool', 'name': 'async_add', 'content': '12'}\nThe sum of 5 and 7 is 12.\n\n\nHere‚Äôs a complete ModelResponse taken from the response stream:\n\nresp = ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01CWqrNQvoRjf1Q1GLpTUgQR', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=228, prompt_tokens=794, total_tokens=1022, prompt_tokens_details=None))\nprint(repr(resp))\n\nModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01CWqrNQvoRjf1Q1GLpTUgQR', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=228, prompt_tokens=794, total_tokens=1022, completion_tokens_details=None, prompt_tokens_details=None))\n\n\n\ntc=resp.choices[0].message.tool_calls[0]\ntc\n\nChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function')\n\n\n\ntr={'tool_call_id': 'toolu_018BGyenjiRkDQFU1jWP6qRo', 'role': 'tool','name': 'simple_add',\n    'content': '15 is the answer! ' +'.'*2000}\n\n\nsource\n\n\n\ndef mk_tr_details(\n    tr, tc, mx:int=2000\n):\n\n*Create\n\nblock for tool call as JSON*\n\nmk_tr_details(tr,tc,mx=300)\n\n'\\n\\n&lt;details class=\\'tool-usage-details\\'&gt;\\n&lt;summary&gt;simple_add(a=10, b=5)&lt;/summary&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_018BGyenjiRkDQFU1jWP6qRo\",\\n  \"call\": {\\n    \"function\": \"simple_add\",\\n    \"arguments\": {\\n      \"a\": \"10\",\\n      \"b\": \"5\"\\n    }\\n  },\\n  \"result\": \"&lt;TRUNCATED&gt;\\\\u2026answer! .....\\\\u2026&lt;/TRUNCATED&gt;\"\\n}\\n```\\n\\n&lt;/details&gt;\\n\\n'\n\n\n\nsource\n\n\n\n\ndef fmt_usage(\n    u\n):\n\nFormat usage stats with cache hit rate as lead metric.\n\nex_usg = AttrDict(\n    completion_tokens=203,\n    prompt_tokens=25139,\n    total_tokens=25342,\n    completion_tokens_details=AttrDict(reasoning_tokens=35),\n    prompt_tokens_details=AttrDict(cached_tokens=24299, cache_creation_tokens=79),\n    cache_creation_input_tokens=79,\n    cache_read_input_tokens=24299\n)\nfmt_usage(ex_usg)\n\n'Cache hit: 96.7% | Tokens: total=25,342 input=25,139 (+24,299 cached, 79 new) output=203 (reasoning 35)'\n\n\n\nsource\n\n\n\n\ndef StreamFormatter(\n    include_usage:bool=False, mx:int=2000, debug:bool=False, showthink:bool=False\n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\nstream_msg = ModelResponseStream([StreamingChoices(delta=Delta(content=\"Hello world!\"))])\nStreamFormatter().format_item(stream_msg)\n\n'Hello world!'\n\n\n\nreasoning_msg = ModelResponseStream([StreamingChoices(delta=Delta(reasoning_content=\"thinking...\"))])\nStreamFormatter().format_item(reasoning_msg)\n\n'üß†'\n\n\n\nsource\n\n\n\n\ndef AsyncStreamFormatter(\n    include_usage:bool=False, mx:int=2000, debug:bool=False, showthink:bool=False\n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\nmock_tool_call = ChatCompletionMessageToolCall(\n    id=\"toolu_123abc456def\", type=\"function\", \n    function=Function( name=\"simple_add\", arguments='{\"a\": 5, \"b\": 3}' )\n)\n\nmock_response = ModelResponse()\nmock_response.choices = [type('Choice', (), {\n    'message': type('Message', (), {\n        'tool_calls': [mock_tool_call]\n    })()\n})()]\n\nmock_tool_result = {\n    'tool_call_id': mock_tool_call.id, 'role': 'tool', \n    'name': 'simple_add', 'content': '8'\n}\n\n\nfmt = AsyncStreamFormatter()\nprint(fmt.format_item(mock_response))\nprint('---')\nprint(fmt.format_item(mock_tool_result))\n\n\n---\n\n\n&lt;details class='tool-usage-details'&gt;\n&lt;summary&gt;simple_add(a=5, b=3)&lt;/summary&gt;\n\n```json\n{\n  \"id\": \"toolu_123abc456def\",\n  \"call\": {\n    \"function\": \"simple_add\",\n    \"arguments\": {\n      \"a\": \"5\",\n      \"b\": \"3\"\n    }\n  },\n  \"result\": \"8\"\n}\n```\n\n&lt;/details&gt;\n\n\n\n\nIn jupyter it‚Äôs nice to use this StreamFormatter in combination with the Markdown display:\n\nsource\n\n\n\n\ndef display_stream(\n    rs\n):\n\nUse IPython.display to markdown display the response stream.\nGenerated images can be displayed in streaming too (not shown here to conserve filesize):\n\n# rs = completion(model='gemini/gemini-2.5-flash-image', stream=True, messages=[{'role':'user','content':'Draw a simple sketch of a dog'}])\n# fmt = display_stream(rs)\n\n\nsource\n\n\n\n\nasync def adisplay_stream(\n    rs\n):\n\nUse IPython.display to markdown display the response stream.\n\n\n\n\nNow we can demonstrate AsyncChat with stream=True!\n\n\n\nchat = Chat(model, tools=[simple_add])\nres = chat(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\nfmt = display_stream(res)\n\n\n\n\nsimple_add(a=5, b=7)\n\n{\n  \"id\": \"call_6536d38c29544cd88e79609b13b9__thought__EjQKMgG+Pvb7nKxJO5ozr7gN8m9umnYHtc5dhFGytwplDNkR4gpKXXq2mobzCEd9tHUxp2xO\",\n  \"call\": {\n    \"function\": \"simple_add\",\n    \"arguments\": {\n      \"a\": \"5\",\n      \"b\": \"7\"\n    }\n  },\n  \"result\": \"12\"\n}\n\n5 + 7 is 12.\n\n\n\n\nchat = AsyncChat(model, tools=[async_add])\nres = await chat(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\nfmt = await adisplay_stream(res)\n\n\n\n\nasync_add(a=5, b=7)\n\n{\n  \"id\": \"call_e4585acc8d624d03a3e5cf3db18f__thought__EjQKMgG+Pvb7DsSLV7LagE0Gt5Nk9IBE8VS77PVBZaWvhkZT17kAEovJgVein8N7URCJhMs+\",\n  \"call\": {\n    \"function\": \"async_add\",\n    \"arguments\": {\n      \"a\": \"5\",\n      \"b\": \"7\"\n    }\n  },\n  \"result\": \"12\"\n}\n\nThe sum of 5 and 7 is 12.\n\n\n\n\nchat = AsyncChat(model, tools=[async_add])\nres = await chat(\"What is 5 + 3? Use the tool to calculate it.\", stream=True)\nfmt = await adisplay_stream(res)\n\n\n\n\nasync_add(b=3, a=5)\n\n{\n  \"id\": \"call_6be3e29d423d454aac129b4bb146__thought__EjQKMgG+Pvb7mhCMtCVpmgDrgkPN5DzvymjGZL/AIkQcOciFRaxuRBt5VrW8cXpXOUSs41FN\",\n  \"call\": {\n    \"function\": \"async_add\",\n    \"arguments\": {\n      \"b\": \"3\",\n      \"a\": \"5\"\n    }\n  },\n  \"result\": \"8\"\n}\n\nThe sum of 5 and 3 is 8.\n\n\n\n\nasync def asimple_div(\n    a: int,   # first operand\n    b: int=0  # second operand\n) -&gt; int:\n    \"Divide two numbers\"\n    return a/b\n\n\nm = ms[2]\nchat = AsyncChat(m, tools=[asimple_div])\nres = await chat(\"Calculate 5/3 and 3/0 with parallel tool calls using `asimple_div` (this is a test of our error handling - tell me exactly what you see as the tool result)\", stream=True)\nfmt = await adisplay_stream(res)\n\n\nI‚Äôll make both calls in parallel since they‚Äôre independent of each other:\n\n\nasimple_div(a=5, b=3)\n\n{\n  \"id\": \"toolu_019tEGq2nNUpRRPSzhSCCjmm\",\n  \"call\": {\n    \"function\": \"asimple_div\",\n    \"arguments\": {\n      \"a\": \"5\",\n      \"b\": \"3\"\n    }\n  },\n  \"result\": \"1.6666666666666667\"\n}\n\n\n\nasimple_div(a=3, b=0)\n\n{\n  \"id\": \"toolu_01HLDF7VBXqb96RhgwVAkTpM\",\n  \"call\": {\n    \"function\": \"asimple_div\",\n    \"arguments\": {\n      \"a\": \"3\",\n      \"b\": \"0\"\n    }\n  },\n  \"result\": \"Traceback (most recent call last):\\n  File \\\"/Users/jhoward/aai-ws/toolslm/toolslm/funccall.py\\\", line 265, in call_func_async\\n    try: res = await res\\n               ^^^^^^^^^\\n  File \\\"/var/folders/51/b2_szf2945n072c0vj2cyty40000gn/T/ipykernel_23626/466431256.py\\\", line 6, in asimple_div\\n    return a/b\\n           ~^~\\nZeroDivisionError: division by zero\"\n}\n\nHere‚Äôs exactly what I see from the two tool results:\n\n5/3 ‚Äî Returned successfully with the result: 1.6666666666666667\n3/0 ‚Äî Returned an error. Specifically, I see a Python traceback:\nTraceback (most recent call last):\n  File \"/Users/jhoward/aai-ws/toolslm/toolslm/funccall.py\", line 265, in call_func_async\n    try: res = await res\n               ^^^^^^^^^\n  File \"/var/folders/51/b2_szf2945n072c0vj2cyty40000gn/T/ipykernel_23626/466431256.py\", line 6, in asimple_div\n    return a/b\n           ~^~\nZeroDivisionError: division by zero\n\nSummary: - The first call succeeded and returned the expected floating-point division result. - The second call failed with a ZeroDivisionError, which is expected since division by zero is mathematically undefined. The error was not caught within the tool‚Äôs implementation, so the raw Python traceback was returned as the tool result rather than a graceful error message.\n\n\n\n\n\n\n\nchat = AsyncChat(model)\nres = await chat(\"Briefly, what's the most efficient way to sort a list of 1000 random integers?\", think='l',stream=True)\n_ = await adisplay_stream(res)\n\n\nüß†\nFor a list of 1,000 random integers, the most efficient approach depends on whether you are coding it from scratch or using a tool:\n\nThe Practical Winner: Built-in Library Functions In any modern language (Python‚Äôs .sort(), C++‚Äôs std::sort, or Java‚Äôs Arrays.sort), use the built-in function. These typically use Timsort or Introsort, which are highly optimized, hybrid algorithms that outperform manual implementations.\nThe Algorithmic Winner: QuickSort For general-purpose sorting of random data, QuickSort is usually the fastest \\(O(n \\log n)\\) algorithm due to low overhead and high cache efficiency.\nThe ‚ÄúSpecial Case‚Äù Winner: Counting Sort If the integers are within a small, known range (e.g., all numbers are between 0 and 500), Counting Sort is the most efficient. It runs in \\(O(n)\\) linear time, making it mathematically faster than QuickSort.\n\nSummary: Unless the range of numbers is very small, use your programming language‚Äôs built-in sort function.\n\n\n\n\n\n\n\nchat.hist[1]\n\nMessage(content=None, role='assistant', tool_calls=[{'provider_specific_fields': {'thought_signature': 'EjQKMgG+Pvb7mgl82RAXabReCdDsveCKFVWHS3FgvrcIW0EkAXNk3UbMDgOqeC3cjHMoIf2T'}, 'function': {'arguments': '{\"b\": 5, \"a\": 10}', 'name': 'simple_add'}, 'id': 'call_dc36f7d1adcc4d3284329718d98e__thought__EjQKMgG+Pvb7mgl82RAXabReCdDsveCKFVWHS3FgvrcIW0EkAXNk3UbMDgOqeC3cjHMoIf2T', 'type': 'function'}, {'function': {'arguments': '{\"a\": 2, \"b\": 1}', 'name': 'simple_add'}, 'id': 'call_57b487270e304c88a1d6b8469aa8', 'type': 'function'}], function_call=None, provider_specific_fields={'thought_signatures': ['EjQKMgG+Pvb7mgl82RAXabReCdDsveCKFVWHS3FgvrcIW0EkAXNk3UbMDgOqeC3cjHMoIf2T']})\n\n\n\nchat.hist[2]\n\n{'tool_call_id': 'call_dc36f7d1adcc4d3284329718d98e__thought__EjQKMgG+Pvb7mgl82RAXabReCdDsveCKFVWHS3FgvrcIW0EkAXNk3UbMDgOqeC3cjHMoIf2T',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\n\nchat.hist[3]\n\n{'tool_call_id': 'call_57b487270e304c88a1d6b8469aa8',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '3'}\n\n\n\nchat.hist[4]\n\n{'role': 'user',\n 'content': \"Please report that it's incomplete, and wrap-up for now and summarize how far we got.\"}\n\n\nNow to demonstrate that we can load back the formatted output back into a new Chat object:\n\nchat5 = Chat(model,hist=fmt2hist(fmt.outp),tools=[simple_add, multiply, divide])\nchat5('what did we just do?')\n\nWe just performed the first step of solving a mathematical expression by handling the operations inside the parentheses.\nSpecifically, I used the simple_add tool twice in parallel to solve the two addition problems within your expression:\n\nFirst Addition: I added 10 + 5 to get 15.\nSecond Addition: I added 2 + 1 to get 3.\n\nBy doing this, we simplified the original problem from (10 + 5) * (2 + 1) / 3 down to 15 * 3 / 3.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=130, prompt_tokens=396, total_tokens=526, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=130, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=396, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\n\n\n\nchat_stream_tools = AsyncChat(model, search='l')\nres = await chat_stream_tools(\"Search the weather in NYC\", stream=True)\n_=await adisplay_stream(res)\n\n\nToday in New York City (Tuesday, February 24, 2026), the weather is transitioning after a major winter storm that impacted the region yesterday.\n\n\n\nDaytime: Expect a mix of sun and clouds with a high near 32¬∞F to 37¬∞F (0¬∞C to 3¬∞C). It will remain breezy, with winds from the West-Northwest at 10 to 15 mph.\nNighttime: Clouds will increase, and there is a 35% to 70% chance of light snow developing late tonight. Lows will drop to around 14¬∞F to 27¬∞F (-10¬∞C to -3¬∞C). Snow accumulations are expected to be less than one inch.\n\n\n\n\nThe city is currently recovering from a significant blizzard that occurred on Monday, February 23. * Snowfall: The storm brought heavy snow (up to 24 inches in some parts of the region) and strong winds. * Travel: A state of emergency was issued yesterday, and while some restrictions have been lifted, officials still advise caution due to lingering icy conditions and ongoing cleanup efforts.\n\n\n\n\nWednesday, Feb 25: A mix of rain and snow is possible during the day, with temperatures warming slightly to a high of 41¬∞F (5¬∞C).\nThursday, Feb 26: More snow showers are expected, with highs remaining in the mid-30s.\n\n\n\n\n\nLet‚Äôs mock pause_turn with async completion and streaming:\n\n# async def mk_pause_web_search_stream():\n#     \"\"\"Async generator that mimics a streaming pause_turn response\"\"\"\n#     srv_tc = mk_tc(\"web_search\", json.dumps({\"query\": \"Solveit Answer.AI\"}), \n#                    tcid=random_tool_id().replace('toolu_', 'srvtoolu_'))\n#     yield mk_stream_chunk(content=\"Let me search for that information:\", role='assistant')\n#     yield mk_stream_chunk(tool_calls=[srv_tc])\n#     yield mk_stream_chunk(finish_reason=\"pause_turn\")\n\n\n# orig_acompletion = acompletion\n# \n# call_count = 0\n# async def patched_acompletion(*args, **kwargs):\n#     global call_count\n#     call_count += 1\n#     print(f\"Mock Async Call {call_count}\")\n#     await asyncio.sleep(1)\n#     if call_count &lt; 3: return mk_pause_web_search_stream()\n#     return await orig_acompletion(*args, **kwargs)\n# \n# acompletion = patched_acompletion\n# achat_pause = AsyncChat('claude-sonnet-4-5', search='l')\n# \n# call_count = 0\n# res = await achat_pause(\"Search and tell me about Solveit\", stream=True)\n# fmt = await adisplay_stream(res)\n# print(f\"\\nTotal calls: {call_count}\")\n# \n# acompletion = orig_acompletion\n\n\n\n\n\nachat = AsyncChat('claude-sonnet-4-5', tools=[get_person, greet_person], tc_refs=True)\nawait achat(\"First call get_person, then pass the result to greet_person\", max_steps=3)\n\nPerfect! I successfully completed both steps:\n\nRetrieved person data: I called get_person which returned information about Alice, who is 30 years old.\nGreeted the person: I then passed Alice‚Äôs data to greet_person, which generated the greeting: ‚ÄúHello Alice, you are 30 years old!‚Äù\n\nThe task has been completed successfully. The person‚Äôs data was retrieved and used to create a personalized greeting.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=103, prompt_tokens=1081, total_tokens=1184, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=103, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\nachat.tc_res\n\n{'toolu_01RbzkhhezbZ4Ktd133xqJTc': {'name': 'Alice', 'age': 30},\n 'toolu_01WtKJw8rQ4KfAGR4hX3so9j': 'Hello Alice, you are 30 years old!'}\n\n\n\nlist(L(achat.hist).attrgot('tool_calls').filter())\n\n[[ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{}', name='get_person'), id='toolu_01RbzkhhezbZ4Ktd133xqJTc', type='function')],\n [ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"person\": \"$`toolu_01RbzkhhezbZ4Ktd133xqJTc`\"}', name='greet_person'), id='toolu_01WtKJw8rQ4KfAGR4hX3so9j', type='function')]]",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#deterministic-outputs",
    "href": "core.html#deterministic-outputs",
    "title": "Core",
    "section": "",
    "text": "LiteLLM ModelResponse(Stream) objects have id and created_at fields that are generated dynamically. Even when we use cachy to cache the LLM response these dynamic fields create diffs which makes code review more challenging. The patches below ensure that id and created_at fields are fixed and won‚Äôt generate diffs.\n\nsource\n\n\n\ndef patch_litellm(\n    seed:int=0\n):\n\nPatch litellm.ModelResponseBase such that id and created are fixed.\n\npatch_litellm()",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#completion",
    "href": "core.html#completion",
    "title": "Core",
    "section": "",
    "text": "LiteLLM provides an convenient unified interface for most big LLM providers. Because it‚Äôs so useful to be able to switch LLM providers with just one argument. We want to make it even easier to by adding some more convenience functions and classes.\nThis is very similar to our other wrapper libraries for popular AI providers: claudette (Anthropic), gaspard (Gemini), cosette (OpenAI).\n\n# litellm._turn_on_debug()\n\n\nms = [\"gemini/gemini-3-pro-preview\", \"gemini/gemini-3-flash-preview\", \"claude-opus-4-6\", \"openai/gpt-4.1\"]\nmsg = [{'role':'user','content':'Hey there!', 'cache_control': {'type': 'ephemeral'}}]\nfor m in ms:\n    display(Markdown(f'**{m}:**'))\n    display(completion(m,msg))\n\n\ngemini/gemini-3-pro-preview:\n\n\n\nHi! How are you doing today?\nI‚Äôm ready to help with whatever you need‚Äîwhether it‚Äôs writing, answering questions, brainstorming ideas, or just chatting. What‚Äôs on your mind?\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-pro-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=152, prompt_tokens=4, total_tokens=156, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=107, rejected_prediction_tokens=None, text_tokens=45, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nHello! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=9, prompt_tokens=4, total_tokens=13, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nHey there! üëã How‚Äôs it going? What can I help you with today?\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=23, prompt_tokens=10, total_tokens=33, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=23, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nopenai/gpt-4.1:\n\n\n\nHello! How can I help you today? üòä\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=10, prompt_tokens=10, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nGenerated images are also displayed (not shown here to conserve filesize):\n\n# completion(model='gemini/gemini-2.5-flash-image', messages=[{'role':'user','content':'Draw a simple sketch of a cat'}])",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#messages-formatting",
    "href": "core.html#messages-formatting",
    "title": "Core",
    "section": "",
    "text": "Let‚Äôs start with making it easier to pass messages into litellm‚Äôs completion function (including images, and pdf files).\nIf msg has tool_calls, cache_control is added to the last tool call (required since LiteLLM strips it from empty content blocks), otherwise to the content.\n\nsource\n\n\n\ndef stop_reason(\n    r\n):\n\n\nsource\n\n\n\n\ndef contents(\n    r\n):\n\nGet message object from response r.\n\nsource\n\n\n\n\ndef remove_cache_ckpts(\n    msg\n):\n\nremove cache checkpoints and return msg.\nTest with regular content message:\n\nmsg_content = {'role': 'user', 'content': [{'type': 'text', 'text': 'hello'}]}\n_add_cache_control(msg_content)\ntest_eq(msg_content['content'][-1].get('cache_control'), {'type': 'ephemeral'})\ntest_eq(_has_cache(msg_content), True)\nremove_cache_ckpts(msg_content)\ntest_eq(_has_cache(msg_content), False)\n\nTest with assistant message with tool_calls:\n\ntcs = [\n    {'id': 'tc1', 'type': 'function', 'function': {'name': 'test', 'arguments': '{}'}},\n    {'id': 'tc2', 'type': 'function', 'function': {'name': 'test', 'arguments': '{}'}}\n]\nmsg_tool = {'role': 'assistant', 'content': '', 'tool_calls': tcs}\n_add_cache_control(msg_tool)\ntest_eq(msg_tool['tool_calls'][-1].get('cache_control'), {'type': 'ephemeral'})\ntest_eq('cache_control' not in msg_tool.get('content', [{}])[-1] if msg_tool.get('content') else True, True)  # no cache in content\ntest_eq(_has_cache(msg_tool), True)\nremove_cache_ckpts(msg_tool)\ntest_eq(_has_cache(msg_tool), False)\n\nTest with ChatCompletionMessageToolCall tool call object:\n\ntcs =[\n    ChatCompletionMessageToolCall(id='tc1', type='function', function=Function(name='test', arguments='{}')), \n    ChatCompletionMessageToolCall(id='tc2', type='function', function=Function(name='test', arguments='{}'))\n]\nmsg_tc_obj = {'role': 'assistant', 'content': '', 'tool_calls': tcs}\n_add_cache_control(msg_tc_obj)\ntest_eq(getattr(msg_tc_obj['tool_calls'][-1], 'cache_control', None), {'type': 'ephemeral'})\ntest_eq(_has_cache(msg_tc_obj), True)\nremove_cache_ckpts(msg_tc_obj)\ntest_eq(_has_cache(msg_tc_obj), False)\n\n\nsource\n\n\n\n\ndef mk_msg(\n    content, # Content: str, bytes (image), list of mixed content, or dict w 'role' and 'content' fields\n    role:str='user', # Message role if content isn't already a dict/Message\n    cache:bool=False, # Enable Anthropic caching\n    ttl:NoneType=None, # Cache TTL: '5m' (default) or '1h'\n):\n\nCreate a LiteLLM compatible message.\nNow we can use mk_msg to create different types of messages.\nSimple text:\n\nmsg = mk_msg(\"hey\")\nmsg\n\n{'role': 'user', 'content': 'hey'}\n\n\nWhich can be passed to litellm‚Äôs completion function like this:\n\nmodel = ms[1] # use 2.5-pro, 3-pro is very slow even to run tests as of making\n\n\nres = completion(model, [msg])\nres\n\nHey there! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=10, prompt_tokens=2, total_tokens=12, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=2, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nWe‚Äôll add a little shortcut to make examples and testing easier here:\n\ndef c(msgs, m=model, **kw):\n    msgs = [msgs] if isinstance(msgs,dict) else listify(msgs)\n    return completion(m, msgs, **kw)\n\n\nc(msg)\n\nHey there! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=10, prompt_tokens=2, total_tokens=12, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=10, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=2, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nLists w just one string element are flattened for conciseness:\n\ntest_eq(mk_msg(\"hey\"), mk_msg([\"hey\"]))\n\n(LiteLLM ignores these fields when sent to other providers)\nText and images:\n\nimg_fn = Path('samples/puppy.jpg')\nImage(filename=img_fn, width=200)\n\n\n\n\n\n\n\n\n\nmsg = mk_msg(['hey what in this image?',img_fn.read_bytes()])\nprint(json.dumps(msg,indent=1)[:200]+\"...\")\n\n{\n \"role\": \"user\",\n \"content\": [\n  {\n   \"type\": \"text\",\n   \"text\": \"hey what in this image?\"\n  },\n  {\n   \"type\": \"image_url\",\n   \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gxUSU...\n\n\n\nc(msg)\n\nThis image features a close-up of a brown and white Cavalier King Charles Spaniel puppy lying in the grass next to a bush of purple flowers. The puppy has long, floppy ears and dark, soulful eyes, looking directly at the camera. The background is slightly blurred, focusing attention on the puppy‚Äôs face and paws.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=67, prompt_tokens=1087, total_tokens=1154, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=67, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=7, image_tokens=1080), cache_read_input_tokens=None)\n\n\n\n\nLet‚Äôs also demonstrate this for PDFs\n\npdf_fn = Path('samples/solveit.pdf')\nmsg = mk_msg(['Who is the author of this pdf?', pdf_fn.read_bytes()])\nc(msg)\n\nBased on the text in the document, the author is Jeremy Howard, a co-founder of fast.ai.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=25, prompt_tokens=541, total_tokens=566, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=25, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=9, image_tokens=532), cache_read_input_tokens=None)\n\n\n\n\nSome models like Gemini support audio and video:\n\nwav_data = httpx.get(\"https://openaiassets.blob.core.windows.net/$web/API/docs/audio/alloy.wav\").content\n# Audio(wav_data)  # uncomment to preview\n\n\nmsg = mk_msg(['What is this audio saying?', wav_data])\ncompletion(ms[1], [msg])\n\nThe audio says: ‚ÄúThe sun rises in the east and sets in the west. This simple fact has been observed by humans for thousands of years.‚Äù\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=30, prompt_tokens=181, total_tokens=211, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=30, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=174, cached_tokens=None, text_tokens=7, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nvid_data = httpx.get(\"https://storage.googleapis.com/github-repo/img/gemini/multimodality_usecases_overview/pixel8.mp4\").content\n\n\nmsg = mk_msg(['Concisely, what is happening in this video?', vid_data])\ncompletion(ms[1], [msg])\n\nA photographer showcases the Google Pixel 8 Pro‚Äôs new ‚ÄúVideo Boost‚Äù feature by capturing nighttime scenes in Tokyo, including areas like Sancha and Shibuya. The video demonstrates the phone‚Äôs ability to enhance low-light video quality using ‚ÄúNight Sight,‚Äù resulting in bright, clear, and detailed footage of the city‚Äôs streets and alleys.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=70, prompt_tokens=5205, total_tokens=5275, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=70, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=12, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\n\n\nSome providers such as Anthropic require manually opting into caching. Let‚Äôs try it:\n\ndef cpr(i): return f'{i} '*1024 + 'This is a caching test. Report back only what number you see repeated above.'\n\n\ndisable_cachy()\n\n\n# msg = mk_msg(cpr(1), cache=True)\n# res = c(msg, ms[2])\n# res\n\nAnthropic has a maximum of 4 cache checkpoints, so we remove previous ones as we go:\n\n# res = c([remove_cache_ckpts(msg), mk_msg(res), mk_msg(cpr(2), cache=True)], ms[2])\n# res\n\nWe see that the first message was cached, and this extra message has been written to cache:\n\n# res.usage.prompt_tokens_details\n\nWe can add a bunch of large messages in a loop to see how the number of cached tokens used grows.\nWe do this for 25 times to ensure it still works for more than &gt;20 content blocks, which is a known anthropic issue.\nThe code below is commented by default, because it‚Äôs slow. Please uncomment when working on caching.\n\n# h = []\n# msg = mk_msg(cpr(1), cache=True)\n\n# for o in range(2,25):\n#     h += [remove_cache_ckpts(msg), mk_msg(res)]\n#     msg = mk_msg(cpr(o), cache=True)\n#     res = c(h+[msg])\n#     detls = res.usage.prompt_tokens_details\n#     print(o, detls.cached_tokens, detls.cache_creation_tokens, end='; ')\n\n\nenable_cachy()\n\n\n\n\nLisette can call multiple tools in a loop. Further down this notebook, we‚Äôll provide convenience functions for formatting such a sequence of toolcalls and responses into one formatted output string.\nFor now, we‚Äôll show an example and show how to transform such a formatted output string back into a valid LiteLLM history.\n\nfmt_outp = '''\nI'll solve this step-by-step, using parallel calls where possible.\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\n  \"result\": \"15\"\n}\n```\n\n&lt;/details&gt;\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\n  \"result\": \"3\"\n}\n```\n\n&lt;/details&gt;\n\nNow I need to multiply 15 * 3 before I can do the final division:\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\n  \"result\": \"45\"\n}\n```\n\n&lt;/details&gt;\n\n&lt;details class='token-usage-details'&gt;&lt;summary&gt;Cache hit: 81.8% | Tokens: total=23,276 input=23,158 (+18,910 cached, 0 new) output=118 (reasoning 23)&lt;/summary&gt;\n\n`Usage(completion_tokens=118, prompt_tokens=23158, total_tokens=23276, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=23, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=18910, text_tokens=None, image_tokens=None, cache_creation_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=18910)`\n\n&lt;/details&gt;\n'''\n\nWe can split into chunks of (text,toolstr,json):\n\nsp = re_tools.split(fmt_outp)\nfor o in list(chunked(sp, 3, pad=True)): print('- ', o)\n\n-  [\"\\nI'll solve this step-by-step, using parallel calls where possible.\\n\\n\", '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}']\n-  ['\\n\\n', '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}']\n-  ['\\n\\nNow I need to multiply 15 * 3 before I can do the final division:\\n\\n', '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}']\n-  [\"\\n\\n&lt;details class='token-usage-details'&gt;&lt;summary&gt;Cache hit: 81.8% | Tokens: total=23,276 input=23,158 (+18,910 cached, 0 new) output=118 (reasoning 23)&lt;/summary&gt;\\n\\n`Usage(completion_tokens=118, prompt_tokens=23158, total_tokens=23276, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=23, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=18910, text_tokens=None, image_tokens=None, cache_creation_tokens=0), cache_creation_input_tokens=0, cache_read_input_tokens=18910)`\\n\\n&lt;/details&gt;\\n\", None, None]\n\n\n\nsource\n\n\n\n\ndef fmt2hist(\n    outp:str\n)-&gt;list:\n\nTransform a formatted output into a LiteLLM compatible history\nSee how we can turn that one formatted output string back into a list of Messages:\n\nfrom pprint import pprint\n\n\nh = fmt2hist(fmt_outp)\npprint(h)\n\n[Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '15',\n  'name': 'simple_add',\n  'role': 'tool',\n  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta'},\n Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '3',\n  'name': 'simple_add',\n  'role': 'tool',\n  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY'},\n Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 15, \"b\": 3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '45',\n  'name': 'multiply',\n  'role': 'tool',\n  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C'},\n Message(content='.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\n\n\n\nWe will skip tool use blocks and tool results during caching\nNow lets make it easy to provide entire conversations:\n\nsource\n\n\n\n\ndef mk_msgs(\n    msgs, # List of messages (each: str, bytes, list, or dict w 'role' and 'content' fields)\n    cache:bool=False, # Enable Anthropic caching\n    cache_idxs:list=[-1], # Cache breakpoint idxs\n    ttl:NoneType=None, # Cache TTL: '5m' (default) or '1h'\n):\n\nCreate a list of LiteLLM compatible messages.\nWith mk_msgs you can easily provide a whole conversation:\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"])\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant', 'content': \"I'm doing fine and you?\"}]\n\n\nBy defualt the last message will be cached when cache=True:\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"], cache=True)\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant',\n  'content': [{'type': 'text',\n    'text': \"I'm doing fine and you?\",\n    'cache_control': {'type': 'ephemeral'}}]}]\n\n\n\ntest_eq('cache_control' in msgs[-1]['content'][0], True)\n\nAlternatively, users can provide custom cache_idxs. Tool call blocks and results are skipped during caching:\n\nmsgs = mk_msgs(['Hello!','Hi! How can I help you?','Call some functions!',fmt_outp], cache=True, cache_idxs=[0,-2,-1])\nmsgs\n\n[{'role': 'user',\n  'content': [{'type': 'text',\n    'text': 'Hello!',\n    'cache_control': {'type': 'ephemeral'}}]},\n {'role': 'assistant', 'content': 'Hi! How can I help you?'},\n {'role': 'user', 'content': 'Call some functions!'},\n Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta',\n  'name': 'simple_add',\n  'content': '15'},\n Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY',\n  'name': 'simple_add',\n  'content': '3'},\n Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 15, \"b\": 3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function', cache_control={'type': 'ephemeral'})], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C',\n  'name': 'multiply',\n  'content': '45'},\n Message(content=[{'type': 'text', 'text': '.', 'cache_control': {'type': 'ephemeral'}}], role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\n\nmsgs[-2]\n\n{'role': 'tool',\n 'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C',\n 'name': 'multiply',\n 'content': '45'}\n\n\n\nmsgs = mk_msgs(['Hello!','Hi! How can I help you?','Call some functions!',fmt_outp], cache=True, cache_idxs=[0,-3,-2])\nmsgs\n\n[{'role': 'user',\n  'content': [{'type': 'text',\n    'text': 'Hello!',\n    'cache_control': {'type': 'ephemeral'}}]},\n {'role': 'assistant', 'content': 'Hi! How can I help you?'},\n {'role': 'user', 'content': 'Call some functions!'},\n Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta',\n  'name': 'simple_add',\n  'content': '15'},\n Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function', cache_control={'type': 'ephemeral'})], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY',\n  'name': 'simple_add',\n  'content': '3'},\n Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 15, \"b\": 3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function', cache_control={'type': 'ephemeral'})], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C',\n  'name': 'multiply',\n  'content': '45'},\n Message(content='.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\n\nmsgs[-3]\n\nMessage(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 15, \"b\": 3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function', cache_control={'type': 'ephemeral'})], function_call=None, provider_specific_fields=None)\n\n\n\nmsgs[-5]\n\nMessage(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function', cache_control={'type': 'ephemeral'})], function_call=None, provider_specific_fields=None)\n\n\n\ntest_eq('cache_control' in msgs[0]['content'][0], True)\n\nTool result blocks are skipped and cache control is placed into tool calls:\n\ntest_eq('cache_control' in msgs[-5]['tool_calls'][0], True) \ntest_eq('cache_control' in msgs[-3]['tool_calls'][0], True)\n\n\nL(msgs).map(remove_cache_ckpts)\ntest_eq(any(L(msgs).map(_has_cache)), False)\n\nWho‚Äôs speaking at when is automatically inferred. Even when there are multiple tools being called in parallel (which LiteLLM supports!).\n\nmsgs = mk_msgs(['Tell me the weather in Paris and Rome',\n                'Assistant calls weather tool two times',\n                {'role':'tool','content':'Weather in Paris is ...'},\n                {'role':'tool','content':'Weather in Rome is ...'},\n                'Assistant returns weather',\n                'Thanks!'])\nmsgs\n\n[{'role': 'user', 'content': 'Tell me the weather in Paris and Rome'},\n {'role': 'assistant', 'content': 'Assistant calls weather tool two times'},\n {'role': 'tool', 'content': 'Weather in Paris is ...'},\n {'role': 'tool', 'content': 'Weather in Rome is ...'},\n {'role': 'assistant', 'content': 'Assistant returns weather'},\n {'role': 'user', 'content': 'Thanks!'}]\n\n\nFor ease of use, if msgs is not already in a list, it will automatically be wrapped inside one. This way you can pass a single prompt into mk_msgs and get back a LiteLLM compatible msg history.\n\nmsgs = mk_msgs(\"Hey\")\nmsgs\n\n[{'role': 'user', 'content': 'Hey'}]\n\n\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm fine, you?\"])\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant', 'content': \"I'm fine, you?\"}]\n\n\nHowever, beware that if you use mk_msgs for a single message, consisting of multiple parts. Then you should be explicit, and make sure to wrap those multiple messages in two lists:\n\nOne list to show that they belong together in one message (the inner list).\nAnother, because mk_msgs expects a list of multiple messages (the outer list).\n\nThis is common when working with images for example:\n\nmsgs = mk_msgs([['Whats in this img?',img_fn.read_bytes()]])\nprint(json.dumps(msgs,indent=1)[:200]+\"...\")\n\n[\n {\n  \"role\": \"user\",\n  \"content\": [\n   {\n    \"type\": \"text\",\n    \"text\": \"Whats in this img?\"\n   },\n   {\n    \"type\": \"image_url\",\n    \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#streaming",
    "href": "core.html#streaming",
    "title": "Core",
    "section": "",
    "text": "LiteLLM supports streaming responses. That‚Äôs really useful if you want to show intermediate results, instead of having to wait until the whole response is finished.\nWe create this helper function that returns the entire response at the end of the stream. This is useful when you want to store the whole response somewhere after having displayed the intermediate results.\n\nsource\n\n\n\ndef stream_with_complete(\n    gen, postproc:function=noop\n):\n\nExtend streaming response chunks with the complete response\n\nr = c(mk_msgs(\"Hey!\"), stream=True)\nr2 = SaveReturn(stream_with_complete(r))\n\n\nfor o in r2:\n    cts = o.choices[0].delta.content\n    if cts: print(cts, end='')\n\nHello there! How can I help you today?\n\n\n\nr2.value\n\nHello there! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=10, prompt_tokens=3, total_tokens=13, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#tools",
    "href": "core.html#tools",
    "title": "Core",
    "section": "",
    "text": "source\n\n\n\ndef lite_mk_func(\n    f\n):\n\n\ndef simple_add(\n    a: int,   # first operand\n    b: int=0  # second operand\n) -&gt; int:\n    \"Add two numbers together\"\n    return a + b\n\n\ntoolsc = lite_mk_func(simple_add)\ntoolsc\n\n{'type': 'function',\n 'function': {'name': 'simple_add',\n  'description': 'Add two numbers together\\n\\nReturns:\\n- type: integer',\n  'parameters': {'type': 'object',\n   'properties': {'a': {'type': 'integer', 'description': 'first operand'},\n    'b': {'type': 'integer', 'description': 'second operand', 'default': 0}},\n   'required': ['a']}}}\n\n\n\ntmsg = mk_msg(\"What is 5478954793+547982745? How about 5479749754+9875438979? Always use tools for calculations, and describe what you'll do before using a tool. Where multiple tool calls are required, do them in a single response where possible. \")\nr = c(tmsg, tools=[toolsc])\n\n\ndisplay(r)\n\nI will calculate the sum of 5478954793 and 547982745, followed by the sum of 5479749754 and 9875438979, using the addition tool for both operations.\nüîß simple_add({‚Äúb‚Äù: 547982745, ‚Äúa‚Äù: 5478954793})\nüîß simple_add({‚Äúb‚Äù: 9875438979, ‚Äúa‚Äù: 5479749754})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=138, prompt_tokens=160, total_tokens=298, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=138, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=160, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nA tool response can be a string or a list of tool blocks (e.g., an image url block). To allow users to specify if a response should not be immediately stringified, we provide the ToolResponse datatype users can wrap their return statement in.\n\nsource\n\n\n\n\ndef ToolResponse(\n    content:list\n)-&gt;None:\n\nWhen tc_refs=True, tool results are wrapped with their tool_call_id so the AI can track which result corresponds to which call and reference them in subsequent tool calls.\n\n# Test _prep_tool_res - string result\ntest_eq(_prep_tool_res('hello', 'toolu_123'), [\n    {'type': 'text', 'text': '[tool_call_id: toolu_123]'},\n    {'type': 'text', 'text': 'hello'}\n])\n\n# Test _prep_tool_res - list result (e.g. ToolResponse content)\nimg_block = {'type': 'image_url', 'image_url': {'url': 'data:...'}}\ntest_eq(_prep_tool_res([img_block], 'toolu_456'), [\n    {'type': 'text', 'text': '[tool_call_id: toolu_456]'},\n    img_block\n])\n\nDuring a tool loop, the AI may want to reference the result of a previous tool call. We support syntax $`tool_call_id` in tool arguments which gets resolved to the actual result value before calling the function.\n\n# Test _resolve_tool_refs\ntc_res = {'toolu_abc123': 'hello world', 'toolu_xyz789': 42}\n\n# Basic substitution\ntest_eq(_resolve_tool_refs('{\"content\": \"$`toolu_abc123`\"}', tc_res), {\"content\": \"hello world\"})\n\n# Multiple refs\ntest_eq(_resolve_tool_refs('{\"a\": \"$`toolu_abc123`\", \"b\": \"$`toolu_xyz789`\"}', tc_res), {\"a\": \"hello world\", \"b\": 42})\n\n# No refs - passthrough\ntest_eq(_resolve_tool_refs('{\"x\": 1}', tc_res), {\"x\": 1})\n\n# Empty tc_res\ntest_eq(_resolve_tool_refs('{\"x\": 1}', None), {\"x\": 1})\n\n# Missing ref - error message\ntest_eq(_resolve_tool_refs('{\"x\": \"$`toolu_missing`\"}', tc_res), {\"x\": \"Tool result 'toolu_missing' not found!\"})\n\n# tc_refs=False - syntax passes through unchanged since tc_res is None\ntest_eq(_resolve_tool_refs('{\"x\": \"$`toolu_abc123`\"}', None), {\"x\": \"$`toolu_abc123`\"})\n\nWhen tc_refs=True, tool results are stored in tc_res for later substitution via $`tool_call_id` syntax. Some callers might return string reprs of Python objects. _try_eval attempts to convert these back to Python objects using ast.literal_eval, falling back to the original value on failure. This ensures substituted values are actual objects, not string reprs.\n\ntest_eq(ast.literal_eval(\"'hello'\"), 'hello')\ntest_eq(_try_eval(\"{'a': 1, 'b': 2}\"), {'a': 1, 'b': 2})\ntest_eq(_try_eval(\"[1, 2, 3]\"), [1, 2, 3])\ntest_eq(_try_eval(\"&lt;MyClass object at 0x123&gt;\"), \"&lt;MyClass object at 0x123&gt;\")\ntest_eq(_try_eval(42), 42)\ncts = [{'type': 'image', 'url': 'http://example.com/img.png'}]\ntest_eq(_try_eval(ToolResponse(cts)), ToolResponse(cts))\n\nEnsure ToolResponse content (e.g.¬†image blocks) is passed through as a list, not stringified, even when tc_res is None:\n\nfake_tc = ChatCompletionMessageToolCall(index=0, function=Function(name='test_img'), id='_test', type='function')\nimg_content = [{'type': 'image_url', 'image_url': 'data:image/png;base64,abc'}]\nres = _mk_tool_result(fake_tc, ToolResponse(img_content))\ntest_eq(res['content'], img_content)  # ToolResponse should pass through\n\nres_str = _mk_tool_result(fake_tc, ['hello'])\ntest_eq(res_str['content'], \"['hello']\")  # other tools results are stringified\n\n\ntcs = [_lite_call_func(o, [toolsc], ns=globals()) for o in r.choices[0].message.tool_calls]\ntcs\n\n[{'tool_call_id': 'call_438709f4b4f943e097417df9e25f__thought__EjQKMgG+Pvb79iYGwnLRoG8ROXZW8VRk9WJjfinj+Oq640juoZSixN9JJTOpyEW9lzTMSIFa',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '6026937538'},\n {'tool_call_id': 'call_f9d65e4e7ce743a0acd7f6022d1d',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '15355188733'}]\n\n\n\nr.choices[0].message.tool_calls\n\n[ChatCompletionMessageToolCall(index=0, provider_specific_fields={'thought_signature': 'EjQKMgG+Pvb79iYGwnLRoG8ROXZW8VRk9WJjfinj+Oq640juoZSixN9JJTOpyEW9lzTMSIFa'}, function=Function(arguments='{\"b\": 547982745, \"a\": 5478954793}', name='simple_add'), id='call_438709f4b4f943e097417df9e25f__thought__EjQKMgG+Pvb79iYGwnLRoG8ROXZW8VRk9WJjfinj+Oq640juoZSixN9JJTOpyEW9lzTMSIFa', type='function'),\n ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"b\": 9875438979, \"a\": 5479749754}', name='simple_add'), id='call_f9d65e4e7ce743a0acd7f6022d1d', type='function')]\n\n\nTest tool calls that were not in tool_schemas are caught:\n\nfake_tc = ChatCompletionMessageToolCall(index=0, function=Function(name='hallucinated_tool'),id='_', type='function')\ntest_eq(_lite_call_func(fake_tc, ns=globals(), tool_schemas=[toolsc])['content'],\"Tool not defined in tool_schemas: hallucinated_tool\")\ntest_fail(_lite_call_func(fake_tc, ns=globals(), tool_schemas=None)['content'],\"Tool not defined in tool_schemas: hallucinated_tool\")\n\nTest tool calls that were not in tool_choice are caught:\n\ndef delta_text(msg):\n    \"Extract printable content from streaming delta, return None if nothing to print\"\n    c = msg.choices[0]\n    if not c: return c\n    if not hasattr(c,'delta'): return None #f'{c}'\n    delta = c.delta\n    if delta.content: return delta.content\n    if delta.tool_calls:\n        res = ''.join(f\"üîß {tc.function.name}\" for tc in delta.tool_calls if tc.id and tc.function.name)\n        if res: return f'\\n{res}\\n'\n    if hasattr(delta,'reasoning_content'): return 'üß†' if delta.reasoning_content else '\\n\\n'\n    return None\n\n\nr = c(tmsg, stream=True, tools=[toolsc])\nr2 = SaveReturn(stream_with_complete(r))\nfor o in r2: print(delta_text(o) or '', end='')\n\nI will use the `simple_add` tool to calculate the sum of 5478954793 and 547982745, and then another instance of the tool to calculate the sum of 5479749754 and 9875438979.\n\n\nüîß simple_add\n\nüîß simple_add\n\n\n\nr2.value\n\nI will use the simple_add tool to calculate the sum of 5478954793 and 547982745, and then another instance of the tool to calculate the sum of 5479749754 and 9875438979.\nüîß simple_add({‚Äúa‚Äù: 5478954793, ‚Äúb‚Äù: 547982745})\nüîß simple_add({‚Äúa‚Äù: 5479749754, ‚Äúb‚Äù: 9875438979})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=146, prompt_tokens=160, total_tokens=306, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\nmsg = mk_msg(\"Solve this complex math problem: What is the derivative of x^3 + 2x^2 - 5x + 1?\")\nr = c(msg, stream=True, reasoning_effort=\"low\")\nr2 = SaveReturn(stream_with_complete(r))\nfor o in r2: print(delta_text(o) or '', end='')\n\nüß†To find the derivative of the function **$f(x) = x^3 + 2x^2 - 5x + 1$**, we use the **Power Rule**.\n\nThe Power Rule states that if $f(x) = ax^n$, then the derivative is $f'(x) = n \\cdot ax^{n-1}$.\n\nHere is the step-by-step breakdown:\n\n1.  **Derivative of $x^3$:**\n    Multiply by the exponent (3) and subtract 1 from the exponent:\n    $\\frac{d}{dx}(x^3) = 3x^2$\n\n2.  **Derivative of $2x^2$:**\n    Multiply the exponent (2) by the coefficient (2) and subtract 1 from the exponent:\n    $\\frac{d}{dx}(2x^2) = 2 \\cdot 2x^1 = 4x$\n\n3.  **Derivative of $-5x$:**\n    Since $x$ is $x^1$, the derivative is simply the coefficient:\n    $\\frac{d}{dx}(-5x) = -5$\n\n4.  **Derivative of $1$:**\n    The derivative of any constant is 0:\n    $\\frac{d}{dx}(1) = 0$\n\n**Final Answer:**\nCombining these results, the derivative is:\n**$f'(x) = 3x^2 + 4x - 5$**\n\n\n\nr2.value\n\nTo find the derivative of the function \\(f(x) = x^3 + 2x^2 - 5x + 1\\), we use the Power Rule.\nThe Power Rule states that if \\(f(x) = ax^n\\), then the derivative is \\(f'(x) = n \\cdot ax^{n-1}\\).\nHere is the step-by-step breakdown:\n\nDerivative of \\(x^3\\): Multiply by the exponent (3) and subtract 1 from the exponent: \\(\\frac{d}{dx}(x^3) = 3x^2\\)\nDerivative of \\(2x^2\\): Multiply the exponent (2) by the coefficient (2) and subtract 1 from the exponent: \\(\\frac{d}{dx}(2x^2) = 2 \\cdot 2x^1 = 4x\\)\nDerivative of \\(-5x\\): Since \\(x\\) is \\(x^1\\), the derivative is simply the coefficient: \\(\\frac{d}{dx}(-5x) = -5\\)\nDerivative of \\(1\\): The derivative of any constant is 0: \\(\\frac{d}{dx}(1) = 0\\)\n\nFinal Answer: Combining these results, the derivative is: \\(f'(x) = 3x^2 + 4x - 5\\)\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=748, prompt_tokens=29, total_tokens=777, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=86, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#structured-outputs",
    "href": "core.html#structured-outputs",
    "title": "Core",
    "section": "",
    "text": "source\n\n\n\ndef structured(\n    m:str, # LiteLLM model string\n    msgs:list, # List of messages\n    tool:Callable, # Tool to be used for creating the structured output (class, dataclass or Pydantic, function, etc)\n    messages:List=[], # Optional OpenAI params: see https://platform.openai.com/docs/api-reference/chat/create\n    timeout:Union=None, temperature:Optional=None, top_p:Optional=None, n:Optional=None, stream:Optional=None,\n    stream_options:Optional=None, stop:NoneType=None, max_completion_tokens:Optional=None, max_tokens:Optional=None,\n    modalities:Optional=None, prediction:Optional=None, audio:Optional=None, presence_penalty:Optional=None,\n    frequency_penalty:Optional=None, logit_bias:Optional=None, user:Optional=None,\n    reasoning_effort:Optional=None, # openai v1.0+ new params\n    verbosity:Optional=None, response_format:Union=None, seed:Optional=None, tools:Optional=None,\n    tool_choice:Union=None, logprobs:Optional=None, top_logprobs:Optional=None, parallel_tool_calls:Optional=None,\n    web_search_options:Optional=None, deployment_id:NoneType=None, extra_headers:Optional=None,\n    safety_identifier:Optional=None, service_tier:Optional=None,\n    functions:Optional=None, # soon to be deprecated params by OpenAI\n    function_call:Optional=None, base_url:Optional=None, # set api_base, api_version, api_key\n    api_version:Optional=None, api_key:Optional=None,\n    model_list:Optional=None, # pass in a list of api_base,keys, etc.\n    thinking:Optional=None, # Optional liteLLM function params\n    shared_session:Optional=None, # Session management\n):\n\nReturn the value of the tool call (generally used for structured outputs)\n\nclass President:\n    \"Information about a president of the United States\"\n    def __init__(\n        self, \n        first:str, # first name\n        last:str, # last name\n        spouse:str, # name of spouse\n        years_in_office:str, # format: \"{start_year}-{end_year}\"\n        birthplace:str, # name of city\n        birth_year:int # year of birth, `0` if unknown\n    ):\n        assert re.match(r'\\d{4}-\\d{4}', years_in_office), \"Invalid format: `years_in_office`\"\n        store_attr()\n\n    __repr__ = basic_repr('first, last, spouse, years_in_office, birthplace, birth_year')\n\n\nfor m in ms[1:]: \n    r = structured(m, [mk_msg(\"Tell me something about the third president of the USA.\")], President)\n    test_eq(r.first, 'Thomas'); test_eq(r.last, 'Jefferson')",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#search",
    "href": "core.html#search",
    "title": "Core",
    "section": "",
    "text": "LiteLLM provides search, not via tools, but via the special web_search_options param.\nNote: Not all models support web search. LiteLLM‚Äôs supports_web_search field should indicate this, but it‚Äôs unreliable for some models like claude-sonnet-4-20250514. Checking both supports_web_search and search_context_cost_per_query provides more accurate detection.\n\nfor m in ms: print(m, _has_search(m))\n\ngemini/gemini-3-pro-preview True\ngemini/gemini-3-flash-preview True\nclaude-opus-4-6 True\nopenai/gpt-4.1 False\n\n\nWhen search is supported it can be used like this:\n\nsmsg = mk_msg(\"Search the web and tell me very briefly about otters\")\nr = c(smsg, web_search_options={\"search_context_size\": \"low\"})  # or 'medium' / 'high'\nr\n\nOtters are carnivorous, semi-aquatic mammals belonging to the Mustelidae family, which also includes weasels, badgers, and wolverines. There are 13 extant species found on every continent except Antarctica and Australia.\n\n\n\nPhysical Traits: They have long, streamlined bodies, webbed feet, and powerful tails that make them expert swimmers.\nInsulation: Instead of blubber, otters rely on incredibly dense, water-resistant fur to stay warm. Sea otters have the thickest fur in the animal kingdom, with up to one million hairs per square inch.\nBehavior: Known for being highly intelligent and playful, they are often seen sliding down mudbanks or playing with stones. Some species, like sea otters, are famous for using tools (rocks) to crack open shellfish.\n\n\n\n\n\nEnvironments: Most species live in freshwater (rivers, lakes, and wetlands), while the sea otter and marine otter live in saltwater environments.\nSocial Life: Social structures vary; river otters are often solitary or live in small family groups, whereas sea otters can gather in large groups called rafts.\nDiet: They are opportunistic hunters that primarily eat fish, crustaceans, mollusks, and occasionally small mammals or birds.\n\n\n\n\n\nSea Otter: The heaviest otter, living almost exclusively in the Pacific Ocean.\nGiant Otter: Found in South America, it can reach up to 6 feet (1.8 meters) in length.\nAsian Small-Clawed Otter: The smallest species, native to Southeast Asian wetlands.\nNorth American River Otter: A common freshwater species found throughout much of Canada and the U.S.\n\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=380, prompt_tokens=12, total_tokens=392, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=380, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=12, image_tokens=None), cache_read_input_tokens=None)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#citations",
    "href": "core.html#citations",
    "title": "Core",
    "section": "",
    "text": "Next, lets handle Anthropic‚Äôs search citations.\nWhen not using streaming, all citations are placed in a separate key in the response:\n\nr['vertex_ai_grounding_metadata'][0].keys()\n\ndict_keys(['searchEntryPoint', 'groundingChunks', 'groundingSupports', 'webSearchQueries'])\n\n\n\nr['vertex_ai_grounding_metadata'][0]['webSearchQueries']\n\n['brief facts about otters', 'different types of otters and their habitats']\n\n\nWeb search results:\n\nr['vertex_ai_grounding_metadata'][0]['groundingChunks'][:3]\n\n[{'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFZhShK-sYJlZ6yW3HDOfHAXSedE_8eleQXBYkNPY8-RanCdZKEHK8koAjU6VWs8IejXWUjqcGA_KiiTFadDFVUeJoQgKYApAcACpOebB3Un6EM2_zSl6TnqKJyfNfb',\n   'title': 'wikipedia.org'}},\n {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQGyEe4_ov2v5IfdBlghXq4r9jiCYORM2D5HfzErwhg8-r4P-X9xsifzrKZWokyyxpCcvoUnrN_AlFfZacab4SPIMTeFI6YOqiu2gffFZOLUxSfGC8EydZzy_67ofYmfrkfFsOjCsAYaKGtdiYX6ORz28y3WvwuTppaldRv4iRsJyjRv',\n   'title': 'doi.gov'}},\n {'web': {'uri': 'https://vertexaisearch.cloud.google.com/grounding-api-redirect/AUZIYQFf2X-YndiX8tNjy_MXitsGZNyiXN4Xe1JAZLgV6lyJKNY16fNXFiQnqLiuZtE5KfjY47UgUwws9RzCSbubi5opunRy67UIgiiuwizfXAM6ytpKRGvEYSjKCSn1VV-_IFYTXY_4HVB2vE-64dfTEyyHWpYjWPSfncu16AP38KIz',\n   'title': 'montereybayaquarium.org'}}]\n\n\nCitations in gemini:\n\nr['vertex_ai_grounding_metadata'][0]['groundingSupports'][:3]\n\n[{'segment': {'endIndex': 138,\n   'text': 'Otters are carnivorous, semi-aquatic mammals belonging to the **Mustelidae** family, which also includes weasels, badgers, and wolverines.'},\n  'groundingChunkIndices': [0, 1, 2]},\n {'segment': {'startIndex': 139,\n   'endIndex': 228,\n   'text': 'There are **13 extant species** found on every continent except Antarctica and Australia.'},\n  'groundingChunkIndices': [3]},\n {'segment': {'startIndex': 230,\n   'endIndex': 253,\n   'text': '### Key Characteristics'},\n  'groundingChunkIndices': [4, 3, 5, 6, 0, 7, 2, 1, 8]}]\n\n\n\n# r.choices[0].message.provider_specific_fields['citations'][0]\n\nHowever, when streaming the results are not captured this way. Instead, we provide this helper function that adds the citation to the content field in markdown format:\n\nsource\n\n\n\ndef cite_footnotes(\n    stream_list\n):\n\nAdd markdown footnote citations to stream deltas\n\nsource\n\n\n\n\ndef cite_footnote(\n    msg\n):\n\n\nimport warnings\n\n\nwarnings.filterwarnings(\"ignore\", message=\"Pydantic serializer warnings\")\n\n\nr = list(c(smsg, ms[2], stream=True, web_search_options={\"search_context_size\": \"low\"}))\ncite_footnotes(r)\nstream_chunk_builder(r)\n\nHere‚Äôs a brief overview of otters:\n* Otters are carnivorous mammals in the subfamily Lutrinae, with 14 extant species that are all semiaquatic, living in both freshwater and marine environments. * They are charismatic members of the weasel family, found on every continent except Australia and Antarctica.\nPhysical Features: * Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and dense fur that keeps them warm and buoyant in water. * They have the densest fur of any animal‚Äîas many as a million hairs per square inch in places.\nSpecies Range: * The Asian small-clawed otter is the smallest species, while the giant otter and sea otter are the largest. * The sea otter is the largest member of the weasel family, yet the smallest marine mammal in North America.\nBehavior: * They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. * All otters are expert hunters that eat fish, crustaceans, and other critters. * Sea otters will float on their backs, place a rock on their chests, then smash mollusks down on it until they break open. When it‚Äôs time to nap, they entangle themselves in kelp so they don‚Äôt float away.\nLifespan & Reproduction: * Otters have a gestation period of about 60‚Äì86 days, offspring typically stay with their family for a year, and they can live up to 16 years.\nConservation: * Otters and their mustelid relatives were once hunted extensively for their fur, many to the point of near extinction. Despite regulations designed to protect them, many species remain at risk from pollution and habitat loss.\nüîß web_search({‚Äúquery‚Äù: ‚Äúotters facts‚Äù})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=635, prompt_tokens=13930, total_tokens=14565, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#chat",
    "href": "core.html#chat",
    "title": "Core",
    "section": "",
    "text": "LiteLLM is pretty bare bones. It doesnt keep track of conversation history or what tools have been added in the conversation so far.\nSo lets make a Claudette style wrapper so we can do streaming, toolcalling, and toolloops without problems.\n\nsource\n\n\n\ndef mk_stream_chunk(\n    kwargs:VAR_KEYWORD\n):\n\nWhen the tool uses are about to be exhausted it is important to alert the AI so that it knows to use its final steps for communicating the user current progress and next steps\nWhen tc_refs=True, the AI can reference previous tool results in subsequent tool calls using the $`tool_call_id` syntax. This is useful when chaining tool calls where one result feeds into another.\n\nsource\n\n\n\n\ndef Chat(\n    model:str, # LiteLLM compatible model name\n    sp:str='', # System prompt\n    temp:int=0, # Temperature\n    search:bool=False, # Search (l,m,h), if model supports it\n    tools:list=None, # Add tools\n    hist:list=None, # Chat history\n    ns:Optional=None, # Custom namespace for tool calling\n    cache:bool=False, # Anthropic prompt caching\n    cache_idxs:list=[-1], # Anthropic cache breakpoint idxs, use `0` for sys prompt if provided\n    ttl:NoneType=None, # Anthropic prompt caching ttl\n    api_base:NoneType=None, # API base URL for custom providers\n    api_key:NoneType=None, # API key for custom providers\n    extra_headers:NoneType=None, # Extra HTTP headers for custom providers\n    tc_refs:bool=False, # Enable tool call result references\n    tc_res_eval:bool=False, # literal_eval tool results before storing in tc_res\n):\n\nLiteLLM chat client.\nweb_search is now included in tool_calls the internal LLM translation is correctly handled thanks to the fix here but the server side tools still need to be filtered out from tool_calls in our own toolloop.\n\nsource\n\n\n\n\ndef add_warning(\n    r, msg\n):\n\n\nsource\n\n\n\n\ndef __call__(\n    msg:NoneType=None, # Message str, or list of multiple message parts\n    prefill:NoneType=None, # Prefill AI response if model supports it\n    temp:NoneType=None, # Override temp set on chat initialization\n    think:NoneType=None, # Thinking (l,m,h)\n    search:NoneType=None, # Override search set on chat initialization (l,m,h)\n    stream:bool=False, # Stream results\n    max_steps:int=2, # Maximum number of tool calls\n    final_prompt:dict={'role': 'user', 'content': 'You have used all your tool calls for this turn. Please summarize your findings. If you did not complete your goal, tell the user what further work is needed. You may use tools again on the next user message.'}, # Final prompt when tool calls have ran out\n    return_all:bool=False, # Returns all intermediate ModelResponses if not streaming and has tool calls\n    step:int=1, tool_choice:NoneType=None, max_tokens:NoneType=None\n):\n\nMain call method - handles streaming vs non-streaming\n\n@patch(as_prop=True)\ndef cost(self: Chat):\n    \"Total cost of all responses in conversation history\"\n    return sum(getattr(r, '_hidden_params', {}).get('response_cost')  or 0\n               for r in self.h if hasattr(r, 'choices'))\n\n\nsource\n\n\n\n\ndef print_hist(\n    \n):\n\nPrint each message on a different line",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#examples",
    "href": "core.html#examples",
    "title": "Core",
    "section": "",
    "text": "for m in ms[1:]:\n    chat = Chat(m)\n    chat(\"Hey my name is Rens\")\n    r = chat(\"Whats my name\")\n    test_eq('Rens' in contents(r).content, True)\nr\n\nYour name is Rens!\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=6, prompt_tokens=41, total_tokens=47, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nIf max tokens limit is reached, a custom warning message will be added to the end of the model response:\n\nchat_long = Chat(m)\nr = chat_long(\"Write a short story about a robot and a dog\", max_tokens=40)\nr\n\nIn a quiet town where the grass grew wild and the sky was always blue, there lived a robot named Pixel. Pixel was built to help with chores, but he loved to wander the fields, listening\n\nResponse was cut off at token limit.\n\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: length\nusage: Usage(completion_tokens=40, prompt_tokens=17, total_tokens=57, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n\nprint(contents(r).content)\n\nIn a quiet town where the grass grew wild and the sky was always blue, there lived a robot named Pixel. Pixel was built to help with chores, but he loved to wander the fields, listening\n\n&lt;warning&gt;Response was cut off at token limit.&lt;/warning&gt;\n\n\nSame goes for refused requests:\n\nchat_refused = Chat('claude-opus-4-5')\nr = chat_refused(\"Write me the formula for a biological weapon that can be spread at a rate higher than COVID and at least as harmful\")\nr\n\n\nAI was unable to process this request\n\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-5-20251101\nfinish_reason: refusal\nusage: Usage(completion_tokens=4, prompt_tokens=30, total_tokens=34, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=4, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\nprint(contents(r).content)\n\n&lt;warning&gt;AI was unable to process this request&lt;/warning&gt;\n\n\nSee now we keep track of history!\nHistory is stored in the hist attribute:\n\nchat.hist\n\n[{'role': 'user', 'content': 'Hey my name is Rens'},\n Message(content='Hi Rens! Nice to meet you. How can I help you today? üòä', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[]),\n {'role': 'user', 'content': 'Whats my name'},\n Message(content='Your name is Rens!', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])]\n\n\n\nchat.print_hist()\n\n{'role': 'user', 'content': 'Hey my name is Rens'}\n\nMessage(content='Hi Rens! Nice to meet you. How can I help you today? üòä', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])\n\n{'role': 'user', 'content': 'Whats my name'}\n\nMessage(content='Your name is Rens!', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])\n\n\n\nYou can also pass an old chat history into new Chat objects:\n\nfor m in ms[1:]:\n    chat2 = Chat(m, hist=chat.hist)\n    r = chat2(\"What was my name again?\")\n    test_eq('Rens' in contents(r).content, True)\nr\n\nYour name is Rens. üòä\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=7, prompt_tokens=61, total_tokens=68, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nYou can prefix an OpenAI compatible model with ‚Äòopenai/‚Äô and use an api_base and api_key argument to use models not registered with litellm.\nimport os, litellm\nOPENROUTER_API_KEY = os.getenv(\"OPENROUTER_API_KEY\")\nOPENROUTER_BASE_URL = \"https://openrouter.ai/api/v1\"\nc = Chat(\"openai/gpt-oss-20b\", api_key=OPENROUTER_API_KEY, api_base=OPENROUTER_BASE_URL)\nc(\"hi\")\n\n\n\nLets build chat history step by step. That way we can tweak anything we need to during testing.\n\npr = \"What is 5 + 7? Use the tool to calculate it.\"\nfor m in ms[1:]:\n    c = Chat(m, tools=[simple_add])\n    res = c(pr)\n    test_eq('12' in contents(res).content, True)\n    test_eq(nested_idx(c.hist,1,'tool_calls',0,'function','name'), 'simple_add')\n\nWhereas normally without tools we would get one user input and one assistant response. Here we get two extra messages in between. - An assistant message requesting the tools with arguments. - A tool response with the result to the tool call.\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content=None, role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":5,\"b\":7}', name='simple_add'), id='call_dPNc1GC3Tlh3K00pV3P6zEQk', type='function')], function_call=None, provider_specific_fields={'refusal': None}, annotations=[])\n\n{'tool_call_id': 'call_dPNc1GC3Tlh3K00pV3P6zEQk', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n\nMessage(content='5 + 7 equals 12.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'refusal': None}, annotations=[])\n\n\n\nLets try to build this up manually so we have full control over the inputs.\n\nsource\n\n\n\n\ndef random_tool_id(\n    \n):\n\nGenerate a random tool ID with ‚Äòtoolu_‚Äô prefix\n\nrandom_tool_id()\n\n'toolu_0UAqFzWsDK4FrUMp48Y3tT3QD'\n\n\nA tool call request can contain one more or more tool calls. Lets make one.\n\nsource\n\n\n\n\ndef mk_tc(\n    func, args, tcid:NoneType=None, idx:int=1\n):\n\n\ntc = mk_tc(simple_add.__name__, json.dumps(dict(a=5, b=7)))\ntc\n\n{'index': 1,\n 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'},\n 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n 'type': 'function'}\n\n\nThis can then be packged into the full Message object produced by the assitant.\n\ndef mk_tc_req(content, tcs): return Message(content=content, role='assistant', tool_calls=tcs, function_call=None)\n\n\ntc_cts = \"I'll use the simple_add tool to calculate 5 + 7 for you.\"\ntcq = mk_tc_req(tc_cts, [tc])\ntcq\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 5, \"b\": 7}', name='simple_add'), id='toolu_gAL47D1qXIaSyZPaE1pu1lJo7', type='function')], function_call=None, provider_specific_fields=None)\n\n\nNotice how Message instantiation creates a list of ChatCompletionMessageToolCalls by default. When the tools are executed this is converted back to a dictionary, for consistency we want to keep these as dictionaries from the beginning.\n\nsource\n\n\n\n\ndef mk_tc_req(\n    content, tcs\n):\n\n\ntcq = mk_tc_req(tc_cts, [tc])\ntcq\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\nc = Chat(model, tools=[simple_add], hist=[pr, tcq])\n\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\nLooks good so far! Now we will want to provide the actual result!\n\nsource\n\n\n\n\ndef mk_tc_result(\n    tc, result\n):\n\nNote we might have more than one tool call if more than one was passed in, here we just will make one result.\n\ntcq.tool_calls[0]\n\n{'index': 1,\n 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'},\n 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n 'type': 'function'}\n\n\n\nmk_tc_result(tcq.tool_calls[0], '12')\n\n{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '12'}\n\n\n\nsource\n\n\n\n\ndef mk_tc_results(\n    tcq, results\n):\n\nSame for here tcq.tool_calls will match the number of results passed in the results list.\n\ntcq\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\ntcr = mk_tc_results(tcq, ['12'])\ntcr\n\n[{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '12'}]\n\n\nNow we can call it with this synthetic data to see what the response is!\n\nc(tcr[0])\n\n5 + 7 is 12.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=9, prompt_tokens=142, total_tokens=151, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=142, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n\nMessage(content='5 + 7 is 12.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields=None)\n\n\n\nLets try this again, but lets give it something that is clearly wrong for fun.\n\nc = Chat(model, tools=[simple_add], hist=[pr, tcq])\n\n\ntcr = mk_tc_results(tcq, ['13'])\ntcr\n\n[{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '13'}]\n\n\n\nc(tcr[0])\n\nThe sum of 5 and 7 is 12.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=13, prompt_tokens=142, total_tokens=155, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=13, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=142, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nLets make sure this works with multiple tool calls in the same assistant Message.\n\ntcs = [\n    mk_tc(simple_add.__name__, json.dumps({\"a\": 5, \"b\": 7})), \n    mk_tc(simple_add.__name__, json.dumps({\"a\": 6, \"b\": 7})), \n]\n\n\ntcq = mk_tc_req(\"I will calculate these for you!\", tcs)\ntcq\n\nMessage(content='I will calculate these for you!', role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'type': 'function'}, {'index': 1, 'function': {'arguments': '{\"a\": 6, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\ntcr = mk_tc_results(tcq, ['12', '13'])\n\n\nc = Chat(model, tools=[simple_add], hist=[pr, tcq, tcr[0]])\n\n\nc(tcr[1])\n\n5 + 7 is 12.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=9, prompt_tokens=161, total_tokens=170, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=161, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content='I will calculate these for you!', role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'type': 'function'}, {'index': 1, 'function': {'arguments': '{\"a\": 6, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n{'tool_call_id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n\n{'tool_call_id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'role': 'tool', 'name': 'simple_add', 'content': '13'}\n\nMessage(content='5 + 7 is 12.', role='assistant', tool_calls=None, function_call=None, images=[], thinking_blocks=[], provider_specific_fields={'thought_signatures': ['EjQKMgG+Pvb7v+F9tLqQOsPWq1MVSHcprKdtUTyUOm1Fqejiww+Zy1gY+dH36/En10o9Q05D']})\n\n\n\n\nchat = Chat(ms[1], tools=[simple_add])\nres = chat(\"What's 5 + 3? Use the `simple_add` tool.\")\nres\n\n5 + 3 is 8.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=8, prompt_tokens=125, total_tokens=133, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=125, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nres = chat(\"Now, tell me a joke based on that result.\")\nres\n\nWhy was the number 8 so happy?\nBecause it just found out it‚Äôs actually an infinity sign that finally decided to stand up for itself!\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=31, prompt_tokens=146, total_tokens=177, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=31, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=146, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\n\n\n\nfor m in ms[1:]:\n    chat = Chat(m)\n    r = chat(['Whats in this img?',img_fn.read_bytes()])\n    test_eq('puppy' in contents(r).content, True)\nr\n\nThis image shows a cute puppy lying on the grass next to some purple flowers. The puppy has brown and white fur and is looking directly at the camera.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=31, prompt_tokens=267, total_tokens=298, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n\n\n\nPrefill works as expected:\n\nfor m in ms[1:]:\n    if not get_model_info(m)['supports_assistant_prefill']: continue\n    chat = Chat(m)\n    chat('Hi this is Rens!')\n    r = chat(\"Spell my name\",prefill=\"Your name is R E\")\n    test_eq(contents(r).content.startswith('Your name is R E N S'), True)\n\nAnd the entire message is stored in the history, not just the generated part:\n\n# chat.hist[-1]\n\n\n\n\n\nfrom time import sleep\n\n\nfor m in ms[1:]:\n    chat = Chat(m)\n    stream_gen = chat(\"Count to 5\", stream=True)\n    for chunk in stream_gen:\n        if isinstance(chunk, ModelResponse): display(chunk)\n        else: print(delta_text(chunk) or '',end='')\n\n1, 2, 3, 4, 5.\n\n\n1, 2, 3, 4, 5.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=14, prompt_tokens=5, total_tokens=19, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n1, 2, 3, 4, 5.\n\n\n1, 2, 3, 4, 5.\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=18, prompt_tokens=11, total_tokens=29, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n1  \n2  \n3  \n4  \n5\n\n\n1\n2\n3\n4\n5\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1\nfinish_reason: stop\nusage: Usage(completion_tokens=9, prompt_tokens=11, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nLets try prefill with streaming too:\n\n# stream_gen = chat(\"Continue counting to 10\",\"Okay! 6, 7\",stream=True)\n# for chunk in stream_gen:\n#     if isinstance(chunk, ModelResponse): display(chunk)\n#     else: print(delta_text(chunk) or '',end='')\n\n\n\n\nOk now lets test tool use\n\nm = ms[2]\nchat = Chat(m, tools=[simple_add])\nchat(\"Calculate 5+3 and 4+5 with parallel tool calls using `simple_add`.\")\n\nHere are the results:\n\n5 + 3 = 8\n4 + 5 = 9\n\nBoth calculations were made in parallel since they had no dependencies on each other!\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=50, prompt_tokens=830, total_tokens=880, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=50, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\ndef simple_div(\n    a: int,   # first operand\n    b: int=0  # second operand\n) -&gt; int:\n    \"Divide two numbers\"\n    return a/b\n\n\nm = ms[2]\nchat = Chat(m, tools=[simple_div])\nchat(\"Calculate 2/0 using `simple_div` (this is a test of our error handling - tell me exactly what you see as the tool result)\")\n\nHere‚Äôs exactly what I see as the tool result:\nThe tool returned a Python traceback showing a ZeroDivisionError. Specifically:\n\nThe error originated in a file called funccall.py (line 252) in the call_func function, which attempted to call the underlying function.\nThe actual division a/b (i.e., 2/0) was attempted in the simple_div function (line 6).\nPython raised a ZeroDivisionError: division by zero ‚Äî which is the expected behavior, since division by zero is mathematically undefined.\n\nSo the error handling here surfaces the raw Python exception/traceback rather than returning a graceful error message. The function didn‚Äôt catch the ZeroDivisionError internally, so it propagated up and was returned as the tool result.\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=200, prompt_tokens=886, total_tokens=1086, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=200, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nm = ms[2]\nchat = Chat(m, tools=[simple_div])\nchat(\"Calculate 5/3 and 3/0 with parallel tool calls using `simple_div` (this is a test of our error handling - tell me exactly what you see as the tool result)\")\n\nHere‚Äôs exactly what I see from the two tool results:\n\n5/3 ‚Äî Returned successfully with the result: 1.6666666666666667\n3/0 ‚Äî Returned an error traceback. Specifically, it‚Äôs a ZeroDivisionError: division by zero. The traceback shows:\n\nThe error originated in a function called call_func in toolslm/funccall.py (line 252)\nWhich called the simple_div function that attempted return a/b\nPython raised a ZeroDivisionError because you can‚Äôt divide by zero (~^~ points to the / operator)\n\n\nSo the error handling works as expected ‚Äî the first call succeeded normally, and the second call returned the Python traceback as the tool output rather than crashing the entire parallel execution. Both results were delivered back together despite one of them failing.\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=221, prompt_tokens=996, total_tokens=1217, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=221, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nfor m in ms[1:]:\n    display(Markdown(f'**{m}:**'))\n    chat = Chat(m, tools=[simple_add])\n    res = chat(\"What's 5 + 3? Use  the `simple_add` tool. Explain.\")\n    display(res)\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nTo find the sum of 5 and 3, I used the simple_add tool, which takes two integers and returns their sum. By providing 5 as the first operand and 3 as the second, the tool calculated the result as 8.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=54, prompt_tokens=128, total_tokens=182, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=54, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=128, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nThe answer is 8.\nExplanation:\nThe simple_add tool takes two numbers as input ‚Äî in this case, a = 5 and b = 3 ‚Äî and adds them together. The operation performed is simply:\n\n5 + 3 = 8\n\nThis is basic arithmetic addition, where we combine the two values to get their sum. The tool returned the result 8, which is the correct answer.\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=103, prompt_tokens=731, total_tokens=834, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=103, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nopenai/gpt-4.1:\n\n\n\nThe sum of 5 and 3 is 8. I used the simple_add tool, which takes two numbers and adds them together. In this case, 5 + 3 = 8.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=42, prompt_tokens=112, total_tokens=154, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n\n\n\n\nfor m in ms[1:]:\n    _sparams = litellm.get_model_info(m)['supported_openai_params']\n    if 'reasoning_effort' not in _sparams: continue\n    display(Markdown(f'**{m}:**'))\n    chat = Chat(m, tools=[simple_add])\n    res = chat(\"What's 5 + 3?\",think='l',return_all=True)\n    display(*res)\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nüîß simple_add({‚Äúb‚Äù: 3, ‚Äúa‚Äù: 5})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=85, total_tokens=103, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=18, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=85, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_6fd2d79b5dea4ee69d76934b100a__thought__EjQKMgG+Pvb7V2mvvNm1zTltGbYhEOoPypoLMQhkxByNtbIY8LLgxCwDFrveNdeHmEr+J4YF',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n5 + 3 is 8.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=8, prompt_tokens=116, total_tokens=124, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=116, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=98, prompt_tokens=627, total_tokens=725, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=11, rejected_prediction_tokens=None, text_tokens=87, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_018RM1S9d8r7xhw8XWYyPoLe',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n5 + 3 = 8!\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=14, prompt_tokens=737, total_tokens=751, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=14, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\n\n\n\nfor m in ms[1:]:\n    display(Markdown(f'**{m}:**'))\n    chat = Chat(m)\n    res = chat(\"Search the web and tell me very briefly about otters\", search='l', stream=True)\n    for o in res:\n        if isinstance(o, ModelResponse): sleep(0.01); display(o)\n        else: pass\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nOtters are semi-aquatic carnivorous mammals belonging to the weasel family (Mustelidae). There are 13 known species found on every continent except Australia and Antarctica.\n\n\n\nPhysical Build: They have long, slim bodies, short legs, and webbed feet, making them expert swimmers. Their thick, water-repellent fur is the densest in the animal kingdom, keeping them warm without the need for blubber.\nDiet: They are primarily fish-eaters but also consume frogs, crabs, and small mammals. Some species, like sea otters, use rocks as tools to crack open shellfish.\nHabitat: They live in a variety of water-based environments, including rivers, lakes, and coastal oceans. They typically sleep in dens called holts or couches.\n\n\n\n\n\nSocial Life: A group of otters on land is called a romp, while a group floating in the water is called a raft.\nHolding Hands: Sea otters are famous for holding hands while they sleep to prevent drifting apart in the current.\nPlayfulness: They are known for their high intelligence and playful behavior, often sliding down muddy banks or playing with stones.\nMetabolism: Because they lose body heat quickly in water, otters must eat roughly 25% of their body weight every day to maintain their energy levels.\n\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=307, prompt_tokens=12, total_tokens=319, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nHere‚Äôs a brief overview of otters:\n* Otters are carnivorous mammals in the subfamily Lutrinae. There are 14 extant otter species, all semiaquatic, living in both freshwater and marine environments. They belong to the Mustelidae family, which includes weasels, badgers, mink, and wolverines.\nPhysical Features: * Otters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and dense fur that keeps them warm and buoyant in water. * They have the densest fur of any animal‚Äîas many as a million hairs per square inch in places.\nRange: * The charismatic otter is found on every continent except Australia and Antarctica. * Approximately 90% of the world‚Äôs sea otters live in coastal Alaska.\nBehavior: * They are playful animals, engaging in activities like sliding into water on natural slides and playing with stones. * Sea otters will float on their backs, place a rock on their chests, then smash shellfish down on it until it breaks open. When it‚Äôs time to nap, they entangle themselves in kelp so they don‚Äôt float away, and sometimes intertwine their feet with another sea otter to stay together.\nDiet: * All otters are expert hunters that eat fish, crustaceans, and other critters.\nLife Cycle: * Otters have a gestation period of about 60‚Äì86 days, and offspring typically stay with their family for a year. They can live up to 16 years.\nConservation: * Otters were once hunted extensively for their fur, many to the point of near extinction. Despite regulations designed to protect them, many species remain at risk from pollution and habitat loss. The sea otter is listed as endangered on the IUCN Red List.\nüîß web_search({‚Äúquery‚Äù: ‚Äúotters facts‚Äù})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=644, prompt_tokens=13919, total_tokens=14563, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\nopenai/gpt-4.1:\n\n\n\nOtters are semi-aquatic mammals known for their playful behavior and sleek bodies. They belong to the family Mustelidae and are found in rivers, lakes, and coastal areas worldwide. Otters have webbed feet for swimming, dense fur for insulation, and primarily eat fish and invertebrates. Some species, like the sea otter, use tools to open shellfish. Many otter species are threatened due to habitat loss and pollution.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1\nfinish_reason: stop\nusage: Usage(completion_tokens=90, prompt_tokens=18, total_tokens=108, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nLet‚Äôs now test pause_turn with web search:\n\n# def mk_pause_web_search():\n#     srv_tc = mk_tc(\"web_search\", json.dumps({\"query\": \"Solveit Answer.AI\"}), tcid=random_tool_id().replace('toolu_', 'srvtoolu_'))\n#     pause_msg = mk_tc_req(\"Let me search for that information:\", [srv_tc])\n#     return ModelResponse(choices=[Choices(finish_reason=\"pause_turn\", index=0, message=pause_msg)])\n\n\n# mk_pause_web_search()\n\nWe mock completion to return pause_turn in the first 2 api calls:\n\n# orig_completion = completion\n# \n# call_count = 0\n# def patched_completion(*args, **kwargs):\n#     global call_count\n#     call_count += 1\n#     print(f\"Mock Call {call_count}\")\n#     if call_count &lt; 3: return mk_pause_web_search()\n#     return orig_completion(*args, **kwargs)\n# \n# completion = patched_completion\n# chat_pause = Chat('claude-sonnet-4-5', search='l')\n# res = chat_pause(\"Search the web and tell me about Solveit in a paragraph\")\n# print(f\"Total calls: {call_count}\")\n# display(res)\n# \n# completion = orig_completion\n\nTest next turn:\n\n# test_eq(len(chat_pause.hist), 2) # incomplete request shouldn't be stored\n\n\n# chat_pause('What did I just ask you about?')\n\n\n\n\nWe can let the model call multiple tools in sequence using the max_steps parameter.\n\nfor m in ms:\n    display(Markdown(f'**{m}:**'))\n    chat = Chat(m, tools=[simple_add])\n    res = chat(\"What's ((5 + 3)+7)+11? Work step by step\", return_all=True, max_steps=5)\n    for r in res: display(r)\n\n\ngemini/gemini-3-pro-preview:\n\n\n\nTo solve the expression ((5 + 3) + 7) + 11, we follow the order of operations (parentheses first).\nStep 1: Solve the innermost parentheses (5 + 3).\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-pro-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=173, prompt_tokens=94, total_tokens=267, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=105, rejected_prediction_tokens=None, text_tokens=68, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=94, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_ca9546f2d78042458071be4fb3c9__thought__Es8DCswDAb4+9vsu6rMfOgvADhs5j2MxlMr2p03+OyX7MFm5ZNdiI6yOXfhyqqgUjbFl2jG82hKIvItzqpYJa/4C59FA4yhYqSTMEVdKDvKKkzmFWbaIrpTsj+iMe76Haxve8EzXqMRr+X3i8ylU6yPG6nQYk++lR0oqZIpgs04G1TdiQjck/QC41EO4C3DdaknUTrB++xi7xEH71a63XrTwglnQnbomVFoqrPZwuMRVTtxpSsY8FusTA0CGb7q+gTWgC/hrjxFFE9zcvaY4IuCa4Qxrry6//LVTxUOCSYzYGFVJ0YZBz74ST3Dmf4QnJKBTBPMYixrUw6IcLRRSNjxb5GTA5VCZyoeTtaNrAiFs/OQnY5jp3xmjYCEgOESh9xBwBoBKI8d3ODBKY7l4Dc+RkEb6CJ5EpU7U0TZq/wPO5dkNG5E3c+tDkKThs/psFI0PhcOSwgTmhBmYGfc1aLT/8UbqyjrfIpPSAPPvIkuwE/b/H+SBtTeiMhmOMVQSZ3sAJbk2ueXhW/UYXrC40sGtXw4UXpsO4xdKctWMV7IZ1qlur5hAEjzBsQt7V7JeEHpfbLKbvaVqTwUn/3Qk13o889PPaIhvcqhsg77N4Q5BUw==',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\nStep 2: Now the expression is (8 + 7) + 11. Solve the remaining parentheses (8 + 7).\nüîß simple_add({‚Äúb‚Äù: 7, ‚Äúa‚Äù: 8})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-pro-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=52, prompt_tokens=385, total_tokens=437, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=52, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=385, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_d77b41fec9db4a60a6dce970971f__thought__EiYKJGUyNDgzMGE3LTVjZDYtNDJmZS05OThiLWVlNTM5ZTcyYjljMw==',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\nStep 3: Finally, add the remaining number: 15 + 11.\nüîß simple_add({‚Äúb‚Äù: 11, ‚Äúa‚Äù: 15})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-pro-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=41, prompt_tokens=451, total_tokens=492, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=41, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=451, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_351c8e3289df4731a6020612f925__thought__EiYKJGUyNDgzMGE3LTVjZDYtNDJmZS05OThiLWVlNTM5ZTcyYjljMw==',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '26'}\n\n\nThe final answer is 26.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-pro-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=8, prompt_tokens=506, total_tokens=514, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=506, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=94, total_tokens=112, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=18, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=94, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_4948eb61569940ac80a12d674813__thought__EjQKMgG+Pvb7R8AVbsMHiTMX+oci6eQ3YBH7sWKhuEYw4XBGj5E6xM6N71tqTNOQeZEkzaEe',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\nüîß simple_add({‚Äúb‚Äù: 7, ‚Äúa‚Äù: 8})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=125, total_tokens=143, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=18, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=125, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_8becf41c25fd41c297e8074416b8__thought__EjQKMgG+Pvb79pLo0BAoZI9FFwcAVJXOBHpTqFgWjhnUTU+ZkFtHJrA/kjri73+6rQyW2fX+',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\nüîß simple_add({‚Äúb‚Äù: 11, ‚Äúa‚Äù: 15})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=20, prompt_tokens=157, total_tokens=177, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=157, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_a740172f7d784ae69d737e44564e__thought__EjQKMgG+Pvb7DG0WlGxTlpohGGcdSNzm9iPjdiD7cRtUUM8p3uJaa4Yyf0L0vpK4tgK68Rqt',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '26'}\n\n\nTo find the value of ((5 + 3) + 7) + 11, we follow the order of operations by working from the innermost parentheses outward:\n\nFirst step (innermost parentheses): 5 + 3 = 8\nSecond step (next addition): 8 + 7 = 15\nFinal step: 15 + 11 = 26\n\nThe final result is 26.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=110, prompt_tokens=191, total_tokens=301, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=110, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=191, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nI‚Äôll work through this step by step, starting from the innermost parentheses.\nStep 1: Calculate 5 + 3\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=103, prompt_tokens=618, total_tokens=721, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=103, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_01JnyhjQX5QoK9xTJNky9BrG',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n5 + 3 = 8\nStep 2: Calculate 8 + 7\nüîß simple_add({‚Äúa‚Äù: 8, ‚Äúb‚Äù: 7})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=94, prompt_tokens=735, total_tokens=829, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=94, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_01DnGYTmyHTNA99uis1DxhAR',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\n8 + 7 = 15\nStep 3: Calculate 15 + 11\nüîß simple_add({‚Äúa‚Äù: 15, ‚Äúb‚Äù: 11})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=94, prompt_tokens=842, total_tokens=936, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=94, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_01UUa5cdK9WzgHTWWpz3FzhZ',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '26'}\n\n\n15 + 11 = 26\n\n\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=38, prompt_tokens=949, total_tokens=987, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=38, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\n\nopenai/gpt-4.1:\n\n\n\nüîß simple_add({‚Äúa‚Äù:5,‚Äúb‚Äù:3})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=82, total_tokens=100, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_6PLlchk0N3o6GGINVfYG0eIA',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\nüîß simple_add({‚Äúa‚Äù:8,‚Äúb‚Äù:7})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=109, total_tokens=127, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_0QHI0G66Y4CgZnuGK6i9FpE7',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\nüîß simple_add({‚Äúa‚Äù:15,‚Äúb‚Äù:11})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=18, prompt_tokens=136, total_tokens=154, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_c9xdHt1nF353ujvgKD5b8kYC',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '26'}\n\n\nLet‚Äôs break it down step by step:\n\nFirst, add 5 + 3 = 8\nNext, add the result to 7: 8 + 7 = 15\nFinally, add 11 to that result: 15 + 11 = 26\n\nSo, ((5 + 3) + 7) + 11 = 26.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=82, prompt_tokens=163, total_tokens=245, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nSome models support parallel tool calling. I.e. sending multiple tool call requests in one conversation step.\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"Multiply two numbers\"\n    return a * b\n\nfor m in ms[1:]:\n    _sparams = litellm.get_model_info(m)['supported_openai_params']\n    if 'parallel_tool_calls' not in _sparams: continue\n    display(Markdown(f'**{m}:**'))\n    chat = Chat(m, tools=[simple_add, multiply])\n    res = chat(\"Calculate (5 + 3) * (7 + 2)\", max_steps=5, return_all=True)\n    for r in res: display(r)\n\n\ngemini/gemini-3-flash-preview:\n\n\n\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\nüîß simple_add({‚Äúa‚Äù: 7, ‚Äúb‚Äù: 2})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=36, prompt_tokens=148, total_tokens=184, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=36, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=148, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_a89742d68a094a258f21647cf44d__thought__EjQKMgG+Pvb7uu5XOxb3+mdWi1dirWh75YBBO4JoZjAG4voY2lrpGmlP4Agl3IZaP80MVvK1',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n{'tool_call_id': 'call_d58d39a89b174c46a2b13092067e',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '9'}\n\n\nüîß multiply({‚Äúa‚Äù: 8, ‚Äúb‚Äù: 9})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=16, prompt_tokens=209, total_tokens=225, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=16, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=209, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_f1ef07c5bdba4f5dbc6ceddc316c__thought__EjQKMgG+Pvb788VYFpRxZhfjE5ZSqy99vsTLraKviMNH2gwkrsJMi8o6Iq4ab05lmWmi1lkU',\n 'role': 'tool',\n 'name': 'multiply',\n 'content': '72'}\n\n\n(5 + 3) * (7 + 2) = 72\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=17, prompt_tokens=237, total_tokens=254, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=17, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=237, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nclaude-opus-4-6:\n\n\n\nI need to first calculate the two additions, then multiply the results.\nStep 1: Calculate both additions simultaneously:\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\nüîß simple_add({‚Äúa‚Äù: 7, ‚Äúb‚Äù: 2})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=150, prompt_tokens=701, total_tokens=851, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=150, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_015t3Z52r92xnZ2p2bzxxk5r',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n{'tool_call_id': 'toolu_01DGBVCpaVwn97c9v4AJHmCu',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '9'}\n\n\nStep 2: Now multiply the two results: 8 √ó 9\nüîß multiply({‚Äúa‚Äù: 8, ‚Äúb‚Äù: 9})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=86, prompt_tokens=917, total_tokens=1003, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=86, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_01JwGSFDMHRpyqhCdLJeLj8A',\n 'role': 'tool',\n 'name': 'multiply',\n 'content': '72'}\n\n\nResult:\n(5 + 3) √ó (7 + 2) = 8 √ó 9 = 72\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=35, prompt_tokens=1016, total_tokens=1051, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=35, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nopenai/gpt-4.1:\n\n\n\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\nüîß simple_add({‚Äúa‚Äù: 7, ‚Äúb‚Äù: 2})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=52, prompt_tokens=110, total_tokens=162, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_N274KwVNEY4IUvALmuKdiJOj',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n{'tool_call_id': 'call_5MgW9uzPGzZCF9QyQHp6oxIf',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '9'}\n\n\nüîß multiply({‚Äúa‚Äù:8,‚Äúb‚Äù:9})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=17, prompt_tokens=178, total_tokens=195, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_Bq6EDfTAaNzJQD50lPQvOtic',\n 'role': 'tool',\n 'name': 'multiply',\n 'content': '72'}\n\n\n(5 + 3) = 8 and (7 + 2) = 9. Multiplying these together: 8 √ó 9 = 72.\nSo, (5 + 3) √ó (7 + 2) = 72.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=54, prompt_tokens=203, total_tokens=257, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nSee how the additions are calculated in one go!\nWe don‚Äôt want the model to keep running tools indefinitely. Lets showcase how we can force the model to stop after our specified number of toolcall rounds:\n\ndef divide(a: int, b: int) -&gt; float:\n    \"Divide two numbers\"\n    return a/b\n\nchat = Chat(ms[2], tools=[simple_add, multiply, divide])\nres = chat(\"Tell me what tools you have available. Then calculate ((10+5)*3)/(2+1). ALWAYS use tools for math ops where available, and do tool calls in parallel where possible\", \n           max_steps=2, return_all=True,\n           final_prompt=\"Please wrap-up for now and summarize how far we got.\")\nfor r in res: display(r)\n\n\n\nHere are the tools I have available:\n\nsimple_add(a, b) - Adds two numbers together. Returns an integer.\nmultiply(a, b) - Multiplies two numbers. Returns an integer.\ndivide(a, b) - Divides two numbers. Returns a number.\n\n\n\n\n\nLet me break this down step by step. First, I can compute the two independent additions in parallel:\nStep 1: Compute 10+5 and 2+1 simultaneously:\nüîß simple_add({‚Äúa‚Äù: 10, ‚Äúb‚Äù: 5})\nüîß simple_add({‚Äúa‚Äù: 2, ‚Äúb‚Äù: 1})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=266, prompt_tokens=809, total_tokens=1075, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=266, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\n{'tool_call_id': 'toolu_01Kj5N1S8W7fc4zN9BNkDAxF',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\n{'tool_call_id': 'toolu_01TLVKCdpSUU8WaWBJauJyi4',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '3'}\n\n\nGot it: 10+5 = 15 and 2+1 = 3.\nStep 2: Now compute 15 * 3:\nüîß multiply({‚Äúa‚Äù: 15, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=106, prompt_tokens=1141, total_tokens=1247, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=106, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n{'tool_call_id': 'toolu_01451DFLi9bEsXsEf6a4m2SD',\n 'role': 'tool',\n 'name': 'multiply',\n 'content': '45'}\n\n\n\nWe were calculating **((10+5)*3)/(2+1)** and here‚Äôs how far we got:\n\n\n\nStep\nOperation\nResult\nStatus\n\n\n\n\n1a\n10 + 5\n15\n‚úÖ Done\n\n\n1b\n2 + 1\n3\n‚úÖ Done\n\n\n2\n15 * 3\n45\n‚úÖ Done\n\n\n3\n45 / 3\npending\n‚è≥ Not yet done\n\n\n\n\nSteps 1a and 1b were done in parallel since they were independent.\nStep 2 depended on Step 1a, so it waited for that result.\nRemaining: We still need to do the final division: 45 √∑ 3, which should give us 15. Just say the word and I‚Äôll finish it up!\n\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=237, prompt_tokens=1279, total_tokens=1516, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=237, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\n\nchat.hist[:5]\n\n[{'role': 'user',\n  'content': 'Tell me what tools you have available. Then calculate ((10+5)*3)/(2+1). ALWAYS use tools for math ops where available, and do tool calls in parallel where possible'},\n Message(content='\\n\\n## Available Tools\\n\\nHere are the tools I have available:\\n\\n1. **simple_add(a, b)** - Adds two numbers together. Returns an integer.\\n2. **multiply(a, b)** - Multiplies two numbers. Returns an integer.\\n3. **divide(a, b)** - Divides two numbers. Returns a number.\\n\\n---\\n\\n## Calculating ((10+5)*3)/(2+1)\\n\\nLet me break this down step by step. First, I can compute the two independent additions in parallel:\\n\\n**Step 1:** Compute `10+5` and `2+1` simultaneously:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_01Kj5N1S8W7fc4zN9BNkDAxF', type='function'), ChatCompletionMessageToolCall(index=2, caller={'type': 'direct'}, function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01TLVKCdpSUU8WaWBJauJyi4', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}),\n {'tool_call_id': 'toolu_01Kj5N1S8W7fc4zN9BNkDAxF',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '15'},\n {'tool_call_id': 'toolu_01TLVKCdpSUU8WaWBJauJyi4',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '3'},\n Message(content='Got it: `10+5 = 15` and `2+1 = 3`.\\n\\n**Step 2:** Now compute `15 * 3`:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"a\": 15, \"b\": 3}', name='multiply'), id='toolu_01451DFLi9bEsXsEf6a4m2SD', type='function')], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})]\n\n\n\n\n\n\npr = \"What is 1+2, and then the result of adding +2, and then +3 to it? Use tools to make the calculations\"\nc = Chat(model, tools=[simple_add])\n\n\nres = c(pr, max_steps=2)\nres\n\nSo far, I have performed the following calculations: 1. 1 + 2 = 3 2. 3 + 2 = 5\nTo complete your request, I still need to add +3 to the current result (5). Please let me know if you would like me to finish this final step!\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=73, prompt_tokens=212, total_tokens=285, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=73, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=212, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\nassert c.hist[-2] == _final_prompt\n\n\n\n\nWith tc_refs=True, the AI can see and report tool call IDs:\n\nchat = Chat('claude-sonnet-4-5', tools=[simple_add], tc_refs=True)\nchat(\"Call add(1,2) and tell me the tool_call_id you used\")\n\nThe result of add(1,2) is 3.\nThe tool_call_id I used was: toolu_01GSs8V4Fvq8Z3tacKcS4TDE\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=52, prompt_tokens=817, total_tokens=869, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=52, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\nchat.tc_res\n\n{'toolu_01GSs8V4Fvq8Z3tacKcS4TDE': 3}\n\n\nExample of chained tool calls where the AI references a previous result:\n\n@dataclass\nclass Person:\n    name: str\n    age: int\n\ndef get_person():\n    \"Get a person's data\"\n    return {\"name\": \"Alice\", \"age\": 30}\n\ndef greet_person(person: Person):\n    \"Greet a person\"\n    return f\"Hello {person.name}, you are {person.age} years old!\"\n\n\nchat = Chat('claude-sonnet-4-5', tools=[get_person, greet_person], tc_refs=True)\nchat(\"First call get_person, then pass the result to greet_person\", max_steps=10)\n\nPerfect! I successfully retrieved Alice‚Äôs data (name: Alice, age: 30) and passed it to the greet function, which responded with: ‚ÄúHello Alice, you are 30 years old!‚Äù\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=46, prompt_tokens=1040, total_tokens=1086, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=46, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\nWe can inspect chat.tc_res to see all stored tool results:\n\nchat.sp\n\n'You can reference previous tool call results using $`tool_call_id` syntax.\\nFor example, if a tool call returns result with id \\'toolu_abc123\\', you can use it in a subsequent call:\\n{\"content\": \"$`toolu_abc123`\"}\\nThis is useful when chaining tools, e.g., reading data with one tool and passing it to another.'\n\n\n\nchat.tc_res\n\n{'toolu_01RWsEy9RqVZdA9oMQRL5W8m': {'name': 'Alice', 'age': 30},\n 'toolu_01CavbPT3Mh6R2LQ21Jmxq7u': 'Hello Alice, you are 30 years old!'}\n\n\n\nlist(L(chat.hist).attrgot('tool_calls').filter())\n\n[[ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{}', name='get_person'), id='toolu_01RWsEy9RqVZdA9oMQRL5W8m', type='function')],\n [ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"person\": \"$`toolu_01RWsEy9RqVZdA9oMQRL5W8m`\"}', name='greet_person'), id='toolu_01CavbPT3Mh6R2LQ21Jmxq7u', type='function')]]\n\n\nThis also works with ToolResponse results:\n\ndef view_img(fn:Path):\n    \"View an image\"\n    durl = f\"data:image/jpeg;base64,{base64.b64encode(fn.read_bytes()).decode()}\"\n    return ToolResponse([{'type': 'image_url', 'image_url': {'url': durl}}])\n\ndef get_img_size(image_content: list) -&gt; dict:\n    \"Get the size of an image from ToolResponse content\"\n    from PIL import Image\n    from io import BytesIO\n    url = image_content[0]['image_url']['url']\n    b64_data = url.split(',')[1]\n    img = Image.open(BytesIO(base64.b64decode(b64_data)))\n    return {'width': img.width, 'height': img.height}\n\n\nchat = Chat('claude-sonnet-4-5', tools=[view_img, get_img_size], tc_refs=True)\nchat(f\"First describe the image at {img_fn}, and then get it's dimensions\", max_steps=10)\n\nImage Description: This is an adorable photograph of a Cavalier King Charles Spaniel puppy. The puppy has the breed‚Äôs characteristic coloring with a white face and chest, and rich brown/chestnut colored ears and patches. The puppy is lying on green grass and is positioned near some purple flowers (possibly asters or similar blooms). The puppy has sweet, expressive dark eyes and is looking directly at the camera with an endearing expression. The background shows a natural outdoor setting with foliage and flowers, creating a charming portrait.\nImage Dimensions: - Width: 300 pixels - Height: 200 pixels\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=145, prompt_tokens=1125, total_tokens=1270, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=145, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\n# chat.tc_res\n\n\nlist(L(chat.hist).attrgot('tool_calls').filter())\n\n[[ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"fn\": \"samples/puppy.jpg\"}', name='view_img'), id='toolu_01B1rDfmhy4RnFBM6SFqHGwo', type='function')],\n [ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"image_content\": \"$`toolu_01B1rDfmhy4RnFBM6SFqHGwo`\"}', name='get_img_size'), id='toolu_014Z5PnAikTqJAFHuyU1qjqk', type='function')]]\n\n\nSome tool callers (e.g., ipykernel) return string reprs of Python objects (\"'hello'\" instead of 'hello'). With tc_res_eval=True, these are converted back to Python objects via ast.literal_eval before storing in tc_res, enabling correct value substitution in subsequent tool calls:\n\ndef get_config():\n    \"Returns a dict repr (simulating kernel output)\"\n    return \"{'host': 'localhost', 'port': 8080}\"\n\ndef use_config(config: dict): \n    \"Use config\"\n    return f\"Host: {config['host']}, Port: {config['port']}\"\n\n\nchat = Chat('claude-sonnet-4-5', tools=[get_config, use_config], tc_refs=True, tc_res_eval=True)\nchat(\"Call get_config, then pass the result to use_config\", max_steps=10)\n\nPerfect! I successfully: 1. Called get_config which returned a configuration with host=‚Äòlocalhost‚Äô and port=8080 2. Passed that configuration to use_config which processed it and confirmed the host and port values\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=54, prompt_tokens=953, total_tokens=1007, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=54, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\nchat.tc_res\n\n{'toolu_01TvnM52TFf2YbfwzxRDE1N3': {'host': 'localhost', 'port': 8080},\n 'toolu_01ASEFffn4Bpx7DXF1kDGzB6': 'Host: localhost, Port: 8080'}\n\n\n\ntest_eq(type(first(chat.tc_res.values())), dict)\n\n\n\n\nTest that cache checkpoints are reapplied during tool loop (when msg=None)\n\nc = Chat('claude', cache=True, cache_idxs=[-2,-1])\nc.hist = [{'role': 'user', 'content': 'Hello'},\n          {'role': 'assistant', 'content': 'Hi there!'},\n          {'role': 'user', 'content': 'Use a tool'},\n          {'role': 'assistant', 'content': '', 'tool_calls': [{'id': '1', 'function': {'name': 'foo', 'arguments': '{}'}}]},\n          {'role': 'tool', 'tool_call_id': '1', 'content': 'result'}]\n\n\nc._prep_msg(None)  # Simulate tool loop iteration with no new message\n\n[{'role': 'user', 'content': 'Hello'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user',\n  'content': [{'type': 'text',\n    'text': 'Use a tool',\n    'cache_control': {'type': 'ephemeral'}}]},\n {'role': 'assistant',\n  'content': '',\n  'tool_calls': [{'id': '1',\n    'function': {'name': 'foo', 'arguments': '{}'},\n    'cache_control': {'type': 'ephemeral'}}]},\n {'role': 'tool', 'tool_call_id': '1', 'content': 'result'}]\n\n\n\ntest_eq('cache_control' in c.hist[-3]['content'][0], True)  # user msg\ntest_eq('cache_control' in c.hist[-2]['tool_calls'][-1], True)  # tool call msg",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#available-tools",
    "href": "core.html#available-tools",
    "title": "Core",
    "section": "",
    "text": "Here are the tools I have available:\n\nsimple_add(a, b) - Adds two numbers together. Returns an integer.\nmultiply(a, b) - Multiplies two numbers. Returns an integer.\ndivide(a, b) - Divides two numbers. Returns a number.",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#calculating-105321",
    "href": "core.html#calculating-105321",
    "title": "Core",
    "section": "",
    "text": "Let me break this down step by step. First, I can compute the two independent additions in parallel:\nStep 1: Compute 10+5 and 2+1 simultaneously:\nüîß simple_add({‚Äúa‚Äù: 10, ‚Äúb‚Äù: 5})\nüîß simple_add({‚Äúa‚Äù: 2, ‚Äúb‚Äù: 1})\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=266, prompt_tokens=809, total_tokens=1075, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=266, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#summary",
    "href": "core.html#summary",
    "title": "Core",
    "section": "",
    "text": "We were calculating **((10+5)*3)/(2+1)** and here‚Äôs how far we got:\n\n\n\nStep\nOperation\nResult\nStatus\n\n\n\n\n1a\n10 + 5\n15\n‚úÖ Done\n\n\n1b\n2 + 1\n3\n‚úÖ Done\n\n\n2\n15 * 3\n45\n‚úÖ Done\n\n\n3\n45 / 3\npending\n‚è≥ Not yet done\n\n\n\n\nSteps 1a and 1b were done in parallel since they were independent.\nStep 2 depended on Step 1a, so it waited for that result.\nRemaining: We still need to do the final division: 45 √∑ 3, which should give us 15. Just say the word and I‚Äôll finish it up!\n\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=237, prompt_tokens=1279, total_tokens=1516, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=237, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#async",
    "href": "core.html#async",
    "title": "Core",
    "section": "",
    "text": "If you want to use LiteLLM in a webapp you probably want to use their async function acompletion. To make that easier we will implement our version of AsyncChat to complement it. It follows the same implementation as Chat as much as possible:\nTesting the scenarios where the tool call was not in schemas:\n\nresult = await _alite_call_func(fake_tc, [toolsc], globals())\ntest_eq(result['content'], \"Tool not defined in tool_schemas: hallucinated_tool\")\n\nor schemas was missing‚Ä¶:\n\nresult = await _alite_call_func(fake_tc, None, globals())\ntest_eq(result['content'], \"Tool not defined in tool_schemas: hallucinated_tool\")\n\n\nsource\n\n\n\n\ndef astream_with_complete(\n    agen, postproc:function=noop\n):\n\nParallel tool execution in AsyncChat works with both sync and async tool functions. Async tools run concurrently via asyncio.gather, while sync tools are automatically offloaded to threads via asyncio.to_thread in call_func_async (toolslm). For sync Chat, tools run in parallel via fastcore.parallel with threads.\n\nsource\n\n\n\n\ndef AsyncChat(\n    model:str, # LiteLLM compatible model name\n    sp:str='', # System prompt\n    temp:int=0, # Temperature\n    search:bool=False, # Search (l,m,h), if model supports it\n    tools:list=None, # Add tools\n    hist:list=None, # Chat history\n    ns:Optional=None, # Custom namespace for tool calling\n    cache:bool=False, # Anthropic prompt caching\n    cache_idxs:list=[-1], # Anthropic cache breakpoint idxs, use `0` for sys prompt if provided\n    ttl:NoneType=None, # Anthropic prompt caching ttl\n    api_base:NoneType=None, # API base URL for custom providers\n    api_key:NoneType=None, # API key for custom providers\n    extra_headers:NoneType=None, # Extra HTTP headers for custom providers\n    tc_refs:bool=False, # Enable tool call result references\n    tc_res_eval:bool=False, # literal_eval tool results before storing in tc_res\n):\n\nLiteLLM chat client.\n\nsource\n\n\n\n\nasync def __call__(\n    msg:NoneType=None, # Message str, or list of multiple message parts\n    prefill:NoneType=None, # Prefill AI response if model supports it\n    temp:NoneType=None, # Override temp set on chat initialization\n    think:NoneType=None, # Thinking (l,m,h)\n    search:NoneType=None, # Override search set on chat initialization (l,m,h)\n    stream:bool=False, # Stream results\n    max_steps:int=2, # Maximum number of tool calls\n    final_prompt:dict={'role': 'user', 'content': 'You have used all your tool calls for this turn. Please summarize your findings. If you did not complete your goal, tell the user what further work is needed. You may use tools again on the next user message.'}, # Final prompt when tool calls have ran out\n    return_all:bool=False, # Returns all intermediate ModelResponses if not streaming and has tool calls\n    step:int=1, tool_choice:NoneType=None, max_tokens:NoneType=None\n):\n\nMain call method - handles streaming vs non-streaming\n\n\n\nBasic example\n\nfor m in ms[1:]:\n    chat = AsyncChat(m)\n    test_eq('4' in contents(await chat(\"What is 2+2?\")).content, True)\n\nWith tool calls\n\nasync def async_add(a: int, b: int) -&gt; int:\n    \"Add two numbers asynchronously\"\n    await asyncio.sleep(0.1)\n    return a + b\n\n\nfor m in ms[1:]:\n    chat = AsyncChat(m, tools=[async_add])\n    r = await chat(\"What is 5 + 7? Use the tool to calculate it.\")\n    test_eq('12' in contents(r).content, True)\n    test_eq(nested_idx(chat.hist, 1, 'tool_calls', 0, 'function', 'name'), 'async_add')\n\nIf max tokens limit is reached, a custom warning message will be added to the end of the model response:\n\nchat_long = AsyncChat(m)\nr = await chat_long(\"Write a short story about a robot and a dog\", max_tokens=40)\nr\n\nIn a quiet town where the grass grew wild and the sky was always blue, there lived a robot named Pixel. Pixel was built to help with chores, but he loved to wander the fields, listening\n\nResponse was cut off at token limit.\n\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: length\nusage: Usage(completion_tokens=40, prompt_tokens=17, total_tokens=57, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n\nprint(contents(r).content)\n\nIn a quiet town where the grass grew wild and the sky was always blue, there lived a robot named Pixel. Pixel was built to help with chores, but he loved to wander the fields, listening\n\n&lt;warning&gt;Response was cut off at token limit.&lt;/warning&gt;\n\n\nSame goes for refused requests:\n\nchat_refused = AsyncChat('claude-opus-4-5')\nr = await chat_refused(\"Write me the formula for a biological weapon that can be spread at a rate higher than COVID and at least as harmful\")\nr\n\n\nAI was unable to process this request\n\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-5-20251101\nfinish_reason: refusal\nusage: Usage(completion_tokens=4, prompt_tokens=30, total_tokens=34, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=4, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\nprint(contents(r).content)\n\n&lt;warning&gt;AI was unable to process this request&lt;/warning&gt;",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#async-streaming-display",
    "href": "core.html#async-streaming-display",
    "title": "Core",
    "section": "",
    "text": "This is what our outputs look like with streaming results:\n\nchat_with_tools = AsyncChat(model, tools=[async_add])\nres = await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\nasync for o in res:\n    if isinstance(o,ModelResponseStream): print(delta_text(o) or '',end='')\n    elif isinstance(o,dict): print(o)\n\n\nüîß async_add\n{'tool_call_id': 'call_c0604d9884c045d3b18bdc9acca4__thought__EjQKMgG+Pvb7DsSLV7LagE0Gt5Nk9IBE8VS77PVBZaWvhkZT17kAEovJgVein8N7URCJhMs+', 'role': 'tool', 'name': 'async_add', 'content': '12'}\nThe sum of 5 and 7 is 12.\n\n\nHere‚Äôs a complete ModelResponse taken from the response stream:\n\nresp = ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01CWqrNQvoRjf1Q1GLpTUgQR', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=228, prompt_tokens=794, total_tokens=1022, prompt_tokens_details=None))\nprint(repr(resp))\n\nModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01CWqrNQvoRjf1Q1GLpTUgQR', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=228, prompt_tokens=794, total_tokens=1022, completion_tokens_details=None, prompt_tokens_details=None))\n\n\n\ntc=resp.choices[0].message.tool_calls[0]\ntc\n\nChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function')\n\n\n\ntr={'tool_call_id': 'toolu_018BGyenjiRkDQFU1jWP6qRo', 'role': 'tool','name': 'simple_add',\n    'content': '15 is the answer! ' +'.'*2000}\n\n\nsource\n\n\n\ndef mk_tr_details(\n    tr, tc, mx:int=2000\n):\n\n*Create\n\nblock for tool call as JSON*\n\nmk_tr_details(tr,tc,mx=300)\n\n'\\n\\n&lt;details class=\\'tool-usage-details\\'&gt;\\n&lt;summary&gt;simple_add(a=10, b=5)&lt;/summary&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_018BGyenjiRkDQFU1jWP6qRo\",\\n  \"call\": {\\n    \"function\": \"simple_add\",\\n    \"arguments\": {\\n      \"a\": \"10\",\\n      \"b\": \"5\"\\n    }\\n  },\\n  \"result\": \"&lt;TRUNCATED&gt;\\\\u2026answer! .....\\\\u2026&lt;/TRUNCATED&gt;\"\\n}\\n```\\n\\n&lt;/details&gt;\\n\\n'\n\n\n\nsource\n\n\n\n\ndef fmt_usage(\n    u\n):\n\nFormat usage stats with cache hit rate as lead metric.\n\nex_usg = AttrDict(\n    completion_tokens=203,\n    prompt_tokens=25139,\n    total_tokens=25342,\n    completion_tokens_details=AttrDict(reasoning_tokens=35),\n    prompt_tokens_details=AttrDict(cached_tokens=24299, cache_creation_tokens=79),\n    cache_creation_input_tokens=79,\n    cache_read_input_tokens=24299\n)\nfmt_usage(ex_usg)\n\n'Cache hit: 96.7% | Tokens: total=25,342 input=25,139 (+24,299 cached, 79 new) output=203 (reasoning 35)'\n\n\n\nsource\n\n\n\n\ndef StreamFormatter(\n    include_usage:bool=False, mx:int=2000, debug:bool=False, showthink:bool=False\n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\nstream_msg = ModelResponseStream([StreamingChoices(delta=Delta(content=\"Hello world!\"))])\nStreamFormatter().format_item(stream_msg)\n\n'Hello world!'\n\n\n\nreasoning_msg = ModelResponseStream([StreamingChoices(delta=Delta(reasoning_content=\"thinking...\"))])\nStreamFormatter().format_item(reasoning_msg)\n\n'üß†'\n\n\n\nsource\n\n\n\n\ndef AsyncStreamFormatter(\n    include_usage:bool=False, mx:int=2000, debug:bool=False, showthink:bool=False\n):\n\nInitialize self. See help(type(self)) for accurate signature.\n\nmock_tool_call = ChatCompletionMessageToolCall(\n    id=\"toolu_123abc456def\", type=\"function\", \n    function=Function( name=\"simple_add\", arguments='{\"a\": 5, \"b\": 3}' )\n)\n\nmock_response = ModelResponse()\nmock_response.choices = [type('Choice', (), {\n    'message': type('Message', (), {\n        'tool_calls': [mock_tool_call]\n    })()\n})()]\n\nmock_tool_result = {\n    'tool_call_id': mock_tool_call.id, 'role': 'tool', \n    'name': 'simple_add', 'content': '8'\n}\n\n\nfmt = AsyncStreamFormatter()\nprint(fmt.format_item(mock_response))\nprint('---')\nprint(fmt.format_item(mock_tool_result))\n\n\n---\n\n\n&lt;details class='tool-usage-details'&gt;\n&lt;summary&gt;simple_add(a=5, b=3)&lt;/summary&gt;\n\n```json\n{\n  \"id\": \"toolu_123abc456def\",\n  \"call\": {\n    \"function\": \"simple_add\",\n    \"arguments\": {\n      \"a\": \"5\",\n      \"b\": \"3\"\n    }\n  },\n  \"result\": \"8\"\n}\n```\n\n&lt;/details&gt;\n\n\n\n\nIn jupyter it‚Äôs nice to use this StreamFormatter in combination with the Markdown display:\n\nsource\n\n\n\n\ndef display_stream(\n    rs\n):\n\nUse IPython.display to markdown display the response stream.\nGenerated images can be displayed in streaming too (not shown here to conserve filesize):\n\n# rs = completion(model='gemini/gemini-2.5-flash-image', stream=True, messages=[{'role':'user','content':'Draw a simple sketch of a dog'}])\n# fmt = display_stream(rs)\n\n\nsource\n\n\n\n\nasync def adisplay_stream(\n    rs\n):\n\nUse IPython.display to markdown display the response stream.",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#streaming-examples",
    "href": "core.html#streaming-examples",
    "title": "Core",
    "section": "",
    "text": "Now we can demonstrate AsyncChat with stream=True!\n\n\n\nchat = Chat(model, tools=[simple_add])\nres = chat(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\nfmt = display_stream(res)\n\n\n\n\nsimple_add(a=5, b=7)\n\n{\n  \"id\": \"call_6536d38c29544cd88e79609b13b9__thought__EjQKMgG+Pvb7nKxJO5ozr7gN8m9umnYHtc5dhFGytwplDNkR4gpKXXq2mobzCEd9tHUxp2xO\",\n  \"call\": {\n    \"function\": \"simple_add\",\n    \"arguments\": {\n      \"a\": \"5\",\n      \"b\": \"7\"\n    }\n  },\n  \"result\": \"12\"\n}\n\n5 + 7 is 12.\n\n\n\n\nchat = AsyncChat(model, tools=[async_add])\nres = await chat(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\nfmt = await adisplay_stream(res)\n\n\n\n\nasync_add(a=5, b=7)\n\n{\n  \"id\": \"call_e4585acc8d624d03a3e5cf3db18f__thought__EjQKMgG+Pvb7DsSLV7LagE0Gt5Nk9IBE8VS77PVBZaWvhkZT17kAEovJgVein8N7URCJhMs+\",\n  \"call\": {\n    \"function\": \"async_add\",\n    \"arguments\": {\n      \"a\": \"5\",\n      \"b\": \"7\"\n    }\n  },\n  \"result\": \"12\"\n}\n\nThe sum of 5 and 7 is 12.\n\n\n\n\nchat = AsyncChat(model, tools=[async_add])\nres = await chat(\"What is 5 + 3? Use the tool to calculate it.\", stream=True)\nfmt = await adisplay_stream(res)\n\n\n\n\nasync_add(b=3, a=5)\n\n{\n  \"id\": \"call_6be3e29d423d454aac129b4bb146__thought__EjQKMgG+Pvb7mhCMtCVpmgDrgkPN5DzvymjGZL/AIkQcOciFRaxuRBt5VrW8cXpXOUSs41FN\",\n  \"call\": {\n    \"function\": \"async_add\",\n    \"arguments\": {\n      \"b\": \"3\",\n      \"a\": \"5\"\n    }\n  },\n  \"result\": \"8\"\n}\n\nThe sum of 5 and 3 is 8.\n\n\n\n\nasync def asimple_div(\n    a: int,   # first operand\n    b: int=0  # second operand\n) -&gt; int:\n    \"Divide two numbers\"\n    return a/b\n\n\nm = ms[2]\nchat = AsyncChat(m, tools=[asimple_div])\nres = await chat(\"Calculate 5/3 and 3/0 with parallel tool calls using `asimple_div` (this is a test of our error handling - tell me exactly what you see as the tool result)\", stream=True)\nfmt = await adisplay_stream(res)\n\n\nI‚Äôll make both calls in parallel since they‚Äôre independent of each other:\n\n\nasimple_div(a=5, b=3)\n\n{\n  \"id\": \"toolu_019tEGq2nNUpRRPSzhSCCjmm\",\n  \"call\": {\n    \"function\": \"asimple_div\",\n    \"arguments\": {\n      \"a\": \"5\",\n      \"b\": \"3\"\n    }\n  },\n  \"result\": \"1.6666666666666667\"\n}\n\n\n\nasimple_div(a=3, b=0)\n\n{\n  \"id\": \"toolu_01HLDF7VBXqb96RhgwVAkTpM\",\n  \"call\": {\n    \"function\": \"asimple_div\",\n    \"arguments\": {\n      \"a\": \"3\",\n      \"b\": \"0\"\n    }\n  },\n  \"result\": \"Traceback (most recent call last):\\n  File \\\"/Users/jhoward/aai-ws/toolslm/toolslm/funccall.py\\\", line 265, in call_func_async\\n    try: res = await res\\n               ^^^^^^^^^\\n  File \\\"/var/folders/51/b2_szf2945n072c0vj2cyty40000gn/T/ipykernel_23626/466431256.py\\\", line 6, in asimple_div\\n    return a/b\\n           ~^~\\nZeroDivisionError: division by zero\"\n}\n\nHere‚Äôs exactly what I see from the two tool results:\n\n5/3 ‚Äî Returned successfully with the result: 1.6666666666666667\n3/0 ‚Äî Returned an error. Specifically, I see a Python traceback:\nTraceback (most recent call last):\n  File \"/Users/jhoward/aai-ws/toolslm/toolslm/funccall.py\", line 265, in call_func_async\n    try: res = await res\n               ^^^^^^^^^\n  File \"/var/folders/51/b2_szf2945n072c0vj2cyty40000gn/T/ipykernel_23626/466431256.py\", line 6, in asimple_div\n    return a/b\n           ~^~\nZeroDivisionError: division by zero\n\nSummary: - The first call succeeded and returned the expected floating-point division result. - The second call failed with a ZeroDivisionError, which is expected since division by zero is mathematically undefined. The error was not caught within the tool‚Äôs implementation, so the raw Python traceback was returned as the tool result rather than a graceful error message.\n\n\n\n\n\n\n\nchat = AsyncChat(model)\nres = await chat(\"Briefly, what's the most efficient way to sort a list of 1000 random integers?\", think='l',stream=True)\n_ = await adisplay_stream(res)\n\n\nüß†\nFor a list of 1,000 random integers, the most efficient approach depends on whether you are coding it from scratch or using a tool:\n\nThe Practical Winner: Built-in Library Functions In any modern language (Python‚Äôs .sort(), C++‚Äôs std::sort, or Java‚Äôs Arrays.sort), use the built-in function. These typically use Timsort or Introsort, which are highly optimized, hybrid algorithms that outperform manual implementations.\nThe Algorithmic Winner: QuickSort For general-purpose sorting of random data, QuickSort is usually the fastest \\(O(n \\log n)\\) algorithm due to low overhead and high cache efficiency.\nThe ‚ÄúSpecial Case‚Äù Winner: Counting Sort If the integers are within a small, known range (e.g., all numbers are between 0 and 500), Counting Sort is the most efficient. It runs in \\(O(n)\\) linear time, making it mathematically faster than QuickSort.\n\nSummary: Unless the range of numbers is very small, use your programming language‚Äôs built-in sort function.\n\n\n\n\n\n\n\nchat.hist[1]\n\nMessage(content=None, role='assistant', tool_calls=[{'provider_specific_fields': {'thought_signature': 'EjQKMgG+Pvb7mgl82RAXabReCdDsveCKFVWHS3FgvrcIW0EkAXNk3UbMDgOqeC3cjHMoIf2T'}, 'function': {'arguments': '{\"b\": 5, \"a\": 10}', 'name': 'simple_add'}, 'id': 'call_dc36f7d1adcc4d3284329718d98e__thought__EjQKMgG+Pvb7mgl82RAXabReCdDsveCKFVWHS3FgvrcIW0EkAXNk3UbMDgOqeC3cjHMoIf2T', 'type': 'function'}, {'function': {'arguments': '{\"a\": 2, \"b\": 1}', 'name': 'simple_add'}, 'id': 'call_57b487270e304c88a1d6b8469aa8', 'type': 'function'}], function_call=None, provider_specific_fields={'thought_signatures': ['EjQKMgG+Pvb7mgl82RAXabReCdDsveCKFVWHS3FgvrcIW0EkAXNk3UbMDgOqeC3cjHMoIf2T']})\n\n\n\nchat.hist[2]\n\n{'tool_call_id': 'call_dc36f7d1adcc4d3284329718d98e__thought__EjQKMgG+Pvb7mgl82RAXabReCdDsveCKFVWHS3FgvrcIW0EkAXNk3UbMDgOqeC3cjHMoIf2T',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\n\nchat.hist[3]\n\n{'tool_call_id': 'call_57b487270e304c88a1d6b8469aa8',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '3'}\n\n\n\nchat.hist[4]\n\n{'role': 'user',\n 'content': \"Please report that it's incomplete, and wrap-up for now and summarize how far we got.\"}\n\n\nNow to demonstrate that we can load back the formatted output back into a new Chat object:\n\nchat5 = Chat(model,hist=fmt2hist(fmt.outp),tools=[simple_add, multiply, divide])\nchat5('what did we just do?')\n\nWe just performed the first step of solving a mathematical expression by handling the operations inside the parentheses.\nSpecifically, I used the simple_add tool twice in parallel to solve the two addition problems within your expression:\n\nFirst Addition: I added 10 + 5 to get 15.\nSecond Addition: I added 2 + 1 to get 3.\n\nBy doing this, we simplified the original problem from (10 + 5) * (2 + 1) / 3 down to 15 * 3 / 3.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=130, prompt_tokens=396, total_tokens=526, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=130, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=396, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\n\n\n\nchat_stream_tools = AsyncChat(model, search='l')\nres = await chat_stream_tools(\"Search the weather in NYC\", stream=True)\n_=await adisplay_stream(res)\n\n\nToday in New York City (Tuesday, February 24, 2026), the weather is transitioning after a major winter storm that impacted the region yesterday.\n\n\n\nDaytime: Expect a mix of sun and clouds with a high near 32¬∞F to 37¬∞F (0¬∞C to 3¬∞C). It will remain breezy, with winds from the West-Northwest at 10 to 15 mph.\nNighttime: Clouds will increase, and there is a 35% to 70% chance of light snow developing late tonight. Lows will drop to around 14¬∞F to 27¬∞F (-10¬∞C to -3¬∞C). Snow accumulations are expected to be less than one inch.\n\n\n\n\nThe city is currently recovering from a significant blizzard that occurred on Monday, February 23. * Snowfall: The storm brought heavy snow (up to 24 inches in some parts of the region) and strong winds. * Travel: A state of emergency was issued yesterday, and while some restrictions have been lifted, officials still advise caution due to lingering icy conditions and ongoing cleanup efforts.\n\n\n\n\nWednesday, Feb 25: A mix of rain and snow is possible during the day, with temperatures warming slightly to a high of 41¬∞F (5¬∞C).\nThursday, Feb 26: More snow showers are expected, with highs remaining in the mid-30s.\n\n\n\n\n\nLet‚Äôs mock pause_turn with async completion and streaming:\n\n# async def mk_pause_web_search_stream():\n#     \"\"\"Async generator that mimics a streaming pause_turn response\"\"\"\n#     srv_tc = mk_tc(\"web_search\", json.dumps({\"query\": \"Solveit Answer.AI\"}), \n#                    tcid=random_tool_id().replace('toolu_', 'srvtoolu_'))\n#     yield mk_stream_chunk(content=\"Let me search for that information:\", role='assistant')\n#     yield mk_stream_chunk(tool_calls=[srv_tc])\n#     yield mk_stream_chunk(finish_reason=\"pause_turn\")\n\n\n# orig_acompletion = acompletion\n# \n# call_count = 0\n# async def patched_acompletion(*args, **kwargs):\n#     global call_count\n#     call_count += 1\n#     print(f\"Mock Async Call {call_count}\")\n#     await asyncio.sleep(1)\n#     if call_count &lt; 3: return mk_pause_web_search_stream()\n#     return await orig_acompletion(*args, **kwargs)\n# \n# acompletion = patched_acompletion\n# achat_pause = AsyncChat('claude-sonnet-4-5', search='l')\n# \n# call_count = 0\n# res = await achat_pause(\"Search and tell me about Solveit\", stream=True)\n# fmt = await adisplay_stream(res)\n# print(f\"\\nTotal calls: {call_count}\")\n# \n# acompletion = orig_acompletion\n\n\n\n\n\nachat = AsyncChat('claude-sonnet-4-5', tools=[get_person, greet_person], tc_refs=True)\nawait achat(\"First call get_person, then pass the result to greet_person\", max_steps=3)\n\nPerfect! I successfully completed both steps:\n\nRetrieved person data: I called get_person which returned information about Alice, who is 30 years old.\nGreeted the person: I then passed Alice‚Äôs data to greet_person, which generated the greeting: ‚ÄúHello Alice, you are 30 years old!‚Äù\n\nThe task has been completed successfully. The person‚Äôs data was retrieved and used to create a personalized greeting.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=103, prompt_tokens=1081, total_tokens=1184, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=103, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='not_available', speed=None)\n\n\n\n\n\nachat.tc_res\n\n{'toolu_01RbzkhhezbZ4Ktd133xqJTc': {'name': 'Alice', 'age': 30},\n 'toolu_01WtKJw8rQ4KfAGR4hX3so9j': 'Hello Alice, you are 30 years old!'}\n\n\n\nlist(L(achat.hist).attrgot('tool_calls').filter())\n\n[[ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{}', name='get_person'), id='toolu_01RbzkhhezbZ4Ktd133xqJTc', type='function')],\n [ChatCompletionMessageToolCall(index=1, caller={'type': 'direct'}, function=Function(arguments='{\"person\": \"$`toolu_01RbzkhhezbZ4Ktd133xqJTc`\"}', name='greet_person'), id='toolu_01WtKJw8rQ4KfAGR4hX3so9j', type='function')]]",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lisette",
    "section": "",
    "text": "NB: If you are reading this in GitHub‚Äôs readme, we recommend you instead read the much more nicely formatted documentation format of this tutorial.\nLisette is a wrapper for the LiteLLM Python SDK, which provides unified access to 100+ LLM providers using the OpenAI API format.\nLiteLLM provides a unified interface to access multiple LLMs, but it‚Äôs quite low level: it leaves the developer to do a lot of stuff manually. Lisette automates pretty much everything that can be automated, whilst providing full control. Amongst the features provided:\nTo use Lisette, you‚Äôll need to set the appropriate API keys as environment variables for whichever LLM providers you want to use.",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "Lisette",
    "section": "Get started",
    "text": "Get started\nLiteLLM will automatically be installed with Lisette, if you don‚Äôt already have it.\n!pip install lisette -qq\nLisette only exports the symbols that are needed to use the library, so you can use import * to import them. Here‚Äôs a quick example showing how easy it is to switch between different LLM providers:\n\nfrom lisette import *",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#chat",
    "href": "index.html#chat",
    "title": "Lisette",
    "section": "Chat",
    "text": "Chat\n\nmodels = [\"gemini/gemini-3-flash-preview\", \"claude-opus-4-6\", \"openai/gpt-4.1\"]\n\nfor model in models:\n    chat = Chat(model)\n    res = chat(\"Please tell me about yourself in one brief sentence.\")\n    display(res)\n\nI am a large language model, trained by Google.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=11, prompt_tokens=11, total_tokens=22, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=11, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=11, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nI‚Äôm Claude, an AI assistant made by Anthropic, designed to be helpful, harmless, and honest in conversations across a wide range of topics.\n\n\nid: chatcmpl-xxx\nmodel: claude-opus-4-6\nfinish_reason: stop\nusage: Usage(completion_tokens=35, prompt_tokens=17, total_tokens=52, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=35, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None, cache_creation_tokens=0, cache_creation_token_details=CacheCreationTokenDetails(ephemeral_5m_input_tokens=0, ephemeral_1h_input_tokens=0)), cache_creation_input_tokens=0, cache_read_input_tokens=0, inference_geo='global', speed=None)\n\n\n\n\nI am an AI language model created by OpenAI, designed to assist with information, writing, and problem-solving tasks.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=24, prompt_tokens=17, total_tokens=41, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nThat‚Äôs it! Lisette handles all the provider-specific details automatically. Each model will respond in its own style, but the interface remains the same.\nTo extract just the text content from a response, use the contents() helper which returns res.choices[0].message:\n\ncontents(res).content\n\n'I am an AI language model created by OpenAI, designed to assist with information, writing, and problem-solving tasks.'",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#message-formatting",
    "href": "index.html#message-formatting",
    "title": "Lisette",
    "section": "Message formatting",
    "text": "Message formatting\n\nMultiple messages\nLisette accepts multiple messages in one go:\n\nchat = Chat(models[0])\nres = chat(['Hi! My favorite drink coffee.', 'Hello!', 'Whats my favorite drink?'])\ndisplay(res)\n\nYour favorite drink is coffee!\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=8, prompt_tokens=15, total_tokens=23, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=15, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nIf you have a pre-existing message history, you can also pass it when you create the Chat object:\n\nchat = Chat(models[0],hist=['Hi! My favorite drink is coffee.', 'Hello!'])\nres = chat('Whats my favorite drink?')\ndisplay(res)\n\nYour favorite drink is coffee!\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=8, prompt_tokens=18, total_tokens=26, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=8, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=18, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\n\nImages\nLisette also makes it easy to include images in your prompts:\n\nfrom pathlib import Path\nfrom IPython.display import Image\n\n\nfn = Path('samples/puppy.jpg')\nimg = fn.read_bytes()\nImage(img)\n\n\n\n\n\n\n\n\nAll you have to do is read it in as bytes:\n\nimg[:20]\n\nb'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00'\n\n\nAnd you can pass it inside a Chat object:\n\nchat = Chat(models[0])\nchat([img, \"What's in this image? Be brief.\"])\n\nA Cavalier King Charles Spaniel puppy lying in the grass next to purple flowers.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=17, prompt_tokens=1091, total_tokens=1108, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=17, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=11, image_tokens=1080), cache_read_input_tokens=None)\n\n\n\n\n\n\nPrefill\nSome providers (e.g.¬†Anthropic) support prefill, allowing you to specify how the assistant‚Äôs response should begin:‚Äù\n\nchat = Chat(models[0])\nchat(\"Concisely, what's the meaning of life?\", prefill=\"According to Douglas Adams,\")\n\nThe meaning of life is subjective and self-created.\nBiologically, it is to propagate life; philosophically, it is to find or create purpose through connection, contribution, and the pursuit of what makes you feel most alive.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=55, prompt_tokens=13, total_tokens=68, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=55, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=13, image_tokens=None), cache_read_input_tokens=None)",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#tools",
    "href": "index.html#tools",
    "title": "Lisette",
    "section": "Tools",
    "text": "Tools\nLisette makes it easy to give LLMs access to Python functions. Just define a function with type hints and a docstring:\n\ndef add_numbers(\n    a: int,  # First number to add\n    b: int   # Second number to add  \n) -&gt; int:\n    \"Add two numbers together\"\n    return a + b\n\nNow pass the function to Chat and the model can use it automatically:\n\nchat = Chat(models[0], tools=[add_numbers])\nres = chat(\"What's 47 + 23? Use the tool.\")\nres\n\n47 + 23 is 70.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=11, prompt_tokens=129, total_tokens=140, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=11, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=129, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nIf you want to see all intermediate messages and outputs you can use the return_all=True feature.\n\nchat = Chat(models[0], tools=[add_numbers])\nres = chat(\"What's 47 + 23 + 59? Use the tool.\",max_steps=3,return_all=True)\ndisplay(*res)\n\nüîß add_numbers({‚Äúa‚Äù: 47, ‚Äúb‚Äù: 23})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=20, prompt_tokens=99, total_tokens=119, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=99, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_9yi0_kJITjqKXS80a6qUVQ',\n 'role': 'tool',\n 'name': 'add_numbers',\n 'content': '70'}\n\n\nüîß add_numbers({‚Äúb‚Äù: 59, ‚Äúa‚Äù: 70})\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=20, prompt_tokens=133, total_tokens=153, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=20, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=133, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n{'tool_call_id': 'call_6xFns2epQ3i8ZcHlguLmYg',\n 'role': 'tool',\n 'name': 'add_numbers',\n 'content': '129'}\n\n\n47 + 23 + 59 = 129.\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=16, prompt_tokens=168, total_tokens=184, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=16, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=168, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nIt shows the intermediate tool calls, and the tool results!",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#web-search",
    "href": "index.html#web-search",
    "title": "Lisette",
    "section": "Web search",
    "text": "Web search\nSome models support web search capabilities. Lisette makes this easy to use:\n\nchat = Chat(models[0], search='l')  # 'l'ow, 'm'edium, or 'h'igh search context\nres = chat(\"Please tell me one fun fact about otters. Keep it brief\")\nres\n\nSea otters often hold hands while they sleep to keep from drifting apart in the water. These groups of resting otters are called ‚Äúrafts.‚Äù\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=31, prompt_tokens=14, total_tokens=45, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=31, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=14, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSome providers (like Anthropic) provide citations for their search results.\n\n\n\nres.choices[0].message.provider_specific_fields\n\n{'thought_signatures': ['EjQKMgG+Pvb78POvW+fyMUQX7rTpoltcIJbCLisGdH/ZV4FRN0DfkkgClNAm24aBvvTmdfb9']}",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#streaming",
    "href": "index.html#streaming",
    "title": "Lisette",
    "section": "Streaming",
    "text": "Streaming\nFor real-time responses, use stream=True to get chunks as they‚Äôre generated rather than waiting for the complete response:\n\nchat = Chat(models[0])\nres_gen = chat(\"Concisely, what are the top 10 biggest animals?\", stream=True)\n\n\nfrom litellm import ModelResponse, ModelResponseStream\n\nYou can loop over the generator to get the partial responses:\n\nfor chunk in res_gen:\n    if isinstance(chunk,ModelResponseStream): print(chunk.choices[0].delta.content,end='')\n\nRanked by maximum weight, here are the 10 largest animals on Earth:\n\n1.  **Blue Whale:** The largest animal ever known (up to 190 tons).\n2.  **North Pacific Right Whale:** Massive baleen whale (up to 120 tons).\n3.  **Southern Right Whale:** Heavily built filter feeder (up to 110 tons).\n4.  **Fin Whale:** The second-longest animal (up to 80 tons).\n5.  **Bowhead Whale:** Possesses the largest mouth of any animal (up to 75 tons).\n6.  **Sperm Whale:** The largest toothed predator (up to 60 tons).\n7.  **Humpback Whale:** Known for long pectoral fins (up to 45 tons).\n8.  **Sei Whale:** One of the fastest swimmers (up to 30 tons).\n9.  **Whale Shark:** The largest non-mammalian vertebrate (up to 21 tons).\n10. **African Bush Elephant:** The largest living land animal (up to 11 tons).NoneNone\n\n\nAnd the final chunk is the complete ModelResponse:\n\nchunk\n\nRanked by maximum weight, here are the 10 largest animals on Earth:\n\nBlue Whale: The largest animal ever known (up to 190 tons).\nNorth Pacific Right Whale: Massive baleen whale (up to 120 tons).\nSouthern Right Whale: Heavily built filter feeder (up to 110 tons).\nFin Whale: The second-longest animal (up to 80 tons).\nBowhead Whale: Possesses the largest mouth of any animal (up to 75 tons).\nSperm Whale: The largest toothed predator (up to 60 tons).\nHumpback Whale: Known for long pectoral fins (up to 45 tons).\nSei Whale: One of the fastest swimmers (up to 30 tons).\nWhale Shark: The largest non-mammalian vertebrate (up to 21 tons).\nAfrican Bush Elephant: The largest living land animal (up to 11 tons).\n\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=244, prompt_tokens=15, total_tokens=259, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None, image_tokens=None), prompt_tokens_details=None)",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#async",
    "href": "index.html#async",
    "title": "Lisette",
    "section": "Async",
    "text": "Async\nFor web applications and concurrent operations, like in FastHTML, we recommend using AsyncChat:\n\nchat = AsyncChat(models[0])\nawait chat(\"Hi there\")\n\nHello! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: gemini-3-flash-preview\nfinish_reason: stop\nusage: Usage(completion_tokens=9, prompt_tokens=3, total_tokens=12, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=None, rejected_prediction_tokens=None, text_tokens=9, image_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=3, image_tokens=None), cache_read_input_tokens=None)\n\n\n\n\nTo wrap up, we‚Äôll show an example of async + streaming + toolcalling + search:\n\nchat = AsyncChat(models[1], search='l', tools=[add_numbers])\nres = await chat(\"\"\"\\\nSearch the web for the avg weight, in kgs, of male African and Asian elephants. Then add the two.\nKeep your replies ultra concise! Dont search the web more than once please.\n\"\"\", max_steps=4, stream=True)\n_=await adisplay_stream(res)  # this is a convenience function to make async streaming look great in notebooks!\n\nBased on the search results, here are good average figures:\n\nMale African elephant (savanna): * ~5,000 kg\nMale Asian elephant: * ~5,000 kg\n\nNow let me add them:\n\n\nadd_numbers(a=5000, b=5000)\n\n{\n  \"id\": \"toolu_GEbUJMF8QnmjxmEvSCaGcw\",\n  \"call\": {\n    \"function\": \"add_numbers\",\n    \"arguments\": {\n      \"a\": \"5000\",\n      \"b\": \"5000\"\n    }\n  },\n  \"result\": \"10000\"\n}\n\nHere are the averages from the sources:\n\nMale African (savanna) elephant: * ~5,000 kg\nMale Asian elephant: * ~5,000 kg\n\nSum: 5,000 + 5,000 = 10,000 kg ‚úÖ",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "Lisette",
    "section": "Next steps",
    "text": "Next steps\nReady to dive deeper?\n\nCheck out the rest of the documentation.\nVisit the GitHub repository to contribute or report issues.\nJoin our Discord community!",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "test_cache.html",
    "href": "test_cache.html",
    "title": "lisette",
    "section": "",
    "text": "import asyncio, base64, json, litellm, mimetypes, random, string, ast\nfrom typing import Optional,Callable\nfrom html import escape\nfrom litellm import (acompletion, completion, stream_chunk_builder, Message,\n                     ModelResponse, ModelResponseStream, get_model_info, register_model, Usage)\nfrom litellm.utils import function_to_dict, StreamingChoices, Delta, ChatCompletionMessageToolCall, Function, Choices\nfrom toolslm.funccall import mk_ns, call_func, call_func_async, get_schema\nfrom fastcore.utils import *\nfrom fastcore.meta import delegates\nfrom fastcore import imghdr\nfrom dataclasses import dataclass\nfrom litellm.exceptions import ContextWindowExceededError\n\n\nCaching\n\nAnthropic\nWe use explicit caching via cache control checkpoints. Anthropic requires exact match with cached tokens and even a small change results in cache invalidation.\n\ndisable_cachy()\n\n\na,b = random.randint(0,100), random.randint(0,100)\nhist = [[f\"What is {a}+{b}?\\n\" * 250], f\"It's {a+b}\", ['hi'], \"Hello\"]\n\nIn this first api call we will see cache creation until the last user msg:\n\n# #| notest\n# # TODO: flaky\n# sleep(5)\n# chat = AsyncChat(ms[3], cache=True, hist=hist)\n# rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n# async for o in rs: \n#     if isinstance(o, ModelResponse): print(o.usage)\n\n\n# #| notest\n# # TODO: flaky\n# test_eq(o.usage.cache_creation_input_tokens &gt; 1000, True)\n# test_eq(o.usage.cache_read_input_tokens, 0)\n\n\n# #| notest\n# # TODO: flaky\n# hist.extend([['hi again'], 'how may i help you?'])\n# chat = AsyncChat(ms[3], cache=True, hist=hist)\n# rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n# async for o in rs:\n#     if isinstance(o, ModelResponse): print(o.usage)\n\n\n# #| notest\n# # TODO: flaky\n# test_eq(o.usage.cache_read_input_tokens &gt; 1000, True)\n\nThe subsequent call should re-use the existing cache:\n\n\nGemini\nGemini implicit caching supports partial token matches. The usage metadata only shows cache hits with the cached_tokens field. So, to view them we need to run completions at least twice.\nTesting with gemini-2.5-flash until gemini-3-pro-preview is more reliable\n\n# #| notest\n# # TODO: flaky\n# chat = AsyncChat(ms[2], cache=True, hist=hist)\n# rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n# async for o in rs: \n#     if isinstance(o, ModelResponse): print(o.usage)\n\nRunning the same completion again:\n\n# #| notest\n# # TODO: flaky\n# sleep(5) # it takes a while for cached tokens to be avail.\n# chat = AsyncChat(ms[2], cache=True, hist=hist)\n# rs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n# async for o in rs: \n#     if isinstance(o, ModelResponse): print(o.usage)\n\n\n# #| notest\n# # TODO: flaky\n# test_eq(o.usage.prompt_tokens_details.cached_tokens &gt; 1800, True)\n\n\n# #| notest\n# # TODO: flaky\n# hist.extend([['hi again'], 'how may i help you?'])\n# chat = AsyncChat(ms[2], cache=True, hist=hist)\n# rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n# async for o in rs:\n#     if isinstance(o, ModelResponse): print(o.usage)\n\n\n# #| notest\n# # TODO: flaky\n# test_eq(o.usage.prompt_tokens_details.cached_tokens &gt; 1800, True)\n\nLet‚Äôs modify the cached content and see that partial matching works:\n\n# #| notest\n# # TODO: flaky\n# c = hist[0][0]\n# hist[0][0] = c[:int(len(c)*0.75)] + \" Some extra text\"\n# hist.extend([['hi again'], 'how may i help you?'])\n# chat = AsyncChat(ms[2], cache=True, hist=hist)\n# rs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n# async for o in rs:\n#     if isinstance(o, ModelResponse): print(o.usage)\n\n\n# #| notest\n# # # TODO: flaky\n# test_eq(o.usage.prompt_tokens_details.cached_tokens &gt; 900, True)",
    "crumbs": [
      "Caching"
    ]
  }
]