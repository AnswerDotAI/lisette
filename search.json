[
  {
    "objectID": "core.html",
    "href": "core.html",
    "title": "Core",
    "section": "",
    "text": "LiteLLM ModelResponse(Stream) objects have id and created_at fields that are generated dynamically. Even when we use cachy to cache the LLM response these dynamic fields create diffs which makes code review more challenging. The patches below ensure that id and created_at fields are fixed and won‚Äôt generate diffs.\n\nsource\n\n\n\n patch_litellm (seed=0)\n\nPatch litellm.ModelResponseBase such that id and created are fixed.\n\npatch_litellm()\n\n\n\n\n\nLiteLLM provides an convenient unified interface for most big LLM providers. Because it‚Äôs so useful to be able to switch LLM providers with just one argument. We want to make it even easier to by adding some more convenience functions and classes.\nThis is very similar to our other wrapper libraries for popular AI providers: claudette (Anthropic), gaspard (Gemini), cosette (OpenAI).\n\n# litellm._turn_on_debug()\n\n\nms = [\"gemini/gemini-2.5-flash\", \"claude-sonnet-4-5\", \"openai/gpt-4.1\"]\nmsg = [{'role':'user','content':'Hey there!', 'cache_control': {'type': 'ephemeral'}}]\nfor m in ms:\n    display(Markdown(f'**{m}:**'))\n    display(completion(m,msg))\n\ngemini/gemini-2.5-flash:\n\n\nHey there! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: gemini-2.5-flash\nfinish_reason: stop\nusage: Usage(completion_tokens=153, prompt_tokens=4, total_tokens=157, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=143, rejected_prediction_tokens=None, text_tokens=10), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None))\n\n\n\n\nclaude-sonnet-4-5:\n\n\nHello! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=12, prompt_tokens=10, total_tokens=22, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nopenai/gpt-4.1:\n\n\nHello! How can I help you today? üòä\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=10, prompt_tokens=10, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n\n\n\nLet‚Äôs start with making it easier to pass messages into litellm‚Äôs completion function (including images, and pdf files).\n\nsource\n\n\n\n remove_cache_ckpts (msg)\n\nremove cache checkpoints and return msg.\n\nsource\n\n\n\n\n mk_msg (content, role='user', cache=False, ttl=None)\n\nCreate a LiteLLM compatible message.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontent\n\n\nContent: str, bytes (image), list of mixed content, or dict w ‚Äòrole‚Äô and ‚Äòcontent‚Äô fields\n\n\nrole\nstr\nuser\nMessage role if content isn‚Äôt already a dict/Message\n\n\ncache\nbool\nFalse\nEnable Anthropic caching\n\n\nttl\nNoneType\nNone\nCache TTL: ‚Äò5m‚Äô (default) or ‚Äò1h‚Äô\n\n\n\nNow we can use mk_msg to create different types of messages.\nSimple text:\n\nmsg = mk_msg(\"hey\")\nmsg\n\n{'role': 'user', 'content': 'hey'}\n\n\nWhich can be passed to litellm‚Äôs completion function like this:\n\nmodel = ms[1]\n\n\nres = completion(model, [msg])\nres\n\nHey! How‚Äôs it going? What‚Äôs on your mind?\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=16, prompt_tokens=8, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nWe‚Äôll add a little shortcut to make examples and testing easier here:\n\ndef c(msgs, **kw):\n    msgs = [msgs] if isinstance(msgs,dict) else listify(msgs)\n    return completion(model, msgs, **kw)\n\n\nc(msg)\n\nHey! How‚Äôs it going? What‚Äôs on your mind?\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=16, prompt_tokens=8, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nLists w just one string element are flattened for conciseness:\n\ntest_eq(mk_msg(\"hey\"), mk_msg([\"hey\"]))\n\n(LiteLLM ignores these fields when sent to other providers)\nText and images:\n\nimg_fn = Path('samples/puppy.jpg')\nImage(filename=img_fn, width=200)\n\n\n\n\n\n\n\n\n\nmsg = mk_msg(['hey what in this image?',img_fn.read_bytes()])\nprint(json.dumps(msg,indent=1)[:200]+\"...\")\n\n{\n \"role\": \"user\",\n \"content\": [\n  {\n   \"type\": \"text\",\n   \"text\": \"hey what in this image?\"\n  },\n  {\n   \"type\": \"image_url\",\n   \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gxUSU...\n\n\n\nc(msg)\n\nThis image shows an adorable Cavalier King Charles Spaniel puppy! The puppy has the breed‚Äôs characteristic features:\n\nColoring: Brown (chestnut) and white coat\nSweet expression: Large, dark eyes and a gentle face\nSetting: The puppy is lying on grass near some purple flowers (appear to be asters or similar blooms)\n\nThe puppy looks very young and has that irresistibly cute, innocent look that Cavalier puppies are famous for. The photo has a professional quality with nice lighting and composition, capturing the puppy‚Äôs endearing personality perfectly!\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=139, prompt_tokens=104, total_tokens=243, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nLet‚Äôs also demonstrate this for PDFs\n\npdf_fn = Path('samples/solveit.pdf')\nmsg = mk_msg(['Who is the author of this pdf?', pdf_fn.read_bytes()])\nc(msg)\n\nThe author of this PDF is Jeremy Howard from fast.ai. He explicitly introduces himself in the document with ‚ÄúHi, I‚Äôm Jeremy Howard, from fast.ai‚Äù and goes on to describe his work co-founding fast.ai with Rachel Thomas eight years ago.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=59, prompt_tokens=1610, total_tokens=1669, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\n\n\nSome providers such as Anthropic require manually opting into caching. Let‚Äôs try it:\n\ndef cpr(i): return f'{i} '*1024 + 'This is a caching test. Report back only what number you see repeated above.'\n\n\ndisable_cachy()\n\n\nmsg = mk_msg(cpr(1), cache=True)\nres = c(msg)\nres\n\n1\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=5, prompt_tokens=3, total_tokens=8, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=2070, cache_read_input_tokens=0)\n\n\n\n\nAnthropic has a maximum of 4 cache checkpoints, so we remove previous ones as we go:\n\nres = c([remove_cache_ckpts(msg), mk_msg(res), mk_msg(cpr(2), cache=True)])\nres\n\n2\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=5, prompt_tokens=2073, total_tokens=2078, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=2070, text_tokens=None, image_tokens=None), cache_creation_input_tokens=2074, cache_read_input_tokens=2070)\n\n\n\n\nWe see that the first message was cached, and this extra message has been written to cache:\n\nres.usage.prompt_tokens_details\n\nPromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=2070, text_tokens=None, image_tokens=None)\n\n\nWe can add a bunch of large messages in a loop to see how the number of cached tokens used grows.\nWe do this for 25 times to ensure it still works for more than &gt;20 content blocks, which is a known anthropic issue.\nThe code below is commented by default, because it‚Äôs slow. Please uncomment when working on caching.\n\n# h = []\n# msg = mk_msg(cpr(1), cache=True)\n\n# for o in range(2,25):\n#     h += [remove_cache_ckpts(msg), mk_msg(res)]\n#     msg = mk_msg(cpr(o), cache=True)\n#     res = c(h+[msg])\n#     detls = res.usage.prompt_tokens_details\n#     print(o, detls.cached_tokens, detls.cache_creation_tokens, end='; ')\n\n\nenable_cachy()\n\n\n\n\nLisette can call multiple tools in a loop. Further down this notebook, we‚Äôll provide convenience functions for formatting such a sequence of toolcalls and responses into one formatted output string.\nFor now, we‚Äôll show an example and show how to transform such a formatted output string back into a valid LiteLLM history.\n\nfmt_outp = '''\nI'll solve this step-by-step, using parallel calls where possible.\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\n  \"result\": \"15\"\n}\n```\n\n&lt;/details&gt;\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\n  \"result\": \"3\"\n}\n```\n\n&lt;/details&gt;\n\nNow I need to multiply 15 * 3 before I can do the final division:\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\n  \"result\": \"45\"\n}\n```\n\n&lt;/details&gt;\n'''\n\nWe can split into chunks of (text,toolstr,json):\n\nsp = re_tools.split(fmt_outp)\nfor o in list(chunked(sp, 3, pad=True)): print('- ', o)\n\n-  [\"\\nI'll solve this step-by-step, using parallel calls where possible.\\n\\n\", '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}']\n-  ['\\n\\n', '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}']\n-  ['\\n\\nNow I need to multiply 15 * 3 before I can do the final division:\\n\\n', '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}']\n-  ['\\n', None, None]\n\n\n\nsource\n\n\n\n\n fmt2hist (outp:str)\n\nTransform a formatted output into a LiteLLM compatible history\nSee how we can turn that one formatted output string back into a list of Messages:\n\nfrom pprint import pprint\n\n\nh = fmt2hist(fmt_outp)\npprint(h)\n\n[Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":10,\"b\":5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '15',\n  'name': 'simple_add',\n  'role': 'tool',\n  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta'},\n Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":2,\"b\":1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '3',\n  'name': 'simple_add',\n  'role': 'tool',\n  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY'},\n Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":15,\"b\":3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '45',\n  'name': 'multiply',\n  'role': 'tool',\n  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C'},\n Message(content='.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\n\n\n\nWe will skip tool use blocks and tool results during caching\nNow lets make it easy to provide entire conversations:\n\nsource\n\n\n\n\n mk_msgs (msgs, cache=False, cache_idxs=[-1], ttl=None)\n\nCreate a list of LiteLLM compatible messages.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmsgs\n\n\nList of messages (each: str, bytes, list, or dict w ‚Äòrole‚Äô and ‚Äòcontent‚Äô fields)\n\n\ncache\nbool\nFalse\nEnable Anthropic caching\n\n\ncache_idxs\nlist\n[-1]\nCache breakpoint idxs\n\n\nttl\nNoneType\nNone\nCache TTL: ‚Äò5m‚Äô (default) or ‚Äò1h‚Äô\n\n\n\nWith mk_msgs you can easily provide a whole conversation:\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"])\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant', 'content': \"I'm doing fine and you?\"}]\n\n\nBy defualt the last message will be cached when cache=True:\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"], cache=True)\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant',\n  'content': [{'type': 'text',\n    'text': \"I'm doing fine and you?\",\n    'cache_control': {'type': 'ephemeral'}}]}]\n\n\n\ntest_eq('cache_control' in msgs[-1]['content'][0], True)\n\nAlternatively, users can provide custom cache_idxs. Tool call blocks and results are skipped during caching:\n\nmsgs = mk_msgs(['Hello!','Hi! How can I help you?','Call some functions!',fmt_outp], cache=True, cache_idxs=[0,-2,-1])\nmsgs\n\n[{'role': 'user',\n  'content': [{'type': 'text',\n    'text': 'Hello!',\n    'cache_control': {'type': 'ephemeral'}}]},\n {'role': 'assistant', 'content': 'Hi! How can I help you?'},\n {'role': 'user',\n  'content': [{'type': 'text',\n    'text': 'Call some functions!',\n    'cache_control': {'type': 'ephemeral'}}]},\n Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":10,\"b\":5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta',\n  'name': 'simple_add',\n  'content': '15'},\n Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":2,\"b\":1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY',\n  'name': 'simple_add',\n  'content': '3'},\n Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":15,\"b\":3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C',\n  'name': 'multiply',\n  'content': '45'},\n Message(content=[{'type': 'text', 'text': '.', 'cache_control': {'type': 'ephemeral'}}], role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\n\ntest_eq('cache_control' in msgs[0]['content'][0], True)\ntest_eq('cache_control' in msgs[2]['content'][0], True) # shifted idxs to skip tools\ntest_eq('cache_control' in msgs[-1]['content'][0], True)\n\nWho‚Äôs speaking at when is automatically inferred. Even when there are multiple tools being called in parallel (which LiteLLM supports!).\n\nmsgs = mk_msgs(['Tell me the weather in Paris and Rome',\n                'Assistant calls weather tool two times',\n                {'role':'tool','content':'Weather in Paris is ...'},\n                {'role':'tool','content':'Weather in Rome is ...'},\n                'Assistant returns weather',\n                'Thanks!'])\nmsgs\n\n[{'role': 'user', 'content': 'Tell me the weather in Paris and Rome'},\n {'role': 'assistant', 'content': 'Assistant calls weather tool two times'},\n {'role': 'tool', 'content': 'Weather in Paris is ...'},\n {'role': 'tool', 'content': 'Weather in Rome is ...'},\n {'role': 'assistant', 'content': 'Assistant returns weather'},\n {'role': 'user', 'content': 'Thanks!'}]\n\n\nFor ease of use, if msgs is not already in a list, it will automatically be wrapped inside one. This way you can pass a single prompt into mk_msgs and get back a LiteLLM compatible msg history.\n\nmsgs = mk_msgs(\"Hey\")\nmsgs\n\n[{'role': 'user', 'content': 'Hey'}]\n\n\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm fine, you?\"])\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant', 'content': \"I'm fine, you?\"}]\n\n\nHowever, beware that if you use mk_msgs for a single message, consisting of multiple parts. Then you should be explicit, and make sure to wrap those multiple messages in two lists:\n\nOne list to show that they belong together in one message (the inner list).\nAnother, because mk_msgs expects a list of multiple messages (the outer list).\n\nThis is common when working with images for example:\n\nmsgs = mk_msgs([['Whats in this img?',img_fn.read_bytes()]])\nprint(json.dumps(msgs,indent=1)[:200]+\"...\")\n\n[\n {\n  \"role\": \"user\",\n  \"content\": [\n   {\n    \"type\": \"text\",\n    \"text\": \"Whats in this img?\"\n   },\n   {\n    \"type\": \"image_url\",\n    \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...\n\n\n\n\n\n\nLiteLLM supports streaming responses. That‚Äôs really useful if you want to show intermediate results, instead of having to wait until the whole response is finished.\nWe create this helper function that returns the entire response at the end of the stream. This is useful when you want to store the whole response somewhere after having displayed the intermediate results.\n\nsource\n\n\n\n stream_with_complete (gen, postproc=&lt;function noop&gt;)\n\nExtend streaming response chunks with the complete response\n\nr = c(mk_msgs(\"Hey!\"), stream=True)\nr2 = SaveReturn(stream_with_complete(r))\n\n\nfor o in r2:\n    cts = o.choices[0].delta.content\n    if cts: print(cts, end='')\n\nHey! How's it going? üòä What can I help you with today?\n\n\n\nr2.value\n\nHey! How‚Äôs it going? üòä What can I help you with today?\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: stop\nusage: Usage(completion_tokens=22, prompt_tokens=9, total_tokens=31, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\n\n\n\n\nsource\n\n\n\n lite_mk_func (f)\n\n\ndef simple_add(\n    a: int,   # first operand\n    b: int=0  # second operand\n) -&gt; int:\n    \"Add two numbers together\"\n    return a + b\n\n\ntoolsc = lite_mk_func(simple_add)\ntoolsc\n\n{'type': 'function',\n 'function': {'name': 'simple_add',\n  'description': 'Add two numbers together\\n\\nReturns:\\n- type: integer',\n  'parameters': {'type': 'object',\n   'properties': {'a': {'type': 'integer', 'description': 'first operand'},\n    'b': {'type': 'integer', 'description': 'second operand', 'default': 0}},\n   'required': ['a']}}}\n\n\n\ntmsg = mk_msg(\"What is 5478954793+547982745? How about 5479749754+9875438979? Always use tools for calculations, and describe what you'll do before using a tool. Where multiple tool calls are required, do them in a single response where possible. \")\nr = c(tmsg, tools=[toolsc])\n\n\ndisplay(r)\n\nI‚Äôll help you calculate both of those sums using the addition tool.\nLet me break down what I‚Äôll do: 1. First calculation: 5478954793 + 547982745 2. Second calculation: 5479749754 + 9875438979\nSince these are independent calculations, I‚Äôll perform both at the same time.\nüîß simple_add({‚Äúa‚Äù: 5478954793, ‚Äúb‚Äù: 547982745})\nüîß simple_add({‚Äúa‚Äù: 5479749754, ‚Äúb‚Äù: 9875438979})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=211, prompt_tokens=659, total_tokens=870, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\ntcs = [_lite_call_func(o, ns=globals()) for o in r.choices[0].message.tool_calls]\ntcs\n\n[{'tool_call_id': 'toolu_01KATe5b5tmd4tK5D9BUZE5S',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '6026937538'},\n {'tool_call_id': 'toolu_01E4WQj8RkQj8Z7QLJ6ireTe',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '15355188733'}]\n\n\n\ndef delta_text(msg):\n    \"Extract printable content from streaming delta, return None if nothing to print\"\n    c = msg.choices[0]\n    if not c: return c\n    if not hasattr(c,'delta'): return None #f'{c}'\n    delta = c.delta\n    if delta.content: return delta.content\n    if delta.tool_calls:\n        res = ''.join(f\"üîß {tc.function.name}\" for tc in delta.tool_calls if tc.id and tc.function.name)\n        if res: return f'\\n{res}\\n'\n    if hasattr(delta,'reasoning_content'): return 'üß†' if delta.reasoning_content else '\\n\\n'\n    return None\n\n\nr = c(tmsg, stream=True, tools=[toolsc])\nr2 = SaveReturn(stream_with_complete(r))\nfor o in r2: print(delta_text(o) or '', end='')\n\nI'll help you calculate those two sums using the addition tool.\n\nLet me break down what I need to do:\n1. Calculate 5478954793 + 547982745\n2. Calculate 5479749754 + 9875438979\n\nSince these are independent calculations, I'll perform both additions at once.\nüîß simple_add\n\nüîß simple_add\n\n\n\nr2.value\n\nI‚Äôll help you calculate those two sums using the addition tool.\nLet me break down what I need to do: 1. Calculate 5478954793 + 547982745 2. Calculate 5479749754 + 9875438979\nSince these are independent calculations, I‚Äôll perform both additions at once.\nüîß simple_add({‚Äúa‚Äù: 5478954793, ‚Äúb‚Äù: 547982745})\nüîß simple_add({‚Äúa‚Äù: 5479749754, ‚Äúb‚Äù: 9875438979})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=206, prompt_tokens=659, total_tokens=865, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\nmsg = mk_msg(\"Solve this complex math problem: What is the derivative of x^3 + 2x^2 - 5x + 1?\")\nr = c(msg, stream=True, reasoning_effort=\"low\")\nr2 = SaveReturn(stream_with_complete(r))\nfor o in r2: print(delta_text(o) or '', end='')\n\nüß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†\n\n# Derivative Solution\n\nTo find the derivative of **f(x) = x¬≥ + 2x¬≤ - 5x + 1**, I'll apply the power rule to each term.\n\n## Using the Power Rule: d/dx(x‚Åø) = n¬∑x‚Åø‚Åª¬π\n\n**Term by term:**\n- d/dx(x¬≥) = 3x¬≤\n- d/dx(2x¬≤) = 4x\n- d/dx(-5x) = -5\n- d/dx(1) = 0\n\n## Answer:\n**f'(x) = 3x¬≤ + 4x - 5**\n\n\n\nr2.value\n\n\nTo find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 1, I‚Äôll apply the power rule to each term.\n\n\nTerm by term: - d/dx(x¬≥) = 3x¬≤ - d/dx(2x¬≤) = 4x - d/dx(-5x) = -5 - d/dx(1) = 0\n\n\n\nf‚Äô(x) = 3x¬≤ + 4x - 5\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: stop\nusage: Usage(completion_tokens=328, prompt_tokens=66, total_tokens=394, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=148, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\n\n\n\n\nLiteLLM provides search, not via tools, but via the special web_search_options param.\nNote: Not all models support web search. LiteLLM‚Äôs supports_web_search field should indicate this, but it‚Äôs unreliable for some models like claude-sonnet-4-20250514. Checking both supports_web_search and search_context_cost_per_query provides more accurate detection.\n\nfor m in ms: print(m, _has_search(m))\n\ngemini/gemini-2.5-flash True\nclaude-sonnet-4-5 True\nopenai/gpt-4.1 False\n\n\nWhen search is supported it can be used like this:\n\nsmsg = mk_msg(\"Search the web and tell me very briefly about otters\")\nr = c(smsg, web_search_options={\"search_context_size\": \"low\"})  # or 'medium' / 'high'\nr\n\nOtters are carnivorous mammals in the subfamily Lutrinae and members of the weasel family. The 14 extant otter species are all semiaquatic, both freshwater and marine. They‚Äôre found on every continent except Australia and Antarctica.\nOtters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water. In fact, otters have the densest fur of any animal‚Äîas many as a million hairs per square inch in places.\nAll otters are expert hunters that eat fish, crustaceans, and other critters. They‚Äôre known for being playful animals and sea otters famously use rocks as tools to crack open shellfish. When it‚Äôs time to nap, sea otters entangle themselves in kelp so they don‚Äôt float away.\nMany otter species were historically hunted nearly to extinction for their fur but have since recovered in some areas, though several species remain threatened by pollution and habitat loss.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=382, prompt_tokens=18089, total_tokens=18471, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\n\n\nNext, lets handle Anthropic‚Äôs search citations.\nWhen not using streaming, all citations are placed in a separate key in the response:\n\nr.choices[0].message.provider_specific_fields['citations'][0]\n\n[{'type': 'web_search_result_location',\n  'cited_text': 'Otters are carnivorous mammals in the subfamily Lutrinae. ',\n  'url': 'https://en.wikipedia.org/wiki/Otter',\n  'title': 'Otter - Wikipedia',\n  'encrypted_index': 'Eo8BCioICBgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDMlacTT8THSDML7nuhoMyB3Xp2StEfWJOx72IjATEIYmZbwZDH+a0KRLuOHQx4nipGzmvy//B4ItZEaDN4t55aF0a+SnmlUY390IN18qE+y/CtqixJ/kgvGL2GCYkFhQRxMYBA=='}]\n\n\nHowever, when streaming the results are not captured this way. Instead, we provide this helper function that adds the citation to the content field in markdown format:\n\nsource\n\n\n\n cite_footnotes (stream_list)\n\nAdd markdown footnote citations to stream deltas\n\nsource\n\n\n\n\n cite_footnote (msg)\n\n\nr = list(c(smsg, stream=True, web_search_options={\"search_context_size\": \"low\"}))\ncite_footnotes(r)\nstream_chunk_builder(r)\n\nOtters are * charismatic members of the weasel family, found on every continent except Australia and Antarctica. * * There are 13-14 species in total, ranging from the small-clawed otter to the giant otter.\nThese aquatic mammals are known for * their short ears and noses, elongated bodies, long tails, and soft, dense fur. In fact, * otters have the densest fur of any animal‚Äîas many as a million hairs per square inch, which keeps them warm in water since they lack blubber.\n* All otters are expert hunters that eat fish, crustaceans, and other critters. * Sea otters will float on their backs, place a rock on their chests, then smash mollusks down on it until they break open. * River otters are especially playful, gamboling on land and splashing into rivers and streams. They‚Äôre highly adapted for water with webbed feet, and * can stay submerged for more than 5 minutes, with river otters able to hold their breath for up to 8 minutes.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: stop\nusage: Usage(completion_tokens=431, prompt_tokens=15055, total_tokens=15486, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#deterministic-outputs",
    "href": "core.html#deterministic-outputs",
    "title": "Core",
    "section": "",
    "text": "LiteLLM ModelResponse(Stream) objects have id and created_at fields that are generated dynamically. Even when we use cachy to cache the LLM response these dynamic fields create diffs which makes code review more challenging. The patches below ensure that id and created_at fields are fixed and won‚Äôt generate diffs.\n\nsource\n\n\n\n patch_litellm (seed=0)\n\nPatch litellm.ModelResponseBase such that id and created are fixed.\n\npatch_litellm()",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#completion",
    "href": "core.html#completion",
    "title": "Core",
    "section": "",
    "text": "LiteLLM provides an convenient unified interface for most big LLM providers. Because it‚Äôs so useful to be able to switch LLM providers with just one argument. We want to make it even easier to by adding some more convenience functions and classes.\nThis is very similar to our other wrapper libraries for popular AI providers: claudette (Anthropic), gaspard (Gemini), cosette (OpenAI).\n\n# litellm._turn_on_debug()\n\n\nms = [\"gemini/gemini-2.5-flash\", \"claude-sonnet-4-5\", \"openai/gpt-4.1\"]\nmsg = [{'role':'user','content':'Hey there!', 'cache_control': {'type': 'ephemeral'}}]\nfor m in ms:\n    display(Markdown(f'**{m}:**'))\n    display(completion(m,msg))\n\ngemini/gemini-2.5-flash:\n\n\nHey there! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: gemini-2.5-flash\nfinish_reason: stop\nusage: Usage(completion_tokens=153, prompt_tokens=4, total_tokens=157, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=143, rejected_prediction_tokens=None, text_tokens=10), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=4, image_tokens=None))\n\n\n\n\nclaude-sonnet-4-5:\n\n\nHello! How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=12, prompt_tokens=10, total_tokens=22, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nopenai/gpt-4.1:\n\n\nHello! How can I help you today? üòä\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=10, prompt_tokens=10, total_tokens=20, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#messages-formatting",
    "href": "core.html#messages-formatting",
    "title": "Core",
    "section": "",
    "text": "Let‚Äôs start with making it easier to pass messages into litellm‚Äôs completion function (including images, and pdf files).\n\nsource\n\n\n\n remove_cache_ckpts (msg)\n\nremove cache checkpoints and return msg.\n\nsource\n\n\n\n\n mk_msg (content, role='user', cache=False, ttl=None)\n\nCreate a LiteLLM compatible message.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontent\n\n\nContent: str, bytes (image), list of mixed content, or dict w ‚Äòrole‚Äô and ‚Äòcontent‚Äô fields\n\n\nrole\nstr\nuser\nMessage role if content isn‚Äôt already a dict/Message\n\n\ncache\nbool\nFalse\nEnable Anthropic caching\n\n\nttl\nNoneType\nNone\nCache TTL: ‚Äò5m‚Äô (default) or ‚Äò1h‚Äô\n\n\n\nNow we can use mk_msg to create different types of messages.\nSimple text:\n\nmsg = mk_msg(\"hey\")\nmsg\n\n{'role': 'user', 'content': 'hey'}\n\n\nWhich can be passed to litellm‚Äôs completion function like this:\n\nmodel = ms[1]\n\n\nres = completion(model, [msg])\nres\n\nHey! How‚Äôs it going? What‚Äôs on your mind?\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=16, prompt_tokens=8, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nWe‚Äôll add a little shortcut to make examples and testing easier here:\n\ndef c(msgs, **kw):\n    msgs = [msgs] if isinstance(msgs,dict) else listify(msgs)\n    return completion(model, msgs, **kw)\n\n\nc(msg)\n\nHey! How‚Äôs it going? What‚Äôs on your mind?\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=16, prompt_tokens=8, total_tokens=24, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nLists w just one string element are flattened for conciseness:\n\ntest_eq(mk_msg(\"hey\"), mk_msg([\"hey\"]))\n\n(LiteLLM ignores these fields when sent to other providers)\nText and images:\n\nimg_fn = Path('samples/puppy.jpg')\nImage(filename=img_fn, width=200)\n\n\n\n\n\n\n\n\n\nmsg = mk_msg(['hey what in this image?',img_fn.read_bytes()])\nprint(json.dumps(msg,indent=1)[:200]+\"...\")\n\n{\n \"role\": \"user\",\n \"content\": [\n  {\n   \"type\": \"text\",\n   \"text\": \"hey what in this image?\"\n  },\n  {\n   \"type\": \"image_url\",\n   \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD/4gxUSU...\n\n\n\nc(msg)\n\nThis image shows an adorable Cavalier King Charles Spaniel puppy! The puppy has the breed‚Äôs characteristic features:\n\nColoring: Brown (chestnut) and white coat\nSweet expression: Large, dark eyes and a gentle face\nSetting: The puppy is lying on grass near some purple flowers (appear to be asters or similar blooms)\n\nThe puppy looks very young and has that irresistibly cute, innocent look that Cavalier puppies are famous for. The photo has a professional quality with nice lighting and composition, capturing the puppy‚Äôs endearing personality perfectly!\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=139, prompt_tokens=104, total_tokens=243, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nLet‚Äôs also demonstrate this for PDFs\n\npdf_fn = Path('samples/solveit.pdf')\nmsg = mk_msg(['Who is the author of this pdf?', pdf_fn.read_bytes()])\nc(msg)\n\nThe author of this PDF is Jeremy Howard from fast.ai. He explicitly introduces himself in the document with ‚ÄúHi, I‚Äôm Jeremy Howard, from fast.ai‚Äù and goes on to describe his work co-founding fast.ai with Rachel Thomas eight years ago.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=59, prompt_tokens=1610, total_tokens=1669, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\n\n\nSome providers such as Anthropic require manually opting into caching. Let‚Äôs try it:\n\ndef cpr(i): return f'{i} '*1024 + 'This is a caching test. Report back only what number you see repeated above.'\n\n\ndisable_cachy()\n\n\nmsg = mk_msg(cpr(1), cache=True)\nres = c(msg)\nres\n\n1\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=5, prompt_tokens=3, total_tokens=8, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=2070, cache_read_input_tokens=0)\n\n\n\n\nAnthropic has a maximum of 4 cache checkpoints, so we remove previous ones as we go:\n\nres = c([remove_cache_ckpts(msg), mk_msg(res), mk_msg(cpr(2), cache=True)])\nres\n\n2\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=5, prompt_tokens=2073, total_tokens=2078, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=2070, text_tokens=None, image_tokens=None), cache_creation_input_tokens=2074, cache_read_input_tokens=2070)\n\n\n\n\nWe see that the first message was cached, and this extra message has been written to cache:\n\nres.usage.prompt_tokens_details\n\nPromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=2070, text_tokens=None, image_tokens=None)\n\n\nWe can add a bunch of large messages in a loop to see how the number of cached tokens used grows.\nWe do this for 25 times to ensure it still works for more than &gt;20 content blocks, which is a known anthropic issue.\nThe code below is commented by default, because it‚Äôs slow. Please uncomment when working on caching.\n\n# h = []\n# msg = mk_msg(cpr(1), cache=True)\n\n# for o in range(2,25):\n#     h += [remove_cache_ckpts(msg), mk_msg(res)]\n#     msg = mk_msg(cpr(o), cache=True)\n#     res = c(h+[msg])\n#     detls = res.usage.prompt_tokens_details\n#     print(o, detls.cached_tokens, detls.cache_creation_tokens, end='; ')\n\n\nenable_cachy()\n\n\n\n\nLisette can call multiple tools in a loop. Further down this notebook, we‚Äôll provide convenience functions for formatting such a sequence of toolcalls and responses into one formatted output string.\nFor now, we‚Äôll show an example and show how to transform such a formatted output string back into a valid LiteLLM history.\n\nfmt_outp = '''\nI'll solve this step-by-step, using parallel calls where possible.\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\n  \"result\": \"15\"\n}\n```\n\n&lt;/details&gt;\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\n  \"result\": \"3\"\n}\n```\n\n&lt;/details&gt;\n\nNow I need to multiply 15 * 3 before I can do the final division:\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\n  \"result\": \"45\"\n}\n```\n\n&lt;/details&gt;\n'''\n\nWe can split into chunks of (text,toolstr,json):\n\nsp = re_tools.split(fmt_outp)\nfor o in list(chunked(sp, 3, pad=True)): print('- ', o)\n\n-  [\"\\nI'll solve this step-by-step, using parallel calls where possible.\\n\\n\", '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_01KjnQH2Nsz2viQ7XYpLW3Ta\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 10, \"b\": 5 } },\\n  \"result\": \"15\"\\n}']\n-  ['\\n\\n', '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_01Koi2EZrGZsBbnQ13wuuvzY\",\\n  \"call\": { \"function\": \"simple_add\", \"arguments\": { \"a\": 2, \"b\": 1 } },\\n  \"result\": \"3\"\\n}']\n-  ['\\n\\nNow I need to multiply 15 * 3 before I can do the final division:\\n\\n', '&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}\\n```\\n\\n&lt;/details&gt;', '{\\n  \"id\": \"toolu_0141NRaWUjmGtwxZjWkyiq6C\",\\n  \"call\": { \"function\": \"multiply\", \"arguments\": { \"a\": 15, \"b\": 3 } },\\n  \"result\": \"45\"\\n}']\n-  ['\\n', None, None]\n\n\n\nsource\n\n\n\n\n fmt2hist (outp:str)\n\nTransform a formatted output into a LiteLLM compatible history\nSee how we can turn that one formatted output string back into a list of Messages:\n\nfrom pprint import pprint\n\n\nh = fmt2hist(fmt_outp)\npprint(h)\n\n[Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":10,\"b\":5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '15',\n  'name': 'simple_add',\n  'role': 'tool',\n  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta'},\n Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":2,\"b\":1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '3',\n  'name': 'simple_add',\n  'role': 'tool',\n  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY'},\n Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":15,\"b\":3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function')], function_call=None, provider_specific_fields=None),\n {'content': '45',\n  'name': 'multiply',\n  'role': 'tool',\n  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C'},\n Message(content='.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\n\n\n\nWe will skip tool use blocks and tool results during caching\nNow lets make it easy to provide entire conversations:\n\nsource\n\n\n\n\n mk_msgs (msgs, cache=False, cache_idxs=[-1], ttl=None)\n\nCreate a list of LiteLLM compatible messages.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmsgs\n\n\nList of messages (each: str, bytes, list, or dict w ‚Äòrole‚Äô and ‚Äòcontent‚Äô fields)\n\n\ncache\nbool\nFalse\nEnable Anthropic caching\n\n\ncache_idxs\nlist\n[-1]\nCache breakpoint idxs\n\n\nttl\nNoneType\nNone\nCache TTL: ‚Äò5m‚Äô (default) or ‚Äò1h‚Äô\n\n\n\nWith mk_msgs you can easily provide a whole conversation:\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"])\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant', 'content': \"I'm doing fine and you?\"}]\n\n\nBy defualt the last message will be cached when cache=True:\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm doing fine and you?\"], cache=True)\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant',\n  'content': [{'type': 'text',\n    'text': \"I'm doing fine and you?\",\n    'cache_control': {'type': 'ephemeral'}}]}]\n\n\n\ntest_eq('cache_control' in msgs[-1]['content'][0], True)\n\nAlternatively, users can provide custom cache_idxs. Tool call blocks and results are skipped during caching:\n\nmsgs = mk_msgs(['Hello!','Hi! How can I help you?','Call some functions!',fmt_outp], cache=True, cache_idxs=[0,-2,-1])\nmsgs\n\n[{'role': 'user',\n  'content': [{'type': 'text',\n    'text': 'Hello!',\n    'cache_control': {'type': 'ephemeral'}}]},\n {'role': 'assistant', 'content': 'Hi! How can I help you?'},\n {'role': 'user',\n  'content': [{'type': 'text',\n    'text': 'Call some functions!',\n    'cache_control': {'type': 'ephemeral'}}]},\n Message(content=\"I'll solve this step-by-step, using parallel calls where possible.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":10,\"b\":5}', name='simple_add'), id='toolu_01KjnQH2Nsz2viQ7XYpLW3Ta', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01KjnQH2Nsz2viQ7XYpLW3Ta',\n  'name': 'simple_add',\n  'content': '15'},\n Message(content='', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":2,\"b\":1}', name='simple_add'), id='toolu_01Koi2EZrGZsBbnQ13wuuvzY', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_01Koi2EZrGZsBbnQ13wuuvzY',\n  'name': 'simple_add',\n  'content': '3'},\n Message(content='Now I need to multiply 15 * 3 before I can do the final division:', role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\":15,\"b\":3}', name='multiply'), id='toolu_0141NRaWUjmGtwxZjWkyiq6C', type='function')], function_call=None, provider_specific_fields=None),\n {'role': 'tool',\n  'tool_call_id': 'toolu_0141NRaWUjmGtwxZjWkyiq6C',\n  'name': 'multiply',\n  'content': '45'},\n Message(content=[{'type': 'text', 'text': '.', 'cache_control': {'type': 'ephemeral'}}], role='assistant', tool_calls=None, function_call=None, provider_specific_fields=None)]\n\n\n\ntest_eq('cache_control' in msgs[0]['content'][0], True)\ntest_eq('cache_control' in msgs[2]['content'][0], True) # shifted idxs to skip tools\ntest_eq('cache_control' in msgs[-1]['content'][0], True)\n\nWho‚Äôs speaking at when is automatically inferred. Even when there are multiple tools being called in parallel (which LiteLLM supports!).\n\nmsgs = mk_msgs(['Tell me the weather in Paris and Rome',\n                'Assistant calls weather tool two times',\n                {'role':'tool','content':'Weather in Paris is ...'},\n                {'role':'tool','content':'Weather in Rome is ...'},\n                'Assistant returns weather',\n                'Thanks!'])\nmsgs\n\n[{'role': 'user', 'content': 'Tell me the weather in Paris and Rome'},\n {'role': 'assistant', 'content': 'Assistant calls weather tool two times'},\n {'role': 'tool', 'content': 'Weather in Paris is ...'},\n {'role': 'tool', 'content': 'Weather in Rome is ...'},\n {'role': 'assistant', 'content': 'Assistant returns weather'},\n {'role': 'user', 'content': 'Thanks!'}]\n\n\nFor ease of use, if msgs is not already in a list, it will automatically be wrapped inside one. This way you can pass a single prompt into mk_msgs and get back a LiteLLM compatible msg history.\n\nmsgs = mk_msgs(\"Hey\")\nmsgs\n\n[{'role': 'user', 'content': 'Hey'}]\n\n\n\nmsgs = mk_msgs(['Hey!',\"Hi there!\",\"How are you?\",\"I'm fine, you?\"])\nmsgs\n\n[{'role': 'user', 'content': 'Hey!'},\n {'role': 'assistant', 'content': 'Hi there!'},\n {'role': 'user', 'content': 'How are you?'},\n {'role': 'assistant', 'content': \"I'm fine, you?\"}]\n\n\nHowever, beware that if you use mk_msgs for a single message, consisting of multiple parts. Then you should be explicit, and make sure to wrap those multiple messages in two lists:\n\nOne list to show that they belong together in one message (the inner list).\nAnother, because mk_msgs expects a list of multiple messages (the outer list).\n\nThis is common when working with images for example:\n\nmsgs = mk_msgs([['Whats in this img?',img_fn.read_bytes()]])\nprint(json.dumps(msgs,indent=1)[:200]+\"...\")\n\n[\n {\n  \"role\": \"user\",\n  \"content\": [\n   {\n    \"type\": \"text\",\n    \"text\": \"Whats in this img?\"\n   },\n   {\n    \"type\": \"image_url\",\n    \"image_url\": \"data:image/jpeg;base64,/9j/4AAQSkZJRgABAQAAAQABAAD...",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#streaming",
    "href": "core.html#streaming",
    "title": "Core",
    "section": "",
    "text": "LiteLLM supports streaming responses. That‚Äôs really useful if you want to show intermediate results, instead of having to wait until the whole response is finished.\nWe create this helper function that returns the entire response at the end of the stream. This is useful when you want to store the whole response somewhere after having displayed the intermediate results.\n\nsource\n\n\n\n stream_with_complete (gen, postproc=&lt;function noop&gt;)\n\nExtend streaming response chunks with the complete response\n\nr = c(mk_msgs(\"Hey!\"), stream=True)\nr2 = SaveReturn(stream_with_complete(r))\n\n\nfor o in r2:\n    cts = o.choices[0].delta.content\n    if cts: print(cts, end='')\n\nHey! How's it going? üòä What can I help you with today?\n\n\n\nr2.value\n\nHey! How‚Äôs it going? üòä What can I help you with today?\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: stop\nusage: Usage(completion_tokens=22, prompt_tokens=9, total_tokens=31, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#tools",
    "href": "core.html#tools",
    "title": "Core",
    "section": "",
    "text": "source\n\n\n\n lite_mk_func (f)\n\n\ndef simple_add(\n    a: int,   # first operand\n    b: int=0  # second operand\n) -&gt; int:\n    \"Add two numbers together\"\n    return a + b\n\n\ntoolsc = lite_mk_func(simple_add)\ntoolsc\n\n{'type': 'function',\n 'function': {'name': 'simple_add',\n  'description': 'Add two numbers together\\n\\nReturns:\\n- type: integer',\n  'parameters': {'type': 'object',\n   'properties': {'a': {'type': 'integer', 'description': 'first operand'},\n    'b': {'type': 'integer', 'description': 'second operand', 'default': 0}},\n   'required': ['a']}}}\n\n\n\ntmsg = mk_msg(\"What is 5478954793+547982745? How about 5479749754+9875438979? Always use tools for calculations, and describe what you'll do before using a tool. Where multiple tool calls are required, do them in a single response where possible. \")\nr = c(tmsg, tools=[toolsc])\n\n\ndisplay(r)\n\nI‚Äôll help you calculate both of those sums using the addition tool.\nLet me break down what I‚Äôll do: 1. First calculation: 5478954793 + 547982745 2. Second calculation: 5479749754 + 9875438979\nSince these are independent calculations, I‚Äôll perform both at the same time.\nüîß simple_add({‚Äúa‚Äù: 5478954793, ‚Äúb‚Äù: 547982745})\nüîß simple_add({‚Äúa‚Äù: 5479749754, ‚Äúb‚Äù: 9875438979})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=211, prompt_tokens=659, total_tokens=870, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\ntcs = [_lite_call_func(o, ns=globals()) for o in r.choices[0].message.tool_calls]\ntcs\n\n[{'tool_call_id': 'toolu_01KATe5b5tmd4tK5D9BUZE5S',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '6026937538'},\n {'tool_call_id': 'toolu_01E4WQj8RkQj8Z7QLJ6ireTe',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '15355188733'}]\n\n\n\ndef delta_text(msg):\n    \"Extract printable content from streaming delta, return None if nothing to print\"\n    c = msg.choices[0]\n    if not c: return c\n    if not hasattr(c,'delta'): return None #f'{c}'\n    delta = c.delta\n    if delta.content: return delta.content\n    if delta.tool_calls:\n        res = ''.join(f\"üîß {tc.function.name}\" for tc in delta.tool_calls if tc.id and tc.function.name)\n        if res: return f'\\n{res}\\n'\n    if hasattr(delta,'reasoning_content'): return 'üß†' if delta.reasoning_content else '\\n\\n'\n    return None\n\n\nr = c(tmsg, stream=True, tools=[toolsc])\nr2 = SaveReturn(stream_with_complete(r))\nfor o in r2: print(delta_text(o) or '', end='')\n\nI'll help you calculate those two sums using the addition tool.\n\nLet me break down what I need to do:\n1. Calculate 5478954793 + 547982745\n2. Calculate 5479749754 + 9875438979\n\nSince these are independent calculations, I'll perform both additions at once.\nüîß simple_add\n\nüîß simple_add\n\n\n\nr2.value\n\nI‚Äôll help you calculate those two sums using the addition tool.\nLet me break down what I need to do: 1. Calculate 5478954793 + 547982745 2. Calculate 5479749754 + 9875438979\nSince these are independent calculations, I‚Äôll perform both additions at once.\nüîß simple_add({‚Äúa‚Äù: 5478954793, ‚Äúb‚Äù: 547982745})\nüîß simple_add({‚Äúa‚Äù: 5479749754, ‚Äúb‚Äù: 9875438979})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=206, prompt_tokens=659, total_tokens=865, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\nmsg = mk_msg(\"Solve this complex math problem: What is the derivative of x^3 + 2x^2 - 5x + 1?\")\nr = c(msg, stream=True, reasoning_effort=\"low\")\nr2 = SaveReturn(stream_with_complete(r))\nfor o in r2: print(delta_text(o) or '', end='')\n\nüß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†\n\n# Derivative Solution\n\nTo find the derivative of **f(x) = x¬≥ + 2x¬≤ - 5x + 1**, I'll apply the power rule to each term.\n\n## Using the Power Rule: d/dx(x‚Åø) = n¬∑x‚Åø‚Åª¬π\n\n**Term by term:**\n- d/dx(x¬≥) = 3x¬≤\n- d/dx(2x¬≤) = 4x\n- d/dx(-5x) = -5\n- d/dx(1) = 0\n\n## Answer:\n**f'(x) = 3x¬≤ + 4x - 5**\n\n\n\nr2.value\n\n\nTo find the derivative of f(x) = x¬≥ + 2x¬≤ - 5x + 1, I‚Äôll apply the power rule to each term.\n\n\nTerm by term: - d/dx(x¬≥) = 3x¬≤ - d/dx(2x¬≤) = 4x - d/dx(-5x) = -5 - d/dx(1) = 0\n\n\n\nf‚Äô(x) = 3x¬≤ + 4x - 5\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: stop\nusage: Usage(completion_tokens=328, prompt_tokens=66, total_tokens=394, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=148, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#using-the-power-rule-ddxx‚Åø-nx‚Åø¬π",
    "href": "core.html#using-the-power-rule-ddxx‚Åø-nx‚Åø¬π",
    "title": "Core",
    "section": "",
    "text": "Term by term: - d/dx(x¬≥) = 3x¬≤ - d/dx(2x¬≤) = 4x - d/dx(-5x) = -5 - d/dx(1) = 0",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#answer",
    "href": "core.html#answer",
    "title": "Core",
    "section": "",
    "text": "f‚Äô(x) = 3x¬≤ + 4x - 5\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: stop\nusage: Usage(completion_tokens=328, prompt_tokens=66, total_tokens=394, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=148, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#search",
    "href": "core.html#search",
    "title": "Core",
    "section": "",
    "text": "LiteLLM provides search, not via tools, but via the special web_search_options param.\nNote: Not all models support web search. LiteLLM‚Äôs supports_web_search field should indicate this, but it‚Äôs unreliable for some models like claude-sonnet-4-20250514. Checking both supports_web_search and search_context_cost_per_query provides more accurate detection.\n\nfor m in ms: print(m, _has_search(m))\n\ngemini/gemini-2.5-flash True\nclaude-sonnet-4-5 True\nopenai/gpt-4.1 False\n\n\nWhen search is supported it can be used like this:\n\nsmsg = mk_msg(\"Search the web and tell me very briefly about otters\")\nr = c(smsg, web_search_options={\"search_context_size\": \"low\"})  # or 'medium' / 'high'\nr\n\nOtters are carnivorous mammals in the subfamily Lutrinae and members of the weasel family. The 14 extant otter species are all semiaquatic, both freshwater and marine. They‚Äôre found on every continent except Australia and Antarctica.\nOtters are distinguished by their long, slim bodies, powerful webbed feet for swimming, and their dense fur, which keeps them warm and buoyant in water. In fact, otters have the densest fur of any animal‚Äîas many as a million hairs per square inch in places.\nAll otters are expert hunters that eat fish, crustaceans, and other critters. They‚Äôre known for being playful animals and sea otters famously use rocks as tools to crack open shellfish. When it‚Äôs time to nap, sea otters entangle themselves in kelp so they don‚Äôt float away.\nMany otter species were historically hunted nearly to extinction for their fur but have since recovered in some areas, though several species remain threatened by pollution and habitat loss.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=382, prompt_tokens=18089, total_tokens=18471, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#citations",
    "href": "core.html#citations",
    "title": "Core",
    "section": "",
    "text": "Next, lets handle Anthropic‚Äôs search citations.\nWhen not using streaming, all citations are placed in a separate key in the response:\n\nr.choices[0].message.provider_specific_fields['citations'][0]\n\n[{'type': 'web_search_result_location',\n  'cited_text': 'Otters are carnivorous mammals in the subfamily Lutrinae. ',\n  'url': 'https://en.wikipedia.org/wiki/Otter',\n  'title': 'Otter - Wikipedia',\n  'encrypted_index': 'Eo8BCioICBgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDMlacTT8THSDML7nuhoMyB3Xp2StEfWJOx72IjATEIYmZbwZDH+a0KRLuOHQx4nipGzmvy//B4ItZEaDN4t55aF0a+SnmlUY390IN18qE+y/CtqixJ/kgvGL2GCYkFhQRxMYBA=='}]\n\n\nHowever, when streaming the results are not captured this way. Instead, we provide this helper function that adds the citation to the content field in markdown format:\n\nsource\n\n\n\n cite_footnotes (stream_list)\n\nAdd markdown footnote citations to stream deltas\n\nsource\n\n\n\n\n cite_footnote (msg)\n\n\nr = list(c(smsg, stream=True, web_search_options={\"search_context_size\": \"low\"}))\ncite_footnotes(r)\nstream_chunk_builder(r)\n\nOtters are * charismatic members of the weasel family, found on every continent except Australia and Antarctica. * * There are 13-14 species in total, ranging from the small-clawed otter to the giant otter.\nThese aquatic mammals are known for * their short ears and noses, elongated bodies, long tails, and soft, dense fur. In fact, * otters have the densest fur of any animal‚Äîas many as a million hairs per square inch, which keeps them warm in water since they lack blubber.\n* All otters are expert hunters that eat fish, crustaceans, and other critters. * Sea otters will float on their backs, place a rock on their chests, then smash mollusks down on it until they break open. * River otters are especially playful, gamboling on land and splashing into rivers and streams. They‚Äôre highly adapted for water with webbed feet, and * can stay submerged for more than 5 minutes, with river otters able to hold their breath for up to 8 minutes.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: stop\nusage: Usage(completion_tokens=431, prompt_tokens=15055, total_tokens=15486, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#examples",
    "href": "core.html#examples",
    "title": "Core",
    "section": "Examples",
    "text": "Examples\n\nHistory tracking\n\nchat = Chat(model)\nres = chat(\"Hey my name is Rens\")\nres\n\nHey Rens! Nice to meet you. How can I help you today?\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=20, prompt_tokens=14, total_tokens=34, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\nchat(\"Whats my name\")\n\nYour name is Rens! You told me that when you introduced yourself at the start of our conversation.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=25, prompt_tokens=42, total_tokens=67, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nSee now we keep track of history!\nHistory is stored in the hist attribute:\n\nchat.hist\n\n[{'role': 'user', 'content': 'Hey my name is Rens'},\n Message(content='Hey Rens! Nice to meet you. How can I help you today?', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}),\n {'role': 'user', 'content': 'Whats my name'},\n Message(content='Your name is Rens! You told me that when you introduced yourself at the start of our conversation.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})]\n\n\nYou can also pass an old chat history into new Chat objects:\n\nchat2 = Chat(model, hist=chat.hist)\nchat2(\"What was my name again?\")\n\nYour name is Rens! You‚Äôve asked me a couple times now - just checking if I‚Äôm paying attention? üòä\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=30, prompt_tokens=76, total_tokens=106, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\n\nSynthetic History Creation\nLets build chat history step by step. That way we can tweak anything we need to during testing.\n\npr = \"What is 5 + 7? Use the tool to calculate it.\"\nc = Chat(model, tools=[simple_add])\nres = c(pr)\n\n\nsource\n\n\nChat.print_hist\n\n Chat.print_hist ()\n\nPrint each message on a different line\nWhereas normally without tools we would get one user input and one assistant response. Here we get two extra messages in between. - An assistant message requesting the tools with arguments. - A tool response with the result to the tool call.\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content=None, role='assistant', tool_calls=[{'index': 0, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_012bi9eSyzhwaG3TgGpytJbc', 'type': 'function'}], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})\n\n{'tool_call_id': 'toolu_012bi9eSyzhwaG3TgGpytJbc', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n\n{'role': 'assistant', 'content': 'You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.'}\n\nMessage(content='\\n\\nThe result of 5 + 7 is **12**.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})\n\n\n\nLets try to build this up manually so we have full control over the inputs.\n\nsource\n\n\nrandom_tool_id\n\n random_tool_id ()\n\nGenerate a random tool ID with ‚Äòtoolu_‚Äô prefix\n\nrandom_tool_id()\n\n'toolu_0UAqFzWsDK4FrUMp48Y3tT3QD'\n\n\nA tool call request can contain one more or more tool calls. Lets make one.\n\nsource\n\n\nmk_tc\n\n mk_tc (func, args, tcid=None, idx=1)\n\n\ntc = mk_tc(simple_add.__name__, json.dumps(dict(a=5, b=7)))\ntc\n\n{'index': 1,\n 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'},\n 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n 'type': 'function'}\n\n\nThis can then be packged into the full Message object produced by the assitant.\n\ndef mk_tc_req(content, tcs): return Message(content=content, role='assistant', tool_calls=tcs, function_call=None)\n\n\ntc_cts = \"I'll use the simple_add tool to calculate 5 + 7 for you.\"\ntcq = mk_tc_req(tc_cts, [tc])\ntcq\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(index=1, function=Function(arguments='{\"a\": 5, \"b\": 7}', name='simple_add'), id='toolu_gAL47D1qXIaSyZPaE1pu1lJo7', type='function')], function_call=None, provider_specific_fields=None)\n\n\nNotice how Message instantiation creates a list of ChatCompletionMessageToolCalls by default. When the tools are executed this is converted back to a dictionary, for consistency we want to keep these as dictionaries from the beginning.\n\nsource\n\n\nmk_tc_req\n\n mk_tc_req (content, tcs)\n\n\ntcq = mk_tc_req(tc_cts, [tc])\ntcq\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\nc = Chat(model, tools=[simple_add], hist=[pr, tcq])\n\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\nLooks good so far! Now we will want to provide the actual result!\n\nsource\n\n\nmk_tc_result\n\n mk_tc_result (tc, result)\n\nNote we might have more than one tool call if more than one was passed in, here we just will make one result.\n\ntcq.tool_calls[0]\n\n{'index': 1,\n 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'},\n 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n 'type': 'function'}\n\n\n\nmk_tc_result(tcq.tool_calls[0], '12')\n\n{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '12'}\n\n\n\nsource\n\n\nmk_tc_results\n\n mk_tc_results (tcq, results)\n\nSame for here tcq.tool_calls will match the number of results passed in the results list.\n\ntcq\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\ntcr = mk_tc_results(tcq, ['12'])\ntcr\n\n[{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '12'}]\n\n\nNow we can call it with this synthetic data to see what the response is!\n\nc(tcr[0])\n\nThe result of 5 + 7 is 12.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=17, prompt_tokens=720, total_tokens=737, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content=\"I'll use the simple_add tool to calculate 5 + 7 for you.\", role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n\nMessage(content='The result of 5 + 7 is **12**.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})\n\n\n\nLets try this again, but lets give it something that is clearly wrong for fun.\n\nc = Chat(model, tools=[simple_add], hist=[pr, tcq])\n\n\ntcr = mk_tc_results(tcq, ['13'])\ntcr\n\n[{'tool_call_id': 'toolu_gAL47D1qXIaSyZPaE1pu1lJo7',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '13'}]\n\n\n\nc(tcr[0])\n\nThe result of 5 + 7 is 12.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=17, prompt_tokens=720, total_tokens=737, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nLets make sure this works with multiple tool calls in the same assistant Message.\n\ntcs = [\n    mk_tc(simple_add.__name__, json.dumps({\"a\": 5, \"b\": 7})), \n    mk_tc(simple_add.__name__, json.dumps({\"a\": 6, \"b\": 7})), \n]\n\n\ntcq = mk_tc_req(\"I will calculate these for you!\", tcs)\ntcq\n\nMessage(content='I will calculate these for you!', role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'type': 'function'}, {'index': 1, 'function': {'arguments': '{\"a\": 6, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n\n\ntcr = mk_tc_results(tcq, ['12', '13'])\n\n\nc = Chat(model, tools=[simple_add], hist=[pr, tcq, tcr[0]])\n\n\nc(tcr[1])\n\n5 + 7 = 12\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=13, prompt_tokens=812, total_tokens=825, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\nc.print_hist()\n\n{'role': 'user', 'content': 'What is 5 + 7? Use the tool to calculate it.'}\n\nMessage(content='I will calculate these for you!', role='assistant', tool_calls=[{'index': 1, 'function': {'arguments': '{\"a\": 5, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'type': 'function'}, {'index': 1, 'function': {'arguments': '{\"a\": 6, \"b\": 7}', 'name': 'simple_add'}, 'id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'type': 'function'}], function_call=None, provider_specific_fields=None)\n\n{'tool_call_id': 'toolu_XBetF5gIRHYH7LKBKxJsllLOD', 'role': 'tool', 'name': 'simple_add', 'content': '12'}\n\n{'tool_call_id': 'toolu_fU25035HyRrY03K6JBO94XfLE', 'role': 'tool', 'name': 'simple_add', 'content': '13'}\n\nMessage(content='5 + 7 = **12**', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})\n\n\n\n\nchat = Chat(ms[1], tools=[simple_add])\nres = chat(\"What's 5 + 3? Use the `simple_add` tool.\")\nres\n\nThe result of 5 + 3 is 8.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=18, prompt_tokens=742, total_tokens=760, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\nres = chat(\"Now, tell me a joke based on that result.\")\nres\n\nHere‚Äôs a joke based on the number 8:\nWhy was 6 afraid of 7?\nBecause 7 8 (ate) 9!\nBut since we got 8 as our answer, here‚Äôs another one:\nWhat do you call an 8 that‚Äôs been working out?\nAn ‚Äúate‚Äù with great figure! üí™\n(Get it? Because 8 already has a great figure with those curves!)\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=100, prompt_tokens=774, total_tokens=874, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\nchat.hist\n\n[{'role': 'user', 'content': \"What's 5 + 3? Use the `simple_add` tool.\"},\n Message(content=None, role='assistant', tool_calls=[{'index': 0, 'function': {'arguments': '{\"a\": 5, \"b\": 3}', 'name': 'simple_add'}, 'id': 'toolu_016dgFwdeaQXSwLPnJzufcWq', 'type': 'function'}], function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}),\n {'tool_call_id': 'toolu_016dgFwdeaQXSwLPnJzufcWq',\n  'role': 'tool',\n  'name': 'simple_add',\n  'content': '8'},\n {'role': 'assistant',\n  'content': 'You have no more tool uses. Please summarize your findings. If you did not complete your goal please tell the user what further work needs to be done so they can choose how best to proceed.'},\n Message(content='\\n\\nThe result of 5 + 3 is **8**.', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None}),\n {'role': 'user', 'content': 'Now, tell me a joke based on that result.'},\n Message(content='Here\\'s a joke based on the number 8:\\n\\nWhy was 6 afraid of 7?\\n\\nBecause 7 8 (ate) 9!\\n\\nBut since we got 8 as our answer, here\\'s another one:\\n\\nWhat do you call an 8 that\\'s been working out?\\n\\nAn \"ate\" with great figure! üí™\\n\\n(Get it? Because 8 already has a great figure with those curves!)', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})]\n\n\n\n\nImages\n\nchat = Chat(ms[1])\nchat(['Whats in this img?',img_fn.read_bytes()])\n\nImage Description\nThis adorable image shows a Cavalier King Charles Spaniel puppy with the classic Blenheim coloring (chestnut and white markings).\n\nKey features visible:\n\nPuppy with expressive brown eyes looking directly at the camera\nSoft, fluffy coat with rich brown/chestnut patches on the ears and around the eyes\nWhite blaze down the center of the face\nLying on grass in what appears to be a garden setting\nPurple flowers (possibly asters) visible in the background\nThe puppy has a sweet, gentle expression typical of the breed\n\nThe photo has a warm, professional quality with nice depth of field that keeps the focus on the puppy‚Äôs endearing face while softly blurring the floral background.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=188, prompt_tokens=105, total_tokens=293, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\n\n\nPrefill\nPrefill works as expected:\n\nchat = Chat(ms[1])\nchat(\"Spell my name\",prefill=\"Your name is R E\")\n\nYour name is R E D A C T E D\nI don‚Äôt actually know your name - you haven‚Äôt told me what it is yet! If you‚Äôd like me to spell your name, please let me know what it is first.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=47, prompt_tokens=16, total_tokens=63, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nAnd the entire message is stored in the history, not just the generated part:\n\nchat.hist[-1]\n\nMessage(content=\"Your name is R E D A C T E D\\n\\nI don't actually know your name - you haven't told me what it is yet! If you'd like me to spell your name, please let me know what it is first.\", role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})\n\n\n\n\nStreaming\n\nfrom time import sleep\n\n\nchat = Chat(model)\nstream_gen = chat(\"Count to 5\", stream=True)\nfor chunk in stream_gen:\n    if isinstance(chunk, ModelResponse): display(chunk)\n    else: print(delta_text(chunk) or '',end='')\n\n1, 2, 3, 4, 5\n\n\n1, 2, 3, 4, 5\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: stop\nusage: Usage(completion_tokens=17, prompt_tokens=11, total_tokens=28, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)\n\n\n\n\nLets try prefill with streaming too:\n\nstream_gen = chat(\"Continue counting to 10\",\"Okay! 6, 7\",stream=True)\nfor chunk in stream_gen:\n    if isinstance(chunk, ModelResponse): display(chunk)\n    else: print(delta_text(chunk) or '',end='')\n\nOkay! 6, 7, 8, 9, 10\n\n\nOkay! 6, 7, 8, 9, 10\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: stop\nusage: Usage(completion_tokens=12, prompt_tokens=44, total_tokens=56, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\n\nTool use\nOk now lets test tool use\n\nfor m in ms:\n    display(Markdown(f'**{m}:**'))\n    chat = Chat(m, tools=[simple_add])\n    res = chat(\"What's 5 + 3? Use the `simple_add` tool. Explain.\")\n    display(res)\n\ngemini/gemini-2.5-flash:\n\n\nI used the simple_add tool with a=5 and b=3. The tool returned 8.\nTherefore, 5 + 3 = 8.\n\n\nid: chatcmpl-xxx\nmodel: gemini-2.5-flash\nfinish_reason: stop\nusage: Usage(completion_tokens=118, prompt_tokens=159, total_tokens=277, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=79, rejected_prediction_tokens=None, text_tokens=39), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=159, image_tokens=None))\n\n\n\n\nclaude-sonnet-4-5:\n\n\nResult: 5 + 3 = 8\nExplanation: The simple_add function takes two parameters: - a (first operand): I provided 5 - b (second operand): I provided 3\nThe function added these two numbers together and returned 8, which is the correct sum of 5 and 3.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=89, prompt_tokens=764, total_tokens=853, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nopenai/gpt-4.1:\n\n\nThe result of 5 + 3 is 8.\nExplanation: I used the simple_add tool, which takes two numbers and adds them together. By inputting 5 and 3, the tool calculated the sum as 8.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=48, prompt_tokens=155, total_tokens=203, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n\n\nThinking w tool use\n\nchat = Chat(model, tools=[simple_add])\nres = chat(\"What's 5 + 3?\",think='l',return_all=True)\ndisplay(*res)\n\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=125, prompt_tokens=638, total_tokens=763, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=43, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n{'tool_call_id': 'toolu_01SY1R38L37vhWpgNgQz2B5h',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n5 + 3 = 8\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=14, prompt_tokens=816, total_tokens=830, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\n\nSearch\n\nchat = Chat(model)\nres = chat(\"Search the web and tell me very briefly about otters\", search='l', stream=True)\nfor o in res:\n    if isinstance(o, ModelResponse): sleep(0.01); display(o)\n    else: print(delta_text(o) or '',end='')\n\nOtters are charismatic members of the weasel family found on every continent except Australia and Antarctica. There are 13 species in total, including sea otters and river otters.\n\nThese aquatic mammals have elongated bodies, long tails, and soft, dense fur. In fact, otters have the densest fur of any animal‚Äîas many as a million hairs per square inch. Webbed feet and powerful tails make otters strong swimmers.\n\nAll otters are expert hunters that eat fish, crustaceans, and other critters. Sea otters float on their backs, place a rock on their chest, then smash mollusks down on it until it breaks open. They're also known for being playful animals, engaging in activities like sliding into water on natural slides.\n\n\nOtters are * charismatic members of the weasel family found on every continent except Australia and Antarctica. * There are 13 species in total, including sea otters and river otters.\nThese aquatic mammals have * elongated bodies, long tails, and soft, dense fur. In fact, * otters have the densest fur of any animal‚Äîas many as a million hairs per square inch. * Webbed feet and powerful tails make otters strong swimmers.\n* All otters are expert hunters that eat fish, crustaceans, and other critters. * Sea otters float on their backs, place a rock on their chest, then smash mollusks down on it until it breaks open. They‚Äôre also known for being * playful animals, engaging in activities like sliding into water on natural slides.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5\nfinish_reason: stop\nusage: Usage(completion_tokens=362, prompt_tokens=15055, total_tokens=15417, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)\n\n\n\n\n\n\nMulti tool calling\nWe can let the model call multiple tools in sequence using the max_steps parameter.\n\nchat = Chat(model, tools=[simple_add])\nres = chat(\"What's ((5 + 3)+7)+11? Work step by step\", return_all=True, max_steps=5)\nfor r in res: display(r)\n\nI‚Äôll solve this step by step using the addition function.\nStep 1: First, let me calculate 5 + 3\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=100, prompt_tokens=617, total_tokens=717, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n{'tool_call_id': 'toolu_01SykhkA2BGKXm9J56KCkz2B',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\nStep 2: Now I‚Äôll add 7 to that result (8 + 7)\nüîß simple_add({‚Äúa‚Äù: 8, ‚Äúb‚Äù: 7})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=93, prompt_tokens=730, total_tokens=823, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n{'tool_call_id': 'toolu_013LrGqASqf9Bsk38scV5Pu7',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\nStep 3: Finally, I‚Äôll add 11 to that result (15 + 11)\nüîß simple_add({‚Äúa‚Äù: 15, ‚Äúb‚Äù: 11})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=94, prompt_tokens=836, total_tokens=930, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n{'tool_call_id': 'toolu_01RtpzYFxji9ZbQJtTjKwaCi',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '26'}\n\n\nAnswer: ((5 + 3) + 7) + 11 = 26\nHere‚Äôs the breakdown: - 5 + 3 = 8 - 8 + 7 = 15 - 15 + 11 = 26\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=67, prompt_tokens=943, total_tokens=1010, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nSome models support parallel tool calling. I.e. sending multiple tool call requests in one conversation step.\n\ndef multiply(a: int, b: int) -&gt; int:\n    \"Multiply two numbers\"\n    return a * b\n\nchat = Chat('openai/gpt-4.1', tools=[simple_add, multiply])\nres = chat(\"Calculate (5 + 3) * (7 + 2)\", max_steps=5, return_all=True)\nfor r in res: display(r)\n\nüîß simple_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 3})\nüîß simple_add({‚Äúa‚Äù: 7, ‚Äúb‚Äù: 2})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=52, prompt_tokens=110, total_tokens=162, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_qJXSxYvc2ZVHmyIxqQ9OocWM',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '8'}\n\n\n{'tool_call_id': 'call_hCgeAPtd0RhmeADBRWRvY0sG',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '9'}\n\n\nüîß multiply({‚Äúa‚Äù:8,‚Äúb‚Äù:9})\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=17, prompt_tokens=178, total_tokens=195, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\n{'tool_call_id': 'call_1nwxhn7RXLNl9FcsS8pfn6OZ',\n 'role': 'tool',\n 'name': 'multiply',\n 'content': '72'}\n\n\n(5 + 3) = 8 and (7 + 2) = 9. Multiplying them gives: 8 √ó 9 = 72.\nSo, (5 + 3) √ó (7 + 2) = 72.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4.1-2025-04-14\nfinish_reason: stop\nusage: Usage(completion_tokens=55, prompt_tokens=203, total_tokens=258, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nSee it did the additions in one go!\nWe don‚Äôt want the model to keep running tools indefinitely. Lets showcase how we can force thee model to stop after our specified number of toolcall rounds:\n\ndef divide(a: int, b: int) -&gt; float:\n    \"Divide two numbers\"\n    return a / b\n\nchat = Chat(model, tools=[simple_add, multiply, divide])\nres = chat(\"Calculate ((10 + 5) * 3) / (2 + 1) step by step.\", \n           max_steps=3, return_all=True,\n           final_prompt=\"Please wrap-up for now and summarize how far we got.\")\nfor r in res: display(r)\n\nI‚Äôll calculate this step by step, following the order of operations.\nStep 1: Calculate the inner parentheses first - (10 + 5) = ? - (2 + 1) = ?\nüîß simple_add({‚Äúa‚Äù: 10, ‚Äúb‚Äù: 5})\nüîß simple_add({‚Äúa‚Äù: 2, ‚Äúb‚Äù: 1})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=173, prompt_tokens=792, total_tokens=965, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n{'tool_call_id': 'toolu_01NZjJc2q4tMJZcS93T1WQHM',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '15'}\n\n\n{'tool_call_id': 'toolu_013qQVARNY8a6shg4zo2TpNr',\n 'role': 'tool',\n 'name': 'simple_add',\n 'content': '3'}\n\n\nStep 2: Multiply 15 * 3\nüîß multiply({‚Äúa‚Äù: 15, ‚Äúb‚Äù: 3})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=82, prompt_tokens=1030, total_tokens=1112, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n{'tool_call_id': 'toolu_01Uf17eEfZPHcqFo1C3PYZ5E',\n 'role': 'tool',\n 'name': 'multiply',\n 'content': '45'}\n\n\nStep 3: Divide 45 / 3\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=23, prompt_tokens=1139, total_tokens=1162, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\n\nTool call exhaustion\n\npr = \"What is 1+2, and then the result of adding +2, and then +3 to it? Use tools to calculate!\"\nc = Chat(model, tools=[simple_add])\n\n\nres = c(pr, max_steps=2)\nres\n\nLet me continue with the next calculation. Now I‚Äôll add 2 to the result (3+2):\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=33, prompt_tokens=777, total_tokens=810, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\nassert c.hist[-2]['content'] == _final_prompt",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#key-features-visible",
    "href": "core.html#key-features-visible",
    "title": "Core",
    "section": "Key features visible:",
    "text": "Key features visible:\n\nPuppy with expressive brown eyes looking directly at the camera\nSoft, fluffy coat with rich brown/chestnut patches on the ears and around the eyes\nWhite blaze down the center of the face\nLying on grass in what appears to be a garden setting\nPurple flowers (possibly asters) visible in the background\nThe puppy has a sweet, gentle expression typical of the breed\n\nThe photo has a warm, professional quality with nice depth of field that keeps the focus on the puppy‚Äôs endearing face while softly blurring the floral background.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=188, prompt_tokens=105, total_tokens=293, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#async",
    "href": "core.html#async",
    "title": "Core",
    "section": "Async",
    "text": "Async\n\nAsyncChat\nIf you want to use LiteLLM in a webapp you probably want to use their async function acompletion. To make that easier we will implement our version of AsyncChat to complement it. It follows the same implementation as Chat as much as possible:\n\nsource\n\n\nastream_with_complete\n\n astream_with_complete (agen, postproc=&lt;function noop&gt;)\n\n\nsource\n\n\nAsyncChat\n\n AsyncChat (model:str, sp='', temp=0, search=False, tools:list=None,\n            hist:list=None, ns:Optional[dict]=None, cache=False,\n            cache_idxs:list=[-1], ttl=None)\n\nLiteLLM chat client.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nstr\n\nLiteLLM compatible model name\n\n\nsp\nstr\n\nSystem prompt\n\n\ntemp\nint\n0\nTemperature\n\n\nsearch\nbool\nFalse\nSearch (l,m,h), if model supports it\n\n\ntools\nlist\nNone\nAdd tools\n\n\nhist\nlist\nNone\nChat history\n\n\nns\nOptional\nNone\nCustom namespace for tool calling\n\n\ncache\nbool\nFalse\nAnthropic prompt caching\n\n\ncache_idxs\nlist\n[-1]\nAnthropic cache breakpoint idxs, use 0 for sys prompt if provided\n\n\nttl\nNoneType\nNone\nAnthropic prompt caching ttl\n\n\n\n\n\nExamples\nBasic example\n\nchat = AsyncChat(model)\nawait chat(\"What is 2+2?\")\n\n2+2 = 4\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=11, prompt_tokens=14, total_tokens=25, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nWith tool calls\n\nasync def async_add(a: int, b: int) -&gt; int:\n    \"Add two numbers asynchronously\"\n    await asyncio.sleep(0.1)\n    return a + b\n\n\nchat_with_tools = AsyncChat(model, tools=[async_add])\nres = await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\", return_all=True)\nasync for r in res: display(r)\n\nüîß async_add({‚Äúa‚Äù: 5, ‚Äúb‚Äù: 7})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=70, prompt_tokens=607, total_tokens=677, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n{'tool_call_id': 'toolu_01NHDNkcpwxW66XRuRFChLxe',\n 'role': 'tool',\n 'name': 'async_add',\n 'content': '12'}\n\n\nThe result of 5 + 7 is 12.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-5-20250929\nfinish_reason: stop\nusage: Usage(completion_tokens=18, prompt_tokens=731, total_tokens=749, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\nchat.hist\n\n[{'role': 'user', 'content': 'What is 2+2?'},\n Message(content='2+2 = 4', role='assistant', tool_calls=None, function_call=None, provider_specific_fields={'citations': None, 'thinking_blocks': None})]",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#async-streaming-display",
    "href": "core.html#async-streaming-display",
    "title": "Core",
    "section": "Async Streaming Display",
    "text": "Async Streaming Display\nThis is what our outputs look like with streaming results:\n\nchat_with_tools = AsyncChat(model, tools=[async_add])\nres = await chat_with_tools(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\nasync for o in res:\n    if isinstance(o,ModelResponseStream): print(delta_text(o) or '',end='')\n    elif isinstance(o,dict): print(o)\n\n\nüîß async_add\n{'tool_call_id': 'toolu_011RxwEK3HSc3VQwwsBZnXnV', 'role': 'tool', 'name': 'async_add', 'content': '12'}\n\n\nThe result of 5 + 7 is **12**.\n\n\nWe use this one quite a bit so we want to provide some utilities to better format these outputs:\nHere‚Äôs a complete ModelResponse taken from the response stream:\n\nresp = ModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01CWqrNQvoRjf1Q1GLpTUgQR', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=228, prompt_tokens=794, total_tokens=1022, prompt_tokens_details=None))\nprint(repr(resp))\n\nModelResponse(id='chatcmpl-xxx', created=1000000000, model='claude-sonnet-4-5', object='chat.completion', system_fingerprint=None, choices=[Choices(finish_reason='tool_calls', index=0, message=Message(content=\"I'll calculate ((10 + 5) * 3) / (2 + 1) step by step:\", role='assistant', tool_calls=[ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function'), ChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 2, \"b\": 1}', name='simple_add'), id='toolu_01CWqrNQvoRjf1Q1GLpTUgQR', type='function')], function_call=None, provider_specific_fields=None))], usage=Usage(completion_tokens=228, prompt_tokens=794, total_tokens=1022, completion_tokens_details=None, prompt_tokens_details=None))\n\n\n\ntc=resp.choices[0].message.tool_calls[0]\ntc\n\nChatCompletionMessageToolCall(function=Function(arguments='{\"a\": 10, \"b\": 5}', name='simple_add'), id='toolu_018BGyenjiRkDQFU1jWP6qRo', type='function')\n\n\n\ntr={'tool_call_id': 'toolu_018BGyenjiRkDQFU1jWP6qRo', 'role': 'tool','name': 'simple_add',\n    'content': '15 is the answerrrr' +'r'*2000}\n\n\nsource\n\nmk_tr_details\n\n mk_tr_details (tr, tc, mx=2000)\n\n*Create\n\nblock for tool call as JSON*\n\nmk_tr_details(tr,tc)\n\n'\\n\\n&lt;details class=\\'tool-usage-details\\'&gt;\\n\\n```json\\n{\\n  \"id\": \"toolu_018BGyenjiRkDQFU1jWP6qRo\",\\n  \"call\": {\\n    \"function\": \"simple_add\",\\n    \"arguments\": {\\n      \"a\": \"10\",\\n      \"b\": \"5\"\\n    }\\n  },\\n  \"result\": \"15 is the answerrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrrr&lt;TRUNCATED&gt;\"\\n}\\n```\\n\\n&lt;/details&gt;\\n\\n'\n\n\n\nsource\n\n\nAsyncStreamFormatter\n\n AsyncStreamFormatter (include_usage=False, mx=2000)\n\nInitialize self. See help(type(self)) for accurate signature.\n\nstream_msg = ModelResponseStream([StreamingChoices(delta=Delta(content=\"Hello world!\"))])\nprint(repr(AsyncStreamFormatter().format_item(stream_msg)))\n\n'Hello world!'\n\n\n\nreasoning_msg = ModelResponseStream([StreamingChoices(delta=Delta(reasoning_content=\"thinking...\"))])\nprint(repr(AsyncStreamFormatter().format_item(reasoning_msg)))\n\n'üß†'\n\n\n\nmock_tool_call = ChatCompletionMessageToolCall(\n    id=\"toolu_123abc456def\", type=\"function\", \n    function=Function( name=\"simple_add\", arguments='{\"a\": 5, \"b\": 3}' )\n)\n\nmock_response = ModelResponse()\nmock_response.choices = [type('Choice', (), {\n    'message': type('Message', (), {\n        'tool_calls': [mock_tool_call]\n    })()\n})()]\n\nmock_tool_result = {\n    'tool_call_id': 'toolu_123abc456def', 'role': 'tool', \n    'name': 'simple_add', 'content': '8'\n}\n\n\nfmt = AsyncStreamFormatter()\nfmt.format_item(mock_response)\nprint(fmt.format_item(mock_tool_result))\n\n\n\n&lt;details class='tool-usage-details'&gt;\n\n```json\n{\n  \"id\": \"toolu_123abc456def\",\n  \"call\": {\n    \"function\": \"simple_add\",\n    \"arguments\": {\n      \"a\": \"5\",\n      \"b\": \"3\"\n    }\n  },\n  \"result\": \"8\"\n}\n```\n\n&lt;/details&gt;\n\n\n\n\nIn jupyter it‚Äôs nice to use this AsyncStreamFormatter in combination with the Markdown display:\n\nsource\n\n\nadisplay_stream\n\n adisplay_stream (rs)\n\nUse IPython.display to markdown display the response stream.",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "core.html#streaming-examples",
    "href": "core.html#streaming-examples",
    "title": "Core",
    "section": "Streaming examples",
    "text": "Streaming examples\nNow we can demonstrate AsyncChat with stream=True!\n\nTool call\n\nchat = AsyncChat(model, tools=[async_add])\nres = await chat(\"What is 5 + 7? Use the tool to calculate it.\", stream=True)\nfmt = await adisplay_stream(res)\n\n\n{\n  \"id\": \"toolu_011RxwEK3HSc3VQwwsBZnXnV\",\n  \"call\": {\n    \"function\": \"async_add\",\n    \"arguments\": {\n      \"a\": \"5\",\n      \"b\": \"7\"\n    }\n  },\n  \"result\": \"12\"\n}\n\nThe result of 5 + 7 is 12.\n\n\n\n\nThinking tool call\n\nchat = AsyncChat(model)\nres = await chat(\"Briefly, what's the most efficient way to sort a list of 1000 random integers?\",\n                 think='l',stream=True)\n_ = await adisplay_stream(res)\n\nüß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†üß†\n\nUse your language‚Äôs built-in sort\nFor 1000 random integers, use your language‚Äôs built-in sort function (e.g., Python‚Äôs sorted(), Java‚Äôs Arrays.sort(), C++‚Äôs std::sort()).\nThese implementations use highly optimized algorithms like: - Timsort (Python/Java) - Introsort (C++) - Dual-pivot Quicksort (Java primitives)\nAll are O(n log n) and will outperform hand-coded solutions for this dataset size.\n\nIf implementing yourself: Use Quicksort or Mergesort ‚Äî both O(n log n) average case and efficient for this size.\n\n\n\n\n\nMultiple tool calls\n\nchat.hist[1]\n\n\nchat.hist[2]\n\n\nchat.hist[3]\n\n\nchat.hist[4]\n\n\nchat.hist[5]\n\nNow to demonstrate that we can load back the formatted output back into a new Chat object:\n\nchat5 = Chat(model,hist=fmt2hist(fmt.outp),tools=[simple_add, multiply, divide])\nchat5('what did we just do?')\n\n\n\nSearch\n\nchat_stream_tools = AsyncChat(model, search='l')\nres = await chat_stream_tools(\"Search the web and tell me very briefly about otters\", stream=True)\n_=await adisplay_stream(res)\n\n\n\nCaching\n\na,b = random.randint(0,100), random.randint(0,100)\nhist = [[f\"What is {a}+{b}?\\n\" * 200], f\"It's {a+b}\", ['hi'], \"Hello\"]\n\n\nchat = AsyncChat(model, cache=True, hist=hist)\nrs = await chat('hi again', stream=True, stream_options={\"include_usage\": True})\n\n\nasync for o in rs: \n    if isinstance(o, ModelResponse): print(o.usage)\n\nIn this first api call we will see cache creation until the last user msg:\n\ncache_read_toks = o.usage.cache_creation_input_tokens\ntest_eq(cache_read_toks &gt; 1000, True)\ntest_eq(o.usage.cache_read_input_tokens, 0)\n\n\nhist.extend([['hi again'], 'how may i help you?'])\nchat = AsyncChat(model, cache=True, hist=hist)\nrs = await chat('bye!', stream=True, stream_options={\"include_usage\": True})\n\n\nasync for o in rs:\n    if isinstance(o, ModelResponse): print(o.usage)\n\nThe subsequent call should re-use the existing cache:\n\ntest_eq(o.usage.cache_read_input_tokens, cache_read_toks)\n\n\nimport nbdev; nbdev.nbdev_export()",
    "crumbs": [
      "Core"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Lisette",
    "section": "",
    "text": "NB: If you are reading this in GitHub‚Äôs readme, we recommend you instead read the much more nicely formatted documentation format of this tutorial.\nLisette is a wrapper for the LiteLLM Python SDK, which provides unified access to 100+ LLM providers using the OpenAI API format.\nLiteLLM provides a unified interface to access multiple LLMs, but it‚Äôs quite low level: it leaves the developer to do a lot of stuff manually. Lisette automates pretty much everything that can be automated, whilst providing full control. Amongst the features provided:\nTo use Lisette, you‚Äôll need to set the appropriate API keys as environment variables for whichever LLM providers you want to use.",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#get-started",
    "href": "index.html#get-started",
    "title": "Lisette",
    "section": "Get started",
    "text": "Get started\nLiteLLM will automatically be installed with Lisette, if you don‚Äôt already have it.\n\n!pip install lisette -qq\n\nLisette only exports the symbols that are needed to use the library, so you can use import * to import them. Here‚Äôs a quick example showing how easy it is to switch between different LLM providers:\n\nfrom lisette import *",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#chat",
    "href": "index.html#chat",
    "title": "Lisette",
    "section": "Chat",
    "text": "Chat\n\nmodels = ['claude-sonnet-4-20250514', 'gemini/gemini-2.5-flash', 'openai/gpt-4o']\n\nfor model in models:\n    chat = Chat(model)\n    res = chat(\"Please tell me about yourself in one brief sentence.\")\n    display(res)\n\nI‚Äôm Claude, an AI assistant created by Anthropic to be helpful, harmless, and honest in conversations and tasks.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: stop\nusage: Usage(completion_tokens=29, prompt_tokens=17, total_tokens=46, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nI am a large language model, trained by Google, designed to assist with information and generate text.\n\n\nid: chatcmpl-xxx\nmodel: gemini-2.5-flash\nfinish_reason: stop\nusage: Usage(completion_tokens=603, prompt_tokens=11, total_tokens=614, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=583, rejected_prediction_tokens=None, text_tokens=20), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=None, text_tokens=11, image_tokens=None))\n\n\n\n\nI‚Äôm an AI language model created by OpenAI, designed to assist with a wide range of questions and tasks by providing information and generating text-based responses.\n\n\nid: chatcmpl-xxx\nmodel: gpt-4o-2024-08-06\nfinish_reason: stop\nusage: Usage(completion_tokens=30, prompt_tokens=17, total_tokens=47, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=0, audio_tokens=0, reasoning_tokens=0, rejected_prediction_tokens=0, text_tokens=None), prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=0, cached_tokens=0, text_tokens=None, image_tokens=None))\n\n\n\n\nThat‚Äôs it! Lisette handles all the provider-specific details automatically. Each model will respond in its own style, but the interface remains the same.",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#message-formatting",
    "href": "index.html#message-formatting",
    "title": "Lisette",
    "section": "Message formatting",
    "text": "Message formatting\n\nMultiple messages\nLisette accepts multiple messages in one go:\n\nchat = Chat(models[0])\nres = chat(['Hi! My favorite drink coffee.', 'Hello!', 'Whats my favorite drink?'])\ndisplay(res)\n\nHello! Based on what you just told me, your favorite drink is coffee! ‚òï\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: stop\nusage: Usage(completion_tokens=22, prompt_tokens=23, total_tokens=45, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nIf you have a pre-existing message history, you can also pass it when you create the Chat object:\n\nchat = Chat(models[0],hist=['Hi! My favorite drink is coffee.', 'Hello!'])\nres = chat('Whats my favorite drink?')\ndisplay(res)\n\nYour favorite drink is coffee! You just mentioned that in your previous message.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: stop\nusage: Usage(completion_tokens=18, prompt_tokens=30, total_tokens=48, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\n\nImages\nLisette also makes it easy to include images in your prompts:\n\nfrom pathlib import Path\nfrom IPython.display import Image\n\n\nfn = Path('samples/puppy.jpg')\nimg = fn.read_bytes()\nImage(img)\n\n\n\n\n\n\n\n\nAll you have to do is read it in as bytes:\n\nimg[:20]\n\nb'\\xff\\xd8\\xff\\xe0\\x00\\x10JFIF\\x00\\x01\\x01\\x00\\x00\\x01\\x00\\x01\\x00\\x00'\n\n\nAnd you can pass it inside a Chat object:\n\nchat = Chat(models[0])\nchat([img, \"What's in this image? Be brief.\"])\n\nA cute puppy with brown and white fur lying on grass next to purple flowers.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: stop\nusage: Usage(completion_tokens=20, prompt_tokens=108, total_tokens=128, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\n\nPrefill\nSome providers (e.g.¬†Anthropic) support prefill, allowing you to specify how the assistant‚Äôs response should begin:‚Äù\n\nchat = Chat(models[0])\nchat(\"Concisely, what's the meaning of life?\", prefill=\"According to Douglas Adams,\")\n\nAccording to Douglas Adams,it‚Äôs 42.\nMore seriously, there‚Äôs no universal answer. Common perspectives include: - Creating meaning through relationships, growth, and contribution - Fulfilling a divine purpose or spiritual calling - Maximizing well-being and minimizing suffering - Leaving a positive legacy - Simply experiencing and appreciating existence itself\nThe meaning might be something you create rather than discover.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: stop\nusage: Usage(completion_tokens=84, prompt_tokens=24, total_tokens=108, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#tools",
    "href": "index.html#tools",
    "title": "Lisette",
    "section": "Tools",
    "text": "Tools\nLisette makes it easy to give LLMs access to Python functions. Just define a function with type hints and a docstring:\n\ndef add_numbers(\n    a: int,  # First number to add\n    b: int   # Second number to add  \n) -&gt; int:\n    \"Add two numbers together\"\n    return a + b\n\nNow pass the function to Chat and the model can use it automatically:\n\nchat = Chat(models[0], tools=[add_numbers])\nres = chat(\"What's 47 + 23? Use the tool.\")\nres\n\nThe result of 47 + 23 is 70.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: stop\nusage: Usage(completion_tokens=18, prompt_tokens=573, total_tokens=591, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nIf you want to see all intermediate messages and outputs you can use the return_all=True feature.\n\nchat = Chat(models[0], tools=[add_numbers])\nres = chat(\"What's 47 + 23 + 59? Use the tool.\",max_steps=3,return_all=True)\ndisplay(*res)\n\nI‚Äôll help you calculate 47 + 23 + 59 using the add_numbers tool. Since the tool can only add two numbers at a time, I‚Äôll need to do this in two steps.\nüîß add_numbers({‚Äúa‚Äù: 47, ‚Äúb‚Äù: 23})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=116, prompt_tokens=433, total_tokens=549, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n{'tool_call_id': 'toolu_01F9oakoP8ANHkTMD1DyQDi7',\n 'role': 'tool',\n 'name': 'add_numbers',\n 'content': '70'}\n\n\nNow I‚Äôll add the result (70) to the third number (59):\nüîß add_numbers({‚Äúa‚Äù: 70, ‚Äúb‚Äù: 59})\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: tool_calls\nusage: Usage(completion_tokens=87, prompt_tokens=562, total_tokens=649, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n{'tool_call_id': 'toolu_01Cdf3FHJdbx64F8H8ooE1Db',\n 'role': 'tool',\n 'name': 'add_numbers',\n 'content': '129'}\n\n\nThe answer is 129.\nI calculated this by first adding 47 + 23 = 70, then adding 70 + 59 = 129.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: stop\nusage: Usage(completion_tokens=41, prompt_tokens=702, total_tokens=743, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nIt shows the intermediate tool calls, and the tool results!",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#web-search",
    "href": "index.html#web-search",
    "title": "Lisette",
    "section": "Web search",
    "text": "Web search\nSome models support web search capabilities. Lisette makes this easy to use:\n\nchat = Chat(models[0], search='l')  # 'l'ow, 'm'edium, or 'h'igh search context\nres = chat(\"Please tell me one fun fact about otters. Keep it brief\")\nres\n\nHere‚Äôs a fun fact about otters: Sea otters allow themselves to get entangled in kelp forests - this creates a tether so they don‚Äôt drift away on sleep currents as they sleep. They essentially use kelp as a natural anchor to stay in place while floating and resting on the water‚Äôs surface!\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: stop\nusage: Usage(completion_tokens=143, prompt_tokens=15626, total_tokens=15769, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), server_tool_use=ServerToolUse(web_search_requests=1), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nSome providers (like Anthropic) provide citations for their search results.\n\n\n\nres.choices[0].message.provider_specific_fields\n\n{'citations': [[{'type': 'web_search_result_location',\n    'cited_text': 'Sea Otters allow themselves to get entangled in kelp forests this creates a tether so they don‚Äôt drift away on sleep currents as they sleep. ',\n    'url': 'https://www.mygreenworld.org/blog/facts-about-otters',\n    'title': 'Five Fast Facts about Otters ‚Äî My Green World',\n    'encrypted_index': 'EpABCioIBxgCIiQ4ODk4YTFkYy0yMTNkLTRhNmYtOTljYi03ZTBlNTUzZDc0NWISDCMi/kxdYrQXVUX+ZxoMVvW3BHE29cyMhwAFIjBZEBw3PaH+XAslsXWMNucD7FqSwe5Fnnsfh2RzTX9x/q9XQ1Mm1Ke6JOreehNzVI0qFDkJYT4NCX8U4CjHHwoyLKtY66vhGAQ='}]],\n 'thinking_blocks': None}",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#streaming",
    "href": "index.html#streaming",
    "title": "Lisette",
    "section": "Streaming",
    "text": "Streaming\nFor real-time responses, use stream=True to get chunks as they‚Äôre generated rather than waiting for the complete response:\n\nchat = Chat(models[0])\nres_gen = chat(\"Concisely, what are the top 10 biggest animals?\", stream=True)\nres_gen\n\n&lt;generator object Chat._call&gt;\n\n\n\nfrom litellm import ModelResponse, ModelResponseStream\n\nYou can loop over the generator to get the partial responses:\n\nfor chunk in res_gen:\n    if isinstance(chunk,ModelResponseStream): print(chunk.choices[0].delta.content,end='')\n\nHere are the top 10 biggest animals by size/weight:\n\n1. **Blue whale** - largest animal ever, up to 100 feet long\n2. **Fin whale** - second-largest whale, up to 85 feet\n3. **Bowhead whale** - up to 65 feet, very heavy build\n4. **Right whale** - up to 60 feet, extremely bulky\n5. **Sperm whale** - up to 67 feet, largest toothed whale\n6. **Gray whale** - up to 50 feet\n7. **Humpback whale** - up to 52 feet\n8. **African elephant** - largest land animal, up to 13 feet tall\n9. **Colossal squid** - up to 46 feet long (largest invertebrate)\n10. **Giraffe** - tallest animal, up to 18 feet tall\n\n*Note: Various whale species dominate due to the ocean's ability to support massive body sizes.*None\n\n\nAnd the final chunk is the complete ModelResponse:\n\nchunk\n\nHere are the top 10 biggest animals by size/weight:\n\nBlue whale - largest animal ever, up to 100 feet long\nFin whale - second-largest whale, up to 85 feet\nBowhead whale - up to 65 feet, very heavy build\nRight whale - up to 60 feet, extremely bulky\nSperm whale - up to 67 feet, largest toothed whale\nGray whale - up to 50 feet\nHumpback whale - up to 52 feet\nAfrican elephant - largest land animal, up to 13 feet tall\nColossal squid - up to 46 feet long (largest invertebrate)\nGiraffe - tallest animal, up to 18 feet tall\n\nNote: Various whale species dominate due to the ocean‚Äôs ability to support massive body sizes.\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: stop\nusage: Usage(completion_tokens=233, prompt_tokens=22, total_tokens=255, completion_tokens_details=CompletionTokensDetailsWrapper(accepted_prediction_tokens=None, audio_tokens=None, reasoning_tokens=0, rejected_prediction_tokens=None, text_tokens=None), prompt_tokens_details=None)",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#async",
    "href": "index.html#async",
    "title": "Lisette",
    "section": "Async",
    "text": "Async\nFor web applications and concurrent operations, like in FastHTML, we recommend using AsyncChat:\n\nchat = AsyncChat(models[0])\nawait chat(\"Hi there\")\n\nHello! How are you doing today? Is there anything I can help you with?\n\n\nid: chatcmpl-xxx\nmodel: claude-sonnet-4-20250514\nfinish_reason: stop\nusage: Usage(completion_tokens=20, prompt_tokens=9, total_tokens=29, completion_tokens_details=None, prompt_tokens_details=PromptTokensDetailsWrapper(audio_tokens=None, cached_tokens=0, text_tokens=None, image_tokens=None), cache_creation_input_tokens=0, cache_read_input_tokens=0)\n\n\n\n\nTo wrap up, we‚Äôll show an example of async + streaming + toolcalling + search:\n\nchat = AsyncChat(models[0], search='l', tools=[add_numbers])\nres = await chat(\"\"\"\\\nSearch the web for the avg weight, in kgs, of male African and Asian elephants. Then add the two.\nKeep your replies ultra concise! Dont search the web more than once please.\n\"\"\", max_steps=4, stream=True)\nawait adisplay_stream(res)  # this is a convenience function to make async streaming look great in notebooks!\n\nBased on the search results:\nMale African elephants: * * Average weight is 5,000 kg (11,000 pounds)\nMale Asian elephants: * * Average weight is 3,600 kg (7,900 pounds)\n\nadd_numbers({\"a\": 5000, \"b\": 3600}) - 8600\n\nTotal: 8,600 kg",
    "crumbs": [
      "Lisette"
    ]
  },
  {
    "objectID": "index.html#next-steps",
    "href": "index.html#next-steps",
    "title": "Lisette",
    "section": "Next steps",
    "text": "Next steps\nReady to dive deeper?\n\nCheck out the rest of the documentation.\nVisit the GitHub repository to contribute or report issues.\nJoin our Discord community!",
    "crumbs": [
      "Lisette"
    ]
  }
]